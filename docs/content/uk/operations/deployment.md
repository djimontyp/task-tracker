# Посібник розгортання та операцій

**Останнє оновлення:** 26 жовтня 2025 р.
**Цільова аудиторія:** Інженери DevOps, команди SRE
**Передумови:** Docker Compose 2.x, інструменти клієнта PostgreSQL, менеджер пакетів uv

---

## Зміст

1. [Огляд архітектури системи](#огляд-архітектури-системи)
2. [Процедури розгортання](#процедури-розгортання)
3. [Моніторинг та спостережуваність](#моніторинг-та-спостережуваність)
4. [Посібник усунення неполадок](#посібник-усунення-неполадок)
5. [Налаштування продуктивності](#налаштування-продуктивності)
6. [Процедури відкату](#процедури-відкату)

---

## Огляд архітектури системи

### Граф залежностей сервісів

```
┌──────────────────────────────────────────────────────────────────┐
│                      РІВЕНЬ 4: Шлюз                             │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │  Nginx (Alpine)                                            │  │
│  │  Порти: 80 (HTTP), 443 (HTTPS)                             │  │
│  │  Маршрути: /api → api:8000, /dashboard → dashboard:3000  │  │
│  │  WebSocket: /ws → api:8000 (86400s timeout)                │  │
│  └──────────────────┬────────────────────┬────────────────────┘  │
└────────────────────┼────────────────────┼───────────────────────┘
                     │                    │
┌────────────────────┼────────────────────┼───────────────────────┐
│                    │  РІВЕНЬ 3: Фронтенд│                       │
│  ┌─────────────────▼─────────────────┐  │                       │
│  │  Dashboard (React 18 + Vite)      │  │                       │
│  │  Порт: 3000 (внутрішній)         │  │                       │
│  │  Можливості: Реал-часовий WebSocket│  │                       │
│  │  Здоров'я: HTTP 200 на /          │  │                       │
│  └───────────────────────────────────┘  │                       │
└─────────────────────────────────────────┼───────────────────────┘
                                          │
┌─────────────────────────────────────────┼───────────────────────┐
│  РІВЕНЬ 2: Сервіси застосування         │                       │
│  ┌──────────────────────────────────────▼──┐                    │
│  │  FastAPI Backend                        │                    │
│  │  Порт: 8000 (внутрішній + зовнішній)    │                    │
│  │  Робітники: 4 процеси Uvicorn           │                    │
│  │  Кінцеві точки: /api/health, /ws       │                    │
│  └──────────────┬──────────────────────────┘                    │
│                 │                                                │
│  ┌──────────────▼──────────────────────────┐                    │
│  │  TaskIQ Worker                          │                    │
│  │  Без зовнішніх портів                   │                    │
│  │  Завдання: 10 типів фонових завдань     │                    │
│  │  Здоров'я: pgrep -f 'taskiq worker'    │                    │
│  └──────────────┬──────────────────────────┘                    │
│                 │                                                │
└─────────────────┼────────────────────────────────────────────────┘
                  │
┌─────────────────┼────────────────────────────────────────────────┐
│  РІВЕНЬ 1: Інфраструктура                                         │
│  ┌──────────────▼─────────────┐  ┌───────────────────────────┐  │
│  │  PostgreSQL 15 + pgvector  │  │  NATS JetStream           │  │
│  │  Порти: 5432 → 5555 (хост) │  │  Порти: 4222 (клієнт)    │  │
│  │  Том: postgres_data         │  │          8222 (моніторинг)│  │
│  │  Здоров'я: pg_isready      │  │  Том: nats_data          │  │
│  └────────────────────────────┘  └───────────────────────────┘  │
└──────────────────────────────────────────────────────────────────┘
```

### Послідовність запуску

| Порядок | Рівень | Сервіси | Залежності | Час очікування |
|---------|--------|---------|-----------|----------------|
| 1 | Інфраструктура | `postgres`, `nats` | Немає | 10-15s (перевірка здоров'я) |
| 2 | Застосування | `worker`, `api` | postgres + nats здорові | 15-40s (міграція + запуск) |
| 3 | Фронтенд | `dashboard` | api здоровий | 20s (сервер розробки Vite) |
| 4 | Шлюз | `nginx` | api + dashboard здорові | 10s (завантаження конфігу) |

**Загальний час холодного запуску:** 55-85 секунд від `docker compose up` до повної доступності

### Розподіл портів

| Сервіс | Внутрішній порт | Порт хоста | Протокол | Призначення |
|---------|-----------------|-----------|----------|---------|
| **postgres** | 5432 | 5555 | TCP | Підключення до бази даних |
| **nats** | 4222 | 4222 | TCP | Протокол клієнта NATS |
| **nats** | 8222 | 8222 | HTTP | Моніторинг NATS (лише dev) |
| **api** | 8000 | 8000 | HTTP/WS | API + WebSocket (прямий) |
| **dashboard** | 3000 | 3000 | HTTP | Сервер розробки Vite (лише dev) |
| **nginx** | 80 | 80 | HTTP | Публічна точка входу HTTP |
| **nginx** | 443 | 443 | HTTPS | Публічна точка входу HTTPS |

**Примітка для продакшену:** Видаліть `8222` (моніторинг NATS) та `3000` (dev dashboard) з зовнішнього доступу.

### Виділення ресурсів

| Сервіс | Обмеження CPU | Обмеження пам'яті | Резервування CPU | Резервування пам'яті |
|---------|---------------|-----------------|-----------------|-------------------|
| **postgres** | 2.0 ядер | 2 GB | 0.5 ядер | 512 MB |
| **nats** | 0.5 ядер | 512 MB | 0.1 ядер | 64 MB |
| **worker** | 2.0 ядер | 2 GB | 0.5 ядер | 512 MB |
| **api** | 1.0 ядер | 1 GB | 0.25 ядер | 256 MB |
| **dashboard** | 0.5 ядер | 512 MB | 0.1 ядер | 128 MB |
| **nginx** | 0.5 ядер | 256 MB | 0.1 ядер | 64 MB |

**Загальні вимоги кластера:** 6.5 ядер CPU, 6.25 GB ОЗП (обмеження) | 2.0 ядер CPU, 1.5 GB ОЗП (резервування)

---

## Процедури розгортання

### Рабочий процес розгортання у продакшені

#### Контрольний список перед розгортанням

| Крок | Дія | Команда перевірки | Очікуваний результат |
|------|-----|------------------|-------------------|
| 1 | Резервна копія бази даних | `docker exec task-tracker-postgres pg_dump -U postgres tasktracker > backup_$(date +%Y%m%d_%H%M%S).sql` | Файл SQL дампу створено |
| 2 | Перегляд міграцій | `uv run alembic history` | Без конфліктів злиття |
| 3 | Тест на staging | `just alembic-up` на staging | Міграції застосовуються чисто |
| 4 | Збірка образів | `docker compose build --no-cache` | Усі сервіси успішно збудовані |
| 5 | Мітка випуску | `git tag -a v1.0.0 -m "Release 1.0.0"` | Мітка Git створена |
| 6 | Оновлення .env | Перегляд змінних середовища | Секрети перевірені за потреби |

#### Етапи розгортання

| Крок | Команда | Очікувана тривалість | Індикатор успіху |
|------|---------|------------------|------------------|
| 1 | `just services-stop` | 10-15s | Усі контейнери зупинено |
| 2 | `git pull origin main` | 2-5s | Код оновлено |
| 3 | `just alembic-up` | 5-30s | "Running upgrade ... -> head" |
| 4 | `docker compose up -d postgres nats` | 10s | Перевірки здоров'я пройдено |
| 5 | Очікування на інфраструктуру | 15s | `docker ps` показує здорові сервіси |
| 6 | `docker compose up -d worker api dashboard nginx` | 20-30s | Усі сервіси здорові |
| 7 | Перевірка здоров'я | `curl http://localhost/api/health` | HTTP 200 з JSON відповіддю |

**Загальний час розгортання:** 60-100 секунд

#### Перевірка після розгортання

| Перевірка | Команда | Очікуваний результат | Заходи щодо виправлення |
|----------|---------|-----------------|-------------|
| Статус сервісу | `docker compose ps` | Усі "Up" з "(healthy)" | Перевірте логи невдалих сервісів |
| Здоров'я API | `curl http://localhost/api/health` | `{"status": "ok", "timestamp": "..."}` | Перезапустіть сервіс api |
| WebSocket | Browser DevTools → Network → WS | З'єднання встановлено | Перезапустіть сервіс nginx |
| Логи Worker | `docker logs task-tracker-worker | tail -50` | Без помилок, обробка завдань видима | Перевірте з'єднання NATS |
| Використання пам'яті | `docker stats --no-stream` | В межах обмежень ресурсів | Збільшите обмеження або зменшите розміри партій |
| Міграція БД | `uv run alembic current` | Показує останню ревізію | Відкатіть та повторно застосуйте |

### Розгортання для розробки

#### Стандартний режим розробки

| Команда | Сценарій використання | Можливості |
|---------|---------|----------|
| `just services-dev` | Повний стек з гарячою перезавантаженням | Автоматичний перезапуск при змінах backend, HMR Vite на фронтенді |
| `just dev worker` | Лише Worker з гарячою перезавантаженням | Ізольована розробка worker |
| `just rebuild api` | Примусова перебудова одного сервісу | Після змін залежностей |

**Пути гарячої перезавантаження:**
- **Backend:** `backend/app/*`, `backend/core/*` → синхронізація + перезапуск
- **Frontend:** `frontend/src/*`, `frontend/public/*` → синхронізація (Vite HMR)
- **Залежності:** `pyproject.toml`, `package.json` → перебудова контейнера

#### Робочий процес міграції баз даних

| Сценарій | Послідовність команд | Примітки |
|----------|-------------------|---------|
| **Застосування очікуючих міграцій** | `just alembic-up` | Потребує запущеного postgres |
| **Створення нової міграції** | `just alembic-auto -m "опис"` | Автоматично генерує зі змін моделей |
| **Свіжа база даних** | `just db-migrate-fresh` | Запуск postgres + застосування міграцій |
| **Ядерний перезавантаження** | `just db-nuclear-reset` | **ДЕСТРУКТИВНО:** Видаляє томи, повторно запускає міграції |

**Безпека міграції:**
- Завжди робіть резервну копію перед міграцією
- Спочатку протестуйте на staging середовищі
- Міграції сумісні з async (використовує `asyncpg`)
- URL БД повинен використовувати схему `postgresql+asyncpg://`

### Перевірка здоров'я сервісу

#### Конфігурація перевірки здоров'я

| Сервіс | Тип перевірки | Кінцева точка/Команда | Інтервал | Timeout | Повторення | Період запуску |
|---------|----------|-----------|----------|---------|----------|--------|
| **postgres** | Команда shell | `pg_isready -U postgres -d tasktracker` | 10s | 5s | 5 | 10s |
| **nats** | Команда shell | `nats-server --version` | 10s | 5s | 3 | 10s |
| **worker** | Перевірка процесу | `pgrep -f 'taskiq worker'` | 30s | 10s | 3 | 40s |
| **api** | HTTP кінцева точка | `curl -f http://localhost:8000/api/health` | 5s | 3s | 3 | 15s |
| **dashboard** | HTTP кінцева точка | `node -e "require('http').get(...)"` | 5s | 3s | 3 | 20s |
| **nginx** | HTTP кінцева точка | `wget http://127.0.0.1/nginx-health` | 30s | 10s | 3 | 10s |

#### Ручна перевірка здоров'я

| Сервіс | Команда ручної перевірки | Здорова відповідь |
|---------|-----------|---------|
| **postgres** | `docker exec task-tracker-postgres pg_isready -U postgres -d tasktracker` | `tasktracker - accepting connections` |
| **nats** | `curl http://localhost:8222/varz` | JSON з `"connections": N` |
| **worker** | `docker exec task-tracker-worker pgrep -f 'taskiq worker'` | Номер ID процесу |
| **api** | `curl http://localhost:8000/api/health` | `{"status": "ok"}` |
| **nginx** | `curl http://localhost/nginx-health` | HTTP 200 OK |

---

## Моніторинг та спостережуваність

### Кінцеві точки перевірки здоров'я

| Кінцева точка | Сервіс | Час відповіді | Призначення | Аутентифікація |
|----------|--------|-----------|---------|--------|
| `/` | api | <50ms | Основна перевірка здоров'я (назва додатка + статус) | Немає |
| `/api/health` | api | <100ms | Структурована відповідь здоров'я з міткою часу | Немає |
| `/nginx-health` | nginx | <10ms | Кінцева точка здоров'я, специфічна для Nginx | Немає |
| `/varz` | nats:8222 | <50ms | Метрики NATS (з'єднання, повідомлення, байти) | Немає (лише dev) |

### Моніторинг подій WebSocket

**Кінцева точка з'єднання:** `ws://<host>/ws?topics=<теми-розділені-комами>`

#### Доступні теми

| Тема | Подія | Частота трансляції | Сценарій використання |
|------|-------|-------------|----------|
| `agents` | `created`, `updated`, `deleted` | При зміні | Оновлення конфігурації агента |
| `tasks` | `created`, `updated`, `status_changed` | При зміні | Відстеження життєвого циклу завдання |
| `providers` | `created`, `updated`, `deleted`, `validated` | При зміні | Конфігурація постачальника LLM |
| `analysis` | `run_started`, `run_progress`, `run_completed`, `run_failed` | Під час запуску | Моніторинг запуску аналізу |
| `proposals` | `created`, `approved`, `rejected` | При розгляді | Робочий процес рецензування пропозиції |
| `messages` | `message.updated`, `ingestion.*` | При введенні | Подія обробки повідомлення |
| `experiments` | `experiment_started`, `experiment_progress`, `experiment_completed` | Під час експерименту | Експерименти класифікації |
| `knowledge` | `extraction_started`, `extraction_completed`, `topic_created`, `atom_created`, `version_created` | Під час видобування | Трубопровід видобування знань |
| `noise_filtering` | `message_scored`, `batch_scored` | Під час оцінки | Подія оцінки важливості |

**Загальні теми:** 9 основних категорій з 14+ окремими типами подій

#### Моніторинг з'єднання

| Метрика | Метод моніторингу | Місцезнаходження |
|--------|--------|----------|
| Активні з'єднання | Логи API | `docker logs task-tracker-api | grep "WebSocket clients"` |
| Кількість з'єднань на тему | Менеджер WebSocket | Не виявляється через API |
| Загальна кількість унікальних з'єднань | Менеджер WebSocket | Не виявляється через API |
| Подія розривання | Логи API | `docker logs task-tracker-api | grep "WebSocket.*disconnect"` |

**Приклад виходу журналу:**
```
📡 Broadcasting message.updated to 3 WebSocket clients
```

### Моніторинг фонового завдання TaskIQ

#### Типи завдань та продуктивність

| Завдання | Призначення | Розмір за замовчуванням | Типова тривалість | Примітки продуктивності |
|------|---------|----------|-----------|---------|
| `process_message` | Приклад завдання | N/A | <100ms | Завдання тестування/демо |
| `save_telegram_message` | Збереження повідомлення Telegram | 1 | 200-500ms | Запускає завантаження аватара |
| `ingest_telegram_messages_task` | Пакетне введення з чатів | 100 | 10-30s | Обмежено API Telegram |
| `execute_analysis_run` | Координація запуску аналізу | Послідовно | 30-300s | Залежить від партії |
| `execute_classification_experiment` | Класифікація теми | Змінна | 20-120s | Випадкова вибірка |
| `embed_messages_batch_task` | Вбудовування повідомлення | 100 | 5-15s | Затримка OpenAI/Ollama API |
| `embed_atoms_batch_task` | Вбудовування атома | 100 | 5-15s | Затримка OpenAI/Ollama API |
| `score_message_task` | Оцінка одного повідомлення | 1 | 100-300ms | Асинхронний розрахунок важливості |
| `score_unscored_messages_task` | Пакетна оцінка | 100 | 10-30s | Послідовна обробка |
| `extract_knowledge_from_messages_task` | Видобування знань LLM | 10-50 | 30-90s | Більші партії = кращий контекст |

#### Команди моніторингу завдань

| Мета моніторингу | Команда | Інтерпретація |
|--------|---------|---------|
| Здоров'я Worker | `docker exec task-tracker-worker pgrep -f 'taskiq worker'` | Повертає PID якщо запущено |
| Логи виконання завдання | `docker logs -f task-tracker-worker | grep "Starting"` | Показує запуски завдань |
| Завершення завдання | `docker logs task-tracker-worker | grep "completed"` | Статистика успіху/невдачі |
| Глибина черги NATS | `curl http://localhost:8222/varz | jq '.in_msgs'` | Загальна кількість оброблених повідомлень |

**Приклад логу Worker:**
```
Starting batch embedding task: 150 messages with provider abc123
Batch embedding task completed: 148 success, 2 failed, 0 skipped
```

### Моніторинг з'єднання з базою даних

#### Конфігурація пулу з'єднань

| Параметр | Значення за замовчуванням | Джерело | Вплив налаштування |
|----------|-----------|--------|----------|
| Розмір пула | 5 | За замовчуванням SQLAlchemy | Збільшите для високої паралелізму |
| Max Overflow | 10 | За замовчуванням SQLAlchemy | Додаткові з'єднання за межами пула |
| Pool Timeout | 30s | За замовчуванням SQLAlchemy | Час очікування з'єднання |
| Pool Recycle | Немає | За замовчуванням SQLAlchemy | Увімкніть для довгоживучих з'єднань |

#### Запити активного з'єднання

| Запит | Призначення | Очікуваний діапазон |
|-------|---------|--------|
| `SELECT count(*) FROM pg_stat_activity;` | Загальна кількість з'єднань | 5-15 (API + worker) |
| `SELECT pid, usename, application_name, state, query FROM pg_stat_activity WHERE state != 'idle';` | Активні запити | 0-5 під час навантаження |
| `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state = 'active' ORDER BY duration DESC;` | Довгозапущені запити | Жодних, що перевищують 5s |

### Агрегація журналів

#### Місцезнаходження та формати журналів

| Сервіс | Вихід журналу | Формат | Управління рівнем |
|---------|-----------|--------|----------|
| **Backend** | stdout/stderr | Loguru (кольоровий з мітками часу) | Змінна середовища `LOG_LEVEL` |
| **Worker** | stdout/stderr | Loguru (кольоровий з мітками часу) | Змінна середовища `LOG_LEVEL` |
| **Nginx** | `/var/log/nginx/access.log`, `/var/log/nginx/error.log` | Комбінований формат | nginx.conf |
| **PostgreSQL** | stdout/stderr | За замовчуванням PostgreSQL | Конфіг PostgreSQL |
| **Dashboard** | stdout/stderr | Сервер розробки Vite | N/A |

#### Команди перегляду журналів

| Призначення | Команда | Вихід |
|---------|---------|--------|
| Усі сервіси | `docker compose logs -f` | Чергові журналі з усіх контейнерів |
| Специфічний сервіс | `docker logs -f task-tracker-api` | Журналі в реал-часі для API |
| Останні N рядків | `docker logs --tail 100 task-tracker-worker` | Останні 100 рядків від worker |
| Фільтрація за ключовим словом | `docker logs task-tracker-api | grep "WebSocket"` | Журналі, пов'язані лише з WebSocket |
| Лише журналі помилок | `docker logs task-tracker-worker | grep "ERROR"` | Журналі рівня помилки |

**Рівні журналів (Backend):**
- За замовчуванням: `INFO`
- Варіанти: `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`
- Встановіть через: змінну середовища `LOG_LEVEL` або `LOGURU_LEVEL`

**Рекомендація для продакшену:** Інтегруйте з Loki, ELK Stack або CloudWatch Logs для централізованої агрегації.

### Метрики продуктивності (поточний стан)

**Без вбудованої колекції метрик** - Prometheus, StatsD або інтеграція OpenTelemetry не налаштовані.

#### Доступні джерела даних

| Джерело | Доступні метрики | Метод доступу |
|--------|--------|----------|
| **Логи доступу Nginx** | Затримка запиту, коди стану HTTP, швидкості запитів | Розбір логів у `/var/log/nginx/access.log` |
| **Docker Stats** | CPU, пам'ять, мережа I/O на контейнер | `docker stats <container>` |
| **Статистика PostgreSQL** | Розміри таблиць, використання індексу, продуктивність запиту | Системні вради `pg_stat_*` |
| **Логи TaskIQ** | Часи виконання завдань | Логи grep для `execution_time_ms` |

**Рекомендація для продакшену:** Реалізуйте Prometheus + Grafana для колекції та візуалізації метрик.

---

## Посібник усунення неполадок

### Сценарій 1: Сервіси не запускаються після розгортання

**Симптоми:**
- `docker compose up -d` не вдається
- Контейнери виходять негайно після запуску
- Перевірки здоров'я ніколи не проходять

**Етапи діагностики:**

| Крок | Команда | На що звернути увагу |
|------|---------|----------|
| 1 | `docker compose ps` | Які сервіси відмовляють |
| 2 | `docker compose logs postgres nats worker api dashboard nginx` | Повідомлення про помилки в журналах |
| 3 | `sudo lsof -i :5555 -i :4222 -i :8000 -i :3000 -i :80` | Конфлікти портів |
| 4 | `ls -la .env` | Файл .env існує та доступний |
| 5 | `docker compose config` | Помилки синтаксису файлу Compose |

**Загальні причини та розв'язання:**

| Причина | Виявлення | Розв'язання |
|--------|-----------|--------|
| Порт уже використовується | `lsof` показує процес на порту | Зупиніть конфліктний процес або змініть порти |
| Відсутній файл .env | `ls .env` не вдається | Скопіюйте `.env.example` на `.env` та налаштуйте |
| Міграція БД не вдалася | Логи API показують помилку міграції | Запустіть `just alembic-up` вручну |
| Проблема з Docker daemon | `docker ps` не вдається | Перезапустіть Docker daemon |
| Помилка дозволу тому | Журналі показують "Permission denied" | Виправте дозволи тому або пересворіть томи |

**Етапи розв'язання:**

1. Зупиніть усі сервіси: `just services-stop`
2. Очистіть контейнери: `docker compose down`
3. Перебудуйте без кешу: `docker compose build --no-cache`
4. Перевірте наявність та правильність файлу .env
5. Запустіть інфраструктуру спочатку: `docker compose up -d postgres nats`
6. Почекайте на перевірки здоров'я (30s)
7. Застосуйте міграції: `just alembic-up`
8. Запустіть решту сервісів: `docker compose up -d worker api dashboard nginx`

### Сценарій 2: WebSocket з'єднання часто розривається

**Симптоми:**
- Фронтенд показує стан "переконнекція"
- Оновлення в реал-часі затримані або відсутні
- Консоль браузера показує помилки WebSocket

**Етапи діагностики:**

| Крок | Команда | Очікуваний вихід |
|------|---------|----------|
| 1 | Browser DevTools → Network → WS | Статус з'єднання (open/closed) |
| 2 | `docker logs task-tracker-api | grep "WebSocket"` | Подія з'єднання/розривання |
| 3 | `docker logs task-tracker-nginx | grep "/ws"` | WebSocket запити проксі |
| 4 | `docker logs --since 10m task-tracker-api | grep -i "error\|exception"` | Недавні помилки API |

**Загальні причини та розв'язання:**

| Причина | Виявлення | Розв'язання |
|--------|-----------|--------|
| Timeout Nginx | З'єднання розривається після 60s | Перевірте, що `/ws` має `proxy_read_timeout 86400` |
| Перезапуск API | Усі з'єднання одночасно втрачені | Нормально під час розгортання, фронтенд повинен переконектитися |
| Проблема мережі клієнта | Перебійні розриви | Перевірте стабільність мережі клієнта |
| Браузер призупиняє вкладку | Розрив при неактивній вкладці | Реалізуйте механізм heartbeat/ping |

**Етапи розв'язання:**

1. Перевірте конфіг Nginx включає WebSocket timeout
2. Перезапустіть Nginx: `docker restart task-tracker-nginx`
3. Перевірте стабільність API: `docker logs --since 10m task-tracker-api | grep -i error`
4. Перевірте логіку переконнекції фронтенду (макс 5 спроб, експоненціальна затримка)
5. Контролюйте кількість з'єднань: `docker logs task-tracker-api | grep "connection_count"`

### Сценарій 3: Фонові завдання не обробляються

**Симптоми:**
- Завдання застрягають у стані очікування
- Журналі Worker мовчать (виконання завдань відсутнє)
- NATS недоступний

**Етапи діагностики:**

| Крок | Команда | На що звернути увагу |
|------|---------|----------|
| 1 | `docker ps --filter "name=worker"` | Worker контейнер запущено |
| 2 | `docker logs --tail 100 task-tracker-worker` | Помилки або збої Worker |
| 3 | `docker exec task-tracker-worker ping -c 3 nats` | З'єднання NATS |
| 4 | `curl http://localhost:8222/varz` | Статус NATS та з'єднання |
| 5 | `docker inspect task-tracker-worker | jq '.[0].State.OOMKilled'` | Убиття через нестачу пам'яті |

**Загальні причини та розв'язання:**

| Причина | Виявлення | Розв'язання |
|--------|-----------|--------|
| Брокер NATS вниз | `ping nats` не вдається | Перезапустіть NATS: `docker compose restart nats` |
| Worker збій (OOM) | `OOMKilled: true` | Збільшите обмеження пам'яті у `compose.yml` |
| Вичерпаний пул з'єднання БД | Журналі показують помилки з'єднання | Перезапустіть worker для скидання пула |
| Помилка коду завдання | Журналі показують трасування винятку | Виправте код, перебудуйте: `just rebuild worker` |

**Етапи розв'язання:**

1. Перезапустіть NATS та worker: `docker compose restart nats worker`
2. Перевірте убивання OOM: `docker inspect task-tracker-worker | jq '.[0].State.OOMKilled'`
3. Якщо OOM убитий, збільшите обмеження пам'яті до 4GB у `compose.yml`
4. Перевірте з'єднання NATS: `docker exec task-tracker-worker ping -c 3 nats`
5. Перевірте журналі worker на винятки: `docker logs task-tracker-worker | grep "ERROR"`
6. Якщо дані NATS пошкоджені, видаліть том: `docker volume rm task-tracker-nats-data` (пересворюється при перезапуску)

### Сценарій 4: Невдачі міграції бази даних

**Симптоми:**
- `just alembic-up` не вдається
- API не запускається через невідповідність схеми
- Конфлікти версії Alembic

**Етапи діагностики:**

| Крок | Команда | Очікуваний вихід |
|------|---------|----------|
| 1 | `uv run alembic current` | Поточна ревізія міграції |
| 2 | `uv run alembic history` | Історія міграції без конфліктів |
| 3 | `uv run alembic heads` | Одна ревізія голови (без гілок) |
| 4 | `docker logs task-tracker-postgres | grep "ERROR"` | Помилки БД під час міграції |

**Загальні причини та розв'язання:**

| Причина | Виявлення | Розв'язання |
|--------|-----------|--------|
| Конфлікти схеми | Ручні зміни в БД | Штамп до поточного: `uv run alembic stamp head` |
| Кілька голів | `alembic heads` показує 2+ ревізії | Об'єднайте голови: `uv run alembic merge heads` |
| База даних заблокована | Міграція зависає | Убийте блокуючі запити через `pg_terminate_backend` |
| Міграція залежить від відсутніх даних | Міграція не вдається з помилкою даних | Засійте потрібні дані або виправте міграцію |

**Етапи розв'язання:**

1. Визначте поточний стан: `uv run alembic current`
2. Перевірте кілька голів: `uv run alembic heads`
3. Якщо кілька голів, об'єднайте: `uv run alembic merge heads`
4. Якщо схема збігається, але версія невірна: `uv run alembic stamp head`
5. Відкатіть невдалу міграцію: `uv run alembic downgrade -1`
6. Ядерна опція (ДЕСТРУКТИВНО): `just db-nuclear-reset`

**Ручна резервна копія перед міграцією:**

Завжди робіть резервну копію перед міграцією (автоматична резервна копія не налаштована):
```
docker exec task-tracker-postgres pg_dump -U postgres tasktracker > backup_$(date +%Y%m%d_%H%M%S).sql
```

### Сценарій 5: Висока використання пам'яті / убивання OOM

**Симптоми:**
- Сервіси часто перезапускаються
- Docker stats показує пам'ять на обмеженні
- `OOMKilled: true` під час перевірки контейнера

**Етапи діагностики:**

| Крок | Команда | На що звернути увагу |
|------|---------|----------|
| 1 | `docker stats --no-stream` | Поточне використання пам'яті на контейнер |
| 2 | `docker ps -a | grep "Exited"` | Недавно закриті контейнери |
| 3 | `docker inspect <container> | jq '.[0].State.OOMKilled'` | Статус убивання OOM |
| 4 | `docker inspect <container> | jq '.[0].HostConfig.Memory'` | Обмеження пам'яті |

**Загальні причини та розв'язання:**

| Причина | Виявлення | Розв'язання |
|--------|-----------|--------|
| Велика обробка партій | Спайки пам'яті Worker під час завдань | Зменшите розміри партій у конфігурації завдання |
| Витік пам'яті | Пам'ять зростає з часом | Профілюйте код, перезапустіть сервіс |
| Недостатньо обмежень | Пам'ять на 100% обмеження | Збільшите обмеження у `compose.yml` |
| Занадто багато паралельних завдань | Кілька важких завдань одночасно | Реалізуйте черження завдань/обмеження частоти |

**Етапи розв'язання:**

1. Перевірте поточне використання: `docker stats --no-stream`
2. Визначте контейнери, убиті OOM: `docker inspect task-tracker-worker | jq '.[0].State.OOMKilled'`
3. Збільшите обмеження пам'яті у `compose.yml` (наприклад, worker: 2GB → 4GB)
4. Зменшите розміри партій (embedding: 100 → 50, scoring: 100 → 50)
5. Перезапустіть сервіси: `docker compose down && docker compose up -d`
6. Контролюйте час: `watch -n 5 'docker stats --no-stream task-tracker-worker'`

**Місця налаштування розміру партії:**

- Вбудовування повідомлення: `app/tasks.py` (параметр batch_size)
- Видобування знань: `app/tasks.py` (параметр limit)
- Оцінка повідомлення: `app/tasks.py` (параметр limit)

---

## Налаштування продуктивності

### Рекомендації щодо розміру партії

#### Поточна конфігурація проти варіантів налаштування

| Завдання | Поточний розмір партії | Мала пам'ять (512MB) | Збалансовано (2GB) | Висока пропускна спроможність (4GB+) |
|------|-----------|-----------|-----------|---------|
| **Введення Telegram** | 100 | 50 | 100 | 100 (обмеження API) |
| **Видобування знань** | 50 | 20 | 50 | 100 |
| **Генерування вбудовування** | 100 | 50 | 100 | 500 |
| **Оцінка повідомлення** | 100 | 50 | 100 | 500 |
| **Експеримент класифікації** | Змінна | Зменшите на 50% | Як налаштовано | Збільшите на 2x |

**Принципи налаштування:**
- Вищі партії = швидша пропускна спроможність, більше використання пам'яті
- Нижчі партії = повільніша пропускна спроможність, менше використання пам'яті, краща ізоляція помилок
- Обмеження вбудовування OpenAI: 2048 елементів на партію (збільшується безпечно до 500)
- Видобування знань: Більші партії поліпшують якість контексту LLM

### Оптимізація запиту бази даних

#### Гарячі точки продуктивності та рекомендовані індекси

| Шаблон запиту | Поточний стан | Рекомендований індекс | Вплив на продуктивність |
|-----------|-----------|-------------|-----------|
| **Фільтрування повідомлення** (`WHERE topic_id IS NULL`) | Послідовне сканування | `CREATE INDEX idx_message_topic_id_sent_at ON message(topic_id, sent_at) WHERE topic_id IS NULL` | 10-100x швидше |
| **Випадкова вибірка** (`ORDER BY random()`) | Повне сканування таблиці | Використовуйте `TABLESAMPLE BERNOULLI(10)` замість | 50-500x швидше |
| **Відстань оцінки важливості** (`WHERE importance_score IS NULL`) | Послідовне сканування | `CREATE INDEX idx_message_importance_score_sent_at ON message(importance_score, sent_at DESC)` | 10-100x швидше |

#### Моніторинг продуктивності запиту

| Дія | Команда | Призначення |
|------|---------|---------|
| Увімкніть реєстрування повільних запитів | `ALTER DATABASE tasktracker SET log_min_duration_statement = 1000` | Реєструйте запити, що перевищують 1s |
| Переглядайте повільні запити | `SELECT query, mean_exec_time, calls FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10` | Визначте цілі оптимізації |
| Аналізуйте план запиту | `EXPLAIN ANALYZE <query>` | Зрозумійте виконання запиту |

### Продуктивність пошуку векторів (pgvector)

#### Варіанти конфігурації індексу

| Тип індексу | Сценарій використання | Використання пам'яті | Швидкість запиту | Час збірки |
|----------|----------|----------|---------|----------|
| **HNSW** | Пошук високої швидкості | Високе (3-5x розмір даних) | Швидко (під 200ms) | Швидко |
| **IVFFlat** | Середовища з обмеженою пам'яттю | Низьке (1-2x розмір даних) | Помірно (200-500ms) | Помірно |

**Поточна конфігурація:** HNSW з подібністю косинусу

#### Параметри налаштування

| Параметр | За замовчуванням | Низька точність | Збалансовано | Висока точність |
|----------|---------|----------|----------|---------|
| `VECTOR_SIMILARITY_THRESHOLD` | 0.7 | 0.6 | 0.7 | 0.85 |
| `VECTOR_SEARCH_LIMIT` | 10 | 20 | 10 | 5 |
| `EMBEDDING_BATCH_SIZE` | 100 | 50 | 100 | 500 |

**Компроміси налаштування:**
- Вищий поріг = менше результатів, вищої якості збіги
- Нижчий поріг = більше результатів, нижченої якості збіги
- Вищий ліміт = більше результатів пошуку, повільніші запити
- Вищий розмір партії = швидше масове вбудовування, більше використання API

### Налаштування обмеження ресурсів

#### Коли збільшувати обмеження

| Симптом | Сервіс | Поточне обмеження | Рекомендоване збільшення |
|--------|--------|----------|----------|
| Убивання OOM під час пакетних завдань | worker | 2GB RAM | 4GB RAM |
| Повільна продуктивність запиту | postgres | 2GB RAM | 4GB RAM |
| Висока затримка чекання CPU | api | 1 CPU | 2 CPU |
| Затримка черги NATS | nats | 512MB RAM | 1GB RAM |

#### Коли зменшувати розміри партій

| Симптом | Поточний розмір партії | Рекомендоване скорочення | Компроміс |
|--------|-----------|-------------|----------|
| Worker OOM під час вбудовування | 100 | 50 | 2x повільніша пропускна спроможність |
| Timeout API для масових операцій | 100 | 50 | Дольша загальна обробка |
| Взаємоблокування бази даних | N/A | Додайте черження завдань | Краща контроль паралелізму |

---

## Процедури відкату

### Відкат додатка

#### Стандартні етапи відкату

| Крок | Команда | Тривалість | Перевірка |
|------|---------|----------|---------|
| 1 | `just services-stop` | 10-15s | Усі контейнери зупинено |
| 2 | `git checkout <попередній-коміт>` | 2s | Код відновлено |
| 3 | `docker compose build --no-cache` | 2-5 min | Образи перебудовані |
| 4 | `just services` | 60-100s | Сервіси здорові |
| 5 | `docker ps --filter "name=task-tracker" --format "table {{.Names}}\t{{.Status}}"` | 2s | Усі сервіси "Up (healthy)" |

**Загальний час відкату:** 4-7 хвилин

### Відкат бази даних

#### Етапи відкату міграції

| Крок | Команда | Призначення | Примітка безпеки |
|------|---------|---------|---------|
| 1 | `uv run alembic current` | Визначте поточну ревізію | Немає |
| 2 | `uv run alembic downgrade -1` | Відкатіть одну ревізію | **ДЕСТРУКТИВНО** якщо міграція видаляє дані |
| 3 | `docker restart task-tracker-api task-tracker-worker` | Перезапустіть сервіси додатка | Переконнектується до БД |

**Обмеження відкату:**
- Міграції, які видаляють колони/таблиці, не можуть бути повністю відновлені, якщо були присутні дані
- Завжди робіть резервну копію перед міграцією
- Протестуйте відкат на staging спочатку

### Сценарії відновлення даних

#### Сценарій 1: Випадкове видалення даних

**Попередження:**
- Автоматизовані резервні копії БД не налаштовані (потрібна ручна резервна копія)
- Рекомендація: Увімкніть архівацію WAL PostgreSQL для відновлення у певний момент часу

**Етапи відновлення:**

| Крок | Команда | Час відновлення | Вікно втрати даних |
|------|---------|----------|----------|
| 1 | Зупиніть сервіси | `docker compose down` | N/A |
| 2 | Відновіть із резервної копії | `docker exec -i task-tracker-postgres psql -U postgres tasktracker < backup.sql` | 5-30 хв | З моменту останньої резервної копії |
| 3 | Перезапустіть сервіси | `just services` | 60-100s | N/A |

#### Сценарій 2: Пошкодження бази даних

**Симптоми:**
- `FATAL: database is corrupted`
- `ERROR: invalid page header`

**Етапи відновлення:**

| Крок | Команда | Призначення |
|------|---------|---------|
| 1 | `docker compose down` | Зупиніть усі сервіси |
| 2 | `docker volume rm task-tracker-postgres-data` | Видаліть пошкоджений том |
| 3 | `docker compose up -d postgres` | Запустіть чисту postgres |
| 4 | `docker exec -i task-tracker-postgres psql -U postgres -c "CREATE DATABASE tasktracker"` | Створіть БД |
| 5 | `docker exec -i task-tracker-postgres psql -U postgres tasktracker < backup.sql` | Відновіть із резервної копії |
| 6 | `just services` | Запустіть усі сервіси |

**Час відновлення:** 10-45 хвилин (залежить від розміру БД)

#### Сценарій 3: Відновлення невдалого фонового завдання

**Симптоми:**
- Завдання введення застрягло в стані "запущено"
- Завдання не завершуються незважаючи на здоровий worker

**Етапи відновлення:**

| Крок | Команда | Призначення |
|------|---------|---------|
| 1 | Визначте застрягле завдання | `docker logs task-tracker-worker | grep "Ingestion job"` | Знайдіть ID завдання |
| 2 | Скиніть статус завдання | `docker exec -i task-tracker-postgres psql -U postgres tasktracker -c "UPDATE message_ingestion_job SET status = 'failed', error_log = '{\"error\": \"Manual reset\"}' WHERE status = 'running' AND started_at < NOW() - INTERVAL '1 hour'"` | Позначте старі завдання як невдалі |
| 3 | Перезапустіть worker | `docker restart task-tracker-worker` | Очистіть стан worker |
| 4 | Повторіть через API | Запустіть нове завдання введення | Возобновите обробку |

**Без втрати даних:** Завдання можуть бути безпечно повторені (ідемпотентні операції)

---

## Контрольний список розгортання продакшену

### Перед розгортанням

- [ ] Резервна копія БД завершена: `pg_dump > backup_$(date +%Y%m%d_%H%M%S).sql`
- [ ] Сценарії міграції переглянуті: `uv run alembic history`
- [ ] Міграції протестовані на staging
- [ ] Docker образи збудовані: `docker compose build --no-cache`
- [ ] Випуск мітки Git: `git tag -a v1.0.0 -m "Release 1.0.0"`
- [ ] Змінні середовища оновлені (файл .env)
- [ ] Секрети повернені за потреби
- [ ] Видаліть порт моніторингу NATS (8222) зі зовнішнього доступу

### Розгортання

- [ ] Сервіси зупинено: `just services-stop`
- [ ] Код оновлено: `git pull origin main`
- [ ] Міграції застосовані: `just alembic-up`
- [ ] Інфраструктура запущена: `docker compose up -d postgres nats`
- [ ] Інфраструктура здорова (очікування 30s): `docker compose ps`
- [ ] Додаток запущено: `docker compose up -d worker api dashboard nginx`
- [ ] Здоров'я API перевірено: `curl http://localhost/api/health`

### Перевірка після розгортання

- [ ] Статус сервісу: `docker compose ps` (усі здорові)
- [ ] Здоров'я API: `curl http://localhost/api/health` (HTTP 200)
- [ ] З'єднання WebSocket: Browser DevTools → Network → WS (підключено)
- [ ] Логи Worker: `docker logs task-tracker-worker | tail -50` (без помилок)
- [ ] Використання пам'яті: `docker stats --no-stream` (в межах обмежень)
- [ ] Обробка фонових завдань (створіть тестовий запуск аналізу)
- [ ] Міграція БД: `uv run alembic current` (показує останню ревізію)
- [ ] Логи доступу Nginx: `docker exec task-tracker-nginx tail -50 /var/log/nginx/access.log`

### Процедура відкату (якщо виникли проблеми)

- [ ] Зупиніть сервіси: `just services-stop`
- [ ] Перейдіть до попередної версії: `git checkout <попередня-мітка>`
- [ ] Відкатіть міграцію: `uv run alembic downgrade -1`
- [ ] Перебудуйте контейнери: `docker compose build --no-cache`
- [ ] Запустіть сервіси: `just services`
- [ ] Відновіть БД за потреби: `docker exec -i task-tracker-postgres psql -U postgres tasktracker < backup.sql`

### Налаштування моніторингу (Продакшен)

- [ ] Налаштуйте скреперинг Prometheus (рекомендовано)
- [ ] Імпортуйте дашборди Grafana (рекомендовано)
- [ ] Налаштуйте правила сповіщень для критичних збоїв (рекомендовано)
- [ ] Увімкніть агрегацію журналів (Loki/ELK) (рекомендовано)
- [ ] Налаштуйте моніторинг часу роботи (Pingdom/StatusCake) (рекомендовано)
- [ ] Увімкніть архівацію WAL PostgreSQL (відновлення у певний момент часу) (рекомендовано)

---

## Пов'язана документація

- **Управління конфігурацією:** Див. документацію Agent 2B (безпека, змінні середовища)
- **Архітектура фронтенду:** Див. `frontend/architecture.md`
- **Документація API:** Див. каталог `api/`
- **Фонові завдання:** Див. `architecture/background-tasks.md`

---

*Цей посібник розгортання забезпечує комплексні операційні процедури для команд DevOps та SRE. Для деталей безпеки та конфігурації звертайтесь до документації управління конфігурацією.*
