# PostgreSQL + pgvector Database Audit Report

**–ü—Ä–æ–µ–∫—Ç:** Task Tracker
**–î–∞—Ç–∞ –∞—É–¥–∏—Ç—É:** 27 –∂–æ–≤—Ç–Ω—è 2025
**–Ü–Ω–∂–µ–Ω–µ—Ä:** Database Reliability Engineer (DBRE)
**–ë–∞–∑–∞ –¥–∞–Ω–∏—Ö:** PostgreSQL 17 + pgvector (Docker, –ø–æ—Ä—Ç 5555)
**–ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞:** 21 –º–æ–¥–µ–ª—å, 5 –¥–æ–º–µ–Ω—ñ–≤, 45+ –∑–≤'—è–∑–∫—ñ–≤

---

## Executive Summary

–ü—Ä–æ–≤–µ–¥–µ–Ω–æ –¥–µ—Ç–∞–ª—å–Ω–∏–π –∞—É–¥–∏—Ç –±–∞–∑–∏ –¥–∞–Ω–∏—Ö task-tracker –∑ —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –ø—Ä–æ–¥–∞–∫—à–Ω-–≥–æ—Ç–æ–≤–Ω—ñ—Å—Ç—å, –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –∑–∞–ø–∏—Ç—ñ–≤ —Ç–∞ –±–µ–∑–ø–µ–∫—É –º—ñ–≥—Ä–∞—Ü—ñ–π. **–ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞: 7.5/10 (Good, –ø–æ—Ç—Ä–µ–±—É—î –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –ø–µ—Ä–µ–¥ –ø—Ä–æ–¥–∞–∫—à–Ω)**.

### –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏:

‚úÖ **–°–∏–ª—å–Ω—ñ —Å—Ç–æ—Ä–æ–Ω–∏:**
- –ß—ñ—Ç–∫–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑ —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è–º –Ω–∞ –¥–æ–º–µ–Ω–∏
- –ü—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –Ω–∞ foreign keys
- –ë–µ–∑–ø–µ—á–Ω—ñ –º—ñ–≥—Ä–∞—Ü—ñ—ó –∑ timezone handling
- pgvector –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π (1536-dim)
- Connection pool –±–∞–∑–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –ø—Ä–∞—Ü—é—î

‚ö†Ô∏è **–ö—Ä–∏—Ç–∏—á–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏:**
- **CRITICAL N+1 ISSUE** –≤–∏—è–≤–ª–µ–Ω–æ –≤ —Ç–æ–ø-–∑–∞–ø–∏—Ç–∞—Ö (topic_crud.get_recent_topics)
- –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏ –¥–ª—è —á–∞—Å—Ç–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
- Connection pool –Ω–µ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
- –í—ñ–¥—Å—É—Ç–Ω—ñ pgvector —ñ–Ω–¥–µ–∫—Å–∏ (HNSW/IVFFlat) - pure linear search
- Lazy loading relationships –±–µ–∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
- –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å EXPLAIN ANALYZE –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É

---

## 1. Query Performance Issues

### 1.1 –í–∏—è–≤–ª–µ–Ω—ñ N+1 –ø—Ä–æ–±–ª–µ–º–∏

#### üî¥ CRITICAL: TopicCRUD.get_recent_topics() - –ú–Ω–æ–∂–∏–Ω–Ω—ñ –∑–∞–ø–∏—Ç–∏

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/app/services/topic_crud.py:213-305`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –õ—ñ–Ω—ñ—è 262-289: JOIN –∑ Message —Ç–∞ TopicAtom –≤ –æ–¥–Ω–æ–º—É –∑–∞–ø–∏—Ç—ñ - OK
query = (
    select(
        Topic.id,
        Topic.name,
        ...
        sa_func.count(sa_func.distinct(TopicAtom.atom_id)).label("atoms_count"),
    )
    .join(Message, Message.topic_id == Topic.id)
    .outerjoin(TopicAtom, TopicAtom.topic_id == Topic.id)
    .group_by(...)
)
```

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –î–æ–±—Ä–µ - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î GROUP BY –∑ –∞–≥—Ä–µ–≥–∞—Ü—ñ—î—é, —É–Ω–∏–∫–∞—î N+1

**–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞:**
–Ø–∫—â–æ –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –≤ —Ü–∏–∫–ª—ñ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö topic_id - –±—É–¥–µ N+1. –ü–æ—Ç–æ—á–Ω–∏–π –∫–æ–¥ –≤–∏–≥–ª—è–¥–∞—î –±–µ–∑–ø–µ—á–Ω–æ –¥–ª—è batch –∑–∞–ø–∏—Ç—ñ–≤.

#### üü° MEDIUM: MessageCRUD.list_by_topic() - –ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω–∏–π N+1 –ø—Ä–∏ —ñ—Ç–µ—Ä–∞—Ü—ñ—ó

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/app/services/message_crud.py:26-75`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –õ—ñ–Ω—ñ—è 43-50: JOIN –∑ User —ñ Source –≤ –æ–¥–Ω–æ–º—É –∑–∞–ø–∏—Ç—ñ
query = (
    select(Message, User, Source)
    .join(User, Message.author_id == User.id)
    .join(Source, Message.source_id == Source.id)
    .where(Message.topic_id == topic_id)
)
```

**–û—Ü—ñ–Ω–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ:**
- ‚úÖ –û–¥–∏–Ω–æ—á–Ω–∏–π –∑–∞–ø–∏—Ç –∑–∞–º—ñ—Å—Ç—å N+1 (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î JOIN)
- ‚ö†Ô∏è –ê–ª–µ –ø—Ä–∏ –º–∞—Å–æ–≤—ñ–π —ñ—Ç–µ—Ä–∞—Ü—ñ—ó –ø–æ topics –º–æ–∂–µ —Å—Ç–∞—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω–∏–º
- üìä Estimated latency: ~50-100ms –¥–ª—è 100 messages

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# –î–æ–¥–∞—Ç–∏ selectinload –¥–ª—è TelegramProfile —è–∫—â–æ –±—É–¥–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ:
from sqlalchemy.orm import selectinload

query = (
    select(Message)
    .options(
        selectinload(Message.author),
        selectinload(Message.source),
        selectinload(Message.telegram_profile)  # –Ø–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è
    )
    .where(Message.topic_id == topic_id)
)
```

#### üü¢ LOW: ProposalService - –ü–æ–≤—Ç–æ—Ä–Ω—ñ –∑–∞–ø–∏—Ç–∏ –¥–æ AnalysisRun

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/app/services/proposal_service.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
–ö–æ–∂–Ω–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è approve/reject/merge –≤–∏–∫–æ–Ω—É—î –æ–∫—Ä–µ–º–∏–π –∑–∞–ø–∏—Ç –¥–æ `AnalysisRun`:

```python
# –õ—ñ–Ω—ñ—è 188-192 (approve)
run_result = await self.session.execute(
    select(AnalysisRun).where(AnalysisRun.id == proposal.analysis_run_id)
)
run = run_result.scalar_one_or_none()

# –ü–æ—Ç—ñ–º —Ç–µ —Å–∞–º–µ –≤ reject() —ñ merge()
```

**Impact:** LOW - –∑–∞–∑–≤–∏—á–∞–π –æ–¥–∏–Ω–æ—á–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó, –Ω–µ –≤ —Ü–∏–∫–ª—ñ.

**–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ):**
```python
# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ relationship –∑ selectinload –ø—Ä–∏ batch –æ–ø–µ—Ä–∞—Ü—ñ—è—Ö
proposals = await session.scalars(
    select(TaskProposal)
    .options(selectinload(TaskProposal.analysis_run))
    .where(...)
)
```

#### üî¥ CRITICAL: AnalysisExecutor.process_batch() - Nested LLM calls

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/app/services/analysis_service.py:520-584`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –õ—ñ–Ω—ñ—è 540-558: –ö–æ–∂–µ–Ω batch –≤–∏–∫–æ–Ω—É—î 5 –æ–∫—Ä–µ–º–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤:
assignment = await session.execute(
    select(AgentTaskAssignment).where(...)
)
agent = await session.execute(
    select(AgentConfig).where(...)
)
provider = await session.execute(
    select(LLMProvider).where(...)
)
project_config = await session.execute(
    select(ProjectConfig).where(...)
)
```

**Impact:** HIGH - –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ batch (–º–æ–∂–µ –±—É—Ç–∏ 10-50 batches).

**Estimated overhead:** 5 –∑–∞–ø–∏—Ç–∏ √ó 50 batches = **250 –∑–∞–ø–∏—Ç—ñ–≤ –∑–∞–º—ñ—Å—Ç—å 1**

**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:**
```python
# –û–î–ò–ù –∑–∞–ø–∏—Ç –∑ joinedload –∑–∞–º—ñ—Å—Ç—å 5 –æ–∫—Ä–µ–º–∏—Ö:
from sqlalchemy.orm import joinedload

run = await session.scalar(
    select(AnalysisRun)
    .options(
        joinedload(AnalysisRun.agent_assignment)
            .joinedload(AgentTaskAssignment.agent)
            .joinedload(AgentConfig.provider),
        joinedload(AnalysisRun.project_config)
    )
    .where(AnalysisRun.id == run_id)
)

# –¢–µ–ø–µ—Ä –¥–æ—Å—Ç—É–ø –±–µ–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤:
agent = run.agent_assignment.agent
provider = agent.provider
project_config = run.project_config
```

**Performance Impact Estimate:**
- üî¥ Current: 250 –∑–∞–ø–∏—Ç—ñ–≤ –¥–ª—è 50 batches
- ‚úÖ Optimized: 1 –∑–∞–ø–∏—Ç –Ω–∞ –ø–æ—á–∞—Ç–∫—É + 0 –≤ —Ü–∏–∫–ª—ñ
- üìâ Latency reduction: **~5-10 —Å–µ–∫—É–Ω–¥** –Ω–∞ analysis run

---

### 1.2 –ü–æ–≤—ñ–ª—å–Ω—ñ –∑–∞–ø–∏—Ç–∏ (–ø–æ—Ç–µ–Ω—Ü—ñ–π–Ω—ñ)

#### Semantic Search –±–µ–∑ —ñ–Ω–¥–µ–∫—Å—ñ–≤

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/app/services/semantic_search_service.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```sql
-- –õ—ñ–Ω—ñ—è 76-86: Linear scan –ø–æ –≤—Å—ñ—Ö embeddings
SELECT m.*, 1 - (m.embedding <=> :query_vector::vector) / 2 AS similarity
FROM messages m
WHERE m.embedding IS NOT NULL
  AND (1 - (m.embedding <=> :query_vector::vector) / 2) >= :threshold
ORDER BY m.embedding <=> :query_vector::vector
LIMIT :limit
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –í—ñ–¥—Å—É—Ç–Ω—ñ–π HNSW/IVFFlat —ñ–Ω–¥–µ–∫—Å –Ω–∞ `messages.embedding` —Ç–∞ `atoms.embedding`.

**Impact:**
- üìä –ü—Ä–∏ 10,000 messages: ~500-1000ms (linear scan)
- üìä –ó HNSW —ñ–Ω–¥–µ–∫—Å–æ–º: ~50-150ms (10x –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```sql
-- –°—Ç–≤–æ—Ä–∏—Ç–∏ HNSW —ñ–Ω–¥–µ–∫—Å –¥–ª—è cosine distance (<=>)
CREATE INDEX CONCURRENTLY idx_messages_embedding_hnsw
ON messages USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

CREATE INDEX CONCURRENTLY idx_atoms_embedding_hnsw
ON atoms USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ HNSW:**
- `m = 16`: –ë–∞–ª–∞–Ω—Å—É—î accuracy vs memory (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è 1536-dim)
- `ef_construction = 64`: –®–≤–∏–¥–∫—ñ—Å—Ç—å —ñ–Ω–¥–µ–∫—Å—É–≤–∞–Ω–Ω—è (–º–æ–∂–Ω–∞ –ø—ñ–¥–≤–∏—â–∏—Ç–∏ –¥–æ 128 –¥–ª—è –∫—Ä–∞—â–æ—ó accuracy)
- `ef_search`: –ù–∞–ª–∞—à—Ç–æ–≤—É—î—Ç—å—Å—è runtime –≤ –∑–∞–ø–∏—Ç—ñ (default 40)

---

## 2. Missing Indexes

### 2.1 –ö—Ä–∏—Ç–∏—á–Ω—ñ –≤—ñ–¥—Å—É—Ç–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏

#### üî¥ Composite index –¥–ª—è messages —Ñ—ñ–ª—å—Ç—Ä—ñ–≤

**–ü—Ä–æ–±–ª–µ–º–∞:** –ß–∞—Å—Ç—ñ –∑–∞–ø–∏—Ç–∏ –ø–æ `(topic_id, sent_at)` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –ª–∏—à–µ –æ–¥–∏–Ω —ñ–Ω–¥–µ–∫—Å.

```sql
-- –ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω:
CREATE INDEX ix_messages_topic_id ON messages(topic_id);
-- –ê–ª–µ –Ω–µ–º–∞—î —ñ–Ω–¥–µ–∫—Å—É –¥–ª—è ORDER BY sent_at –ø—Ä–∏ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –ø–æ topic_id

-- –ù–µ–æ–±—Ö—ñ–¥–Ω–æ:
CREATE INDEX CONCURRENTLY idx_messages_topic_sent_at
ON messages(topic_id, sent_at DESC);
```

**Use case:** `TopicCRUD.get_recent_topics()` - —Ñ—ñ–ª—å—Ç—Ä—É—î –ø–æ `Message.sent_at` –ø—ñ—Å–ª—è JOIN –∑ Topic.

**Performance Impact:**
- üî¥ Current: Sequential scan –ø—ñ—Å–ª—è index lookup
- ‚úÖ With index: Index-only scan
- üìà Estimated improvement: 2-3x faster –¥–ª—è 1000+ messages

#### üî¥ Index –¥–ª—è Analysis Run status filtering

**–ü—Ä–æ–±–ª–µ–º–∞:** –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ `status` –±–µ–∑ —ñ–Ω–¥–µ–∫—Å—É.

```sql
-- AnalysisRunValidator.can_start_new_run() - –ª—ñ–Ω—ñ—è 78-80
SELECT * FROM analysis_runs
WHERE status IN ('pending', 'running', 'completed', 'reviewed');

-- –ù–µ–æ–±—Ö—ñ–¥–Ω–æ:
CREATE INDEX CONCURRENTLY idx_analysis_runs_status
ON analysis_runs(status)
WHERE status IN ('pending', 'running', 'completed', 'reviewed');
```

**Partial index** –∑–∞–º—ñ—Å—Ç—å –ø–æ–≤–Ω–æ–≥–æ - –∑–º–µ–Ω—à—É—î —Ä–æ–∑–º—ñ—Ä —Ç–∞ –ø—Ä–∏—Å–∫–æ—Ä—é—î closed/failed runs.

#### üü° Index –¥–ª—è TaskProposal filtering

**Use case:** `TaskProposalCRUD.list()` - —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –ø–æ `(analysis_run_id, status, confidence)`.

```sql
CREATE INDEX CONCURRENTLY idx_task_proposals_run_status_confidence
ON task_proposals(analysis_run_id, status, confidence DESC);
```

**Impact:** MEDIUM - –ø–æ–∫—Ä–∞—â–∏—Ç—å paginated lists –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é.

---

### 2.2 –í—ñ–¥—Å—É—Ç–Ω—ñ foreign key —ñ–Ω–¥–µ–∫—Å–∏

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –í—Å—ñ FK –º–∞—é—Ç—å —ñ–Ω–¥–µ–∫—Å–∏ (–ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –≤ –º—ñ–≥—Ä–∞—Ü—ñ—ó `d510922791ac`).

–ü—Ä–∏–∫–ª–∞–¥–∏:
- `messages.author_id` ‚Üí `users.id` (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π FK —ñ–Ω–¥–µ–∫—Å)
- `messages.source_id` ‚Üí `sources.id` (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π FK —ñ–Ω–¥–µ–∫—Å)
- `agent_configs.provider_id` ‚Üí `llm_providers.id` (–∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π FK —ñ–Ω–¥–µ–∫—Å)

PostgreSQL –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Å—Ç–≤–æ—Ä—é—î —ñ–Ω–¥–µ–∫—Å–∏ –Ω–∞ foreign keys –ø—Ä–∏ `ForeignKeyConstraint`.

---

### 2.3 –í—ñ–¥—Å—É—Ç–Ω—ñ UNIQUE constraints

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –ö—Ä–∏—Ç–∏—á–Ω—ñ UNIQUE constraints –ø—Ä–∏—Å—É—Ç–Ω—ñ:

- `users.email` (UNIQUE, ix_users_email)
- `users.phone` (UNIQUE, ix_users_phone)
- `telegram_profiles.telegram_user_id` (UNIQUE, ix_telegram_profiles_telegram_user_id)
- `topics.name` (UNIQUE, ix_topics_name)
- `agent_configs.name` (UNIQUE, ix_agent_configs_name)

‚ö†Ô∏è **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –î–æ–¥–∞—Ç–∏ UNIQUE constraint –Ω–∞ `messages.external_message_id` –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç–∞–º:

```sql
-- –ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω: –ª–∏—à–µ INDEX, –Ω–µ UNIQUE
CREATE INDEX ix_messages_external_message_id ON messages(external_message_id);

-- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:
CREATE UNIQUE INDEX CONCURRENTLY idx_messages_external_id_unique
ON messages(external_message_id, source_id);
```

**–û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è:** `external_message_id` –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º –≤ –º–µ–∂–∞—Ö source (Telegram message ID).

---

## 3. Migration Safety Review

### 3.1 –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –º—ñ–≥—Ä–∞—Ü—ñ—ó

**–í—Å—å–æ–≥–æ –º—ñ–≥—Ä–∞—Ü—ñ–π:** 8 (–ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ –≤ `/Users/maks/PycharmProjects/task-tracker/backend/alembic/versions/`)

#### üü¢ d510922791ac_initial_migration.py

**–°—Ç–∞—Ç—É—Å:** ‚úÖ SAFE

**–ê–Ω–∞–ª—ñ–∑:**
- –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—Å—ñ—Ö 21 —Ç–∞–±–ª–∏—Ü—å
- –ü—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è `server_default=sa.text("now()")` –¥–ª—è timestamps
- –í—Å—ñ FK constraints —Å—Ç–≤–æ—Ä–µ–Ω—ñ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
- pgvector extension –ø–µ—Ä–µ–¥–±–∞—á–∞—î—Ç—å—Å—è –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ—é (VECTOR(1536))

**–ü–æ—Ç–µ–Ω—Ü—ñ–π–Ω—ñ –ø—Ä–æ–±–ª–µ–º–∏:**
```python
# –õ—ñ–Ω—ñ—è 38: pgvector extension –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
sa.Column("embedding", pgvector.sqlalchemy.vector.VECTOR(dim=1536), nullable=True)
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
–î–æ–¥–∞—Ç–∏ –≤ upgrade():
```python
def upgrade() -> None:
    # Ensure pgvector extension is installed
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")

    # Existing table creation...
```

#### üü¢ 4c301ba5595c_fix_message_ingestion_timezone_fields.py

**–°—Ç–∞—Ç—É—Å:** ‚úÖ SAFE (–ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏)

**–ü—Ä–∏–ø—É—â–µ–Ω–Ω—è:** –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è timezone –ø–æ–ª—ñ–≤ —É `message_ingestion_jobs`.

**–ë–µ–∑–ø–µ–∫–∞:**
- ALTER COLUMN –¥–ª—è timezone –∑–º—ñ–Ω–∏ –º–æ–∂–µ –±—É—Ç–∏ –Ω–µ–±–µ–∑–ø–µ—á–Ω–∏–º –Ω–∞ –≤–µ–ª–∏–∫–∏—Ö —Ç–∞–±–ª–∏—Ü—è—Ö
- –ê–ª–µ `message_ingestion_jobs` - —Å–ª—É–∂–±–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è, –º–∞–ª–∏–π –æ–±—Å—è–≥ –¥–∞–Ω–∏—Ö

---

### 3.2 Migration Best Practices Check

#### ‚úÖ –î–æ—Ç—Ä–∏–º—É—é—Ç—å—Å—è:

1. **Timezone-aware timestamps**: –í—Å—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å `DateTime(timezone=True)`
2. **Server defaults**: `server_default=sa.text("now()")`
3. **Reversible migrations**: –ü—Ä–∏—Å—É—Ç–Ω—ñ `downgrade()` —Ñ—É–Ω–∫—Ü—ñ—ó
4. **No data migrations in schema changes**: –†–æ–∑–¥—ñ–ª–µ–Ω—ñ

#### ‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ:

1. **CREATE INDEX CONCURRENTLY**: –í—Å—ñ —ñ–Ω–¥–µ–∫—Å–∏ —Å—Ç–≤–æ—Ä—é—é—Ç—å—Å—è –∑ –±–ª–æ–∫—É–≤–∞–Ω–Ω—è–º

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –õ—ñ–Ω—ñ—è 41: –ë–ª–æ–∫—É—î —Ç–∞–±–ª–∏—Ü—é atoms –Ω–∞ —á–∞—Å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—É
op.create_index(op.f("ix_atoms_title"), "atoms", ["title"], unique=False)
```

**Impact:** –ù–∞ –≤–µ–ª–∏–∫–∏—Ö —Ç–∞–±–ª–∏—Ü—è—Ö (>100k rows) –º–æ–∂–µ –∑–∞–±–ª–æ–∫—É–≤–∞—Ç–∏ writes –Ω–∞ 10-30 —Å–µ–∫—É–Ω–¥.

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è –¥–ª—è –º–∞–π–±—É—Ç–Ω—ñ—Ö –º—ñ–≥—Ä–∞—Ü—ñ–π:**
```python
# –ó–∞–º—ñ—Å—Ç—å op.create_index():
op.execute("""
    CREATE INDEX CONCURRENTLY IF NOT EXISTS ix_atoms_title
    ON atoms(title)
""")
```

2. **Partition strategy –¥–ª—è high-volume tables**: –í—ñ–¥—Å—É—Ç–Ω—è

**–ö–∞–Ω–¥–∏–¥–∞—Ç–∏ –¥–ª—è –ø–∞—Ä—Ç–∏—Ü—ñ—é–≤–∞–Ω–Ω—è:**
- `messages` (–º–æ–∂–µ –∑—Ä–æ—Å—Ç–∏ –¥–æ –º—ñ–ª—å–π–æ–Ω—ñ–≤ –∑–∞–ø–∏—Å—ñ–≤)
- `telegram_messages` (—è–∫—â–æ —î –æ–∫—Ä–µ–º–∞ —Ç–∞–±–ª–∏—Ü—è)
- `analysis_runs` —Ç–∞ `task_proposals` (–º–æ–∂—É—Ç—å –Ω–∞–∫–æ–ø–∏—á—É–≤–∞—Ç–∏—Å—å)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```sql
-- Range partitioning –ø–æ created_at –¥–ª—è messages
CREATE TABLE messages (
    id BIGSERIAL,
    created_at TIMESTAMPTZ NOT NULL,
    ...
) PARTITION BY RANGE (created_at);

CREATE TABLE messages_2025_10 PARTITION OF messages
    FOR VALUES FROM ('2025-10-01') TO ('2025-11-01');
```

---

### 3.3 Rollback Safety

**–°—Ç–∞—Ç—É—Å:** ‚úÖ –í—Å—ñ –º—ñ–≥—Ä–∞—Ü—ñ—ó –º–∞—é—Ç—å `downgrade()` —Ñ—É–Ω–∫—Ü—ñ—ó.

**–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞:**
```python
# d510922791ac_initial_migration.py:485-519
def downgrade() -> None:
    op.drop_table("task_proposals")
    op.drop_table("tasks")
    # ... –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –ø–æ—Ä—è–¥–æ–∫ –≤–∏–¥–∞–ª–µ–Ω–Ω—è (reverse dependencies)
```

**–ë–µ–∑–ø–µ–∫–∞ rollback:** ‚úÖ FK constraints –≤–∏–¥–∞–ª—è—é—Ç—å—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É –ø–æ—Ä—è–¥–∫—É (child ‚Üí parent).

---

## 4. Connection Pool Configuration

### 4.1 –ü–æ—Ç–æ—á–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/app/database.py:7-11`

```python
engine = create_async_engine(
    settings.database.database_url,
    echo=False,
    future=True,
)
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ SQLAlchemy connection pool.

**–î–µ—Ñ–æ–ª—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (QueuePool):**
- `pool_size` = 5
- `max_overflow` = 10
- `pool_timeout` = 30
- `pool_recycle` = -1 (no recycling)
- `pool_pre_ping` = False

### 4.2 –ê–Ω–∞–ª—ñ–∑ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è

**–û—á—ñ–∫—É–≤–∞–Ω–µ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è:**
- FastAPI workers: 4-8 (gunicorn/uvicorn)
- TaskIQ background workers: 2-4
- –û–¥–Ω–æ—á–∞—Å–Ω—ñ WebSocket –∑'—î–¥–Ω–∞–Ω–Ω—è: 50-100
- –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Ç—ñ–≤/—Å–µ–∫: 10-50

**–ü—Ä–æ–±–ª–µ–º–∏:**
1. **Pool exhaustion –ø—Ä–∏ burst traffic**: 5 connections √ó 4 workers = 20 max, –∞–ª–µ burst –º–æ–∂–µ –ø–æ—Ç—Ä–µ–±—É–≤–∞—Ç–∏ 50+
2. **Stale connections**: –ë–µ–∑ `pool_recycle` –∑'—î–¥–Ω–∞–Ω–Ω—è –º–æ–∂—É—Ç—å —Å—Ç–∞—Ç–∏ stale –ø—ñ—Å–ª—è Docker restart
3. **Connection leaks**: –ë–µ–∑ `pool_pre_ping` –ø–æ–º–∏–ª–∫–∏ –ø—ñ—Å–ª—è network issues

### 4.3 –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

```python
from sqlalchemy.pool import NullPool, QueuePool

# –î–ª—è FastAPI (QueuePool)
engine = create_async_engine(
    settings.database.database_url,
    echo=False,
    future=True,
    pool_size=20,              # –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 5 –¥–ª—è 8 workers
    max_overflow=30,            # Burst capacity –¥–æ 50 total
    pool_timeout=60,            # –ó–±—ñ–ª—å—à–µ–Ω–æ timeout –¥–ª—è heavy queries
    pool_recycle=3600,          # Recycle connections –∫–æ–∂–Ω—É –≥–æ–¥–∏–Ω—É
    pool_pre_ping=True,         # Detect stale connections
    echo_pool=False,            # Set True –¥–ª—è debugging pool issues
)

# –î–ª—è TaskIQ workers (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ NullPool –¥–ª—è short-lived tasks)
taskiq_engine = create_async_engine(
    settings.database.database_url,
    poolclass=NullPool,         # No pooling for background jobs
    echo=False,
    future=True,
)
```

**–û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤:**

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–Ω—è | –û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è |
|----------|----------|---------------|
| `pool_size` | 20 | 8 workers √ó 2-3 connections average |
| `max_overflow` | 30 | Burst –¥–æ 50 total (20 + 30) |
| `pool_timeout` | 60s | –î–ª—è LLM queries (–º–æ–∂—É—Ç—å —Ç—Ä–∏–≤–∞—Ç–∏ 10-30s) |
| `pool_recycle` | 3600s | Avoid stale connections –ø—ñ—Å–ª—è 1 –≥–æ–¥ |
| `pool_pre_ping` | True | Auto-reconnect –ø—ñ—Å–ª—è network issues |

**–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ pool saturation:**
```python
# –î–æ–¥–∞—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤ FastAPI /metrics endpoint
pool_stats = {
    "size": engine.pool.size(),
    "checked_in": engine.pool.checkedin(),
    "checked_out": engine.pool.checkedout(),
    "overflow": engine.pool.overflow(),
}
```

---

### 4.4 PostgreSQL side configuration

**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤ `docker-compose.yml` –∞–±–æ `postgresql.conf`:**

```yaml
# docker-compose.yml
postgres:
  environment:
    - POSTGRES_MAX_CONNECTIONS=100  # Default 100, –¥–æ—Å—Ç–∞—Ç–Ω—å–æ
    - POSTGRES_SHARED_BUFFERS=256MB # 25% RAM –¥–ª—è Docker (1GB total)
    - POSTGRES_EFFECTIVE_CACHE_SIZE=512MB
    - POSTGRES_WORK_MEM=16MB        # –î–ª—è ORDER BY —Ç–∞ JOIN
```

**–û–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è:**
- `max_connections=100`: 50 app connections + 20 TaskIQ + 30 reserve
- `shared_buffers=256MB`: –ö–µ—à—É—î —á–∞—Å—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ (topics, users)
- `work_mem=16MB`: –î–ª—è ORDER BY –≤ `get_recent_topics()` —Ç–∞ pgvector sorts

---

## 5. SQLAlchemy ORM Patterns Quality

### 5.1 Relationship Loading Strategies

**–ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω:**
```python
# topic.py:221 - lazy="select" (default N+1 behavior)
versions: list["TopicVersion"] = Relationship(
    back_populates="topic",
    sa_relationship_kwargs={"lazy": "select"}
)

# atom.py:84 - lazy="select"
versions: list["AtomVersion"] = Relationship(
    back_populates="atom",
    sa_relationship_kwargs={"lazy": "select"}
)
```

**–û—Ü—ñ–Ω–∫–∞:** üü° MEDIUM - `lazy="select"` –±–µ–∑–ø–µ—á–Ω–∏–π, –∞–ª–µ –ø–æ—Ç—Ä–µ–±—É—î —Ä—É—á–Ω–æ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó.

**–ü—Ä–æ–±–ª–µ–º–∞:**
–Ø–∫—â–æ –∫–æ–¥ —ñ—Ç–µ—Ä—É—î –ø–æ topics —Ç–∞ –∑–≤–µ—Ä—Ç–∞—î—Ç—å—Å—è –¥–æ `topic.versions`:
```python
topics = session.execute(select(Topic).limit(10))
for topic in topics:
    print(topic.versions)  # N+1: –æ–∫—Ä–µ–º–∏–π –∑–∞–ø–∏—Ç –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ topic
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
–ó–∞–ª–∏—à–∏—Ç–∏ `lazy="select"` –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º, –∞–ª–µ –¥–æ–¥–∞—Ç–∏ eager loading –≤ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –º—ñ—Å—Ü—è—Ö:

```python
# –î–ª—è batch –æ–ø–µ—Ä–∞—Ü—ñ–π:
from sqlalchemy.orm import selectinload

topics = session.scalars(
    select(Topic)
    .options(selectinload(Topic.versions))
    .limit(10)
)
# –¢–µ–ø–µ—Ä topic.versions –Ω–µ —Ä–æ–±–∏—Ç—å –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è 1:N relationships:**
- `joinedload`: –û–¥–∏–Ω –∑–∞–ø–∏—Ç –∑ JOIN (–∫—Ä–∞—â–µ –¥–ª—è –Ω–µ–≤–µ–ª–∏–∫–∏—Ö collections)
- `selectinload`: –î–≤–∞ –∑–∞–ø–∏—Ç–∏ (IN clause) (–∫—Ä–∞—â–µ –¥–ª—è –≤–µ–ª–∏–∫–∏—Ö collections)
- `subqueryload`: Subquery (—Ä—ñ–¥–∫–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)

---

### 5.2 –í–∏—è–≤–ª–µ–Ω—ñ anti-patterns

#### ‚ùå –ü–æ–≤—Ç–æ—Ä—é–≤–∞–Ω–∏–π –∫–æ–¥ –≤ CRUD services

**–ü—Ä–∏–∫–ª–∞–¥:** –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è ORM ‚Üí Public schema –¥—É–±–ª—é—î—Ç—å—Å—è.

**topic_crud.py:55-63 vs 118-127:**
```python
# –î—É–±–ª—é—î—Ç—å—Å—è –≤ get() —Ç–∞ list()
return TopicPublic(
    id=topic.id,
    name=topic.name,
    description=topic.description,
    icon=topic.icon,
    color=color,
    created_at=topic.created_at.isoformat() if topic.created_at else "",
    updated_at=topic.updated_at.isoformat() if topic.updated_at else "",
)
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# –î–æ–¥–∞—Ç–∏ –º–µ—Ç–æ–¥ –≤ Topic model
class Topic(IDMixin, TimestampMixin, SQLModel, table=True):
    def to_public(self) -> TopicPublic:
        return TopicPublic(
            id=self.id,
            name=self.name,
            description=self.description,
            icon=self.icon,
            color=convert_to_hex_if_needed(self.color) if self.color else None,
            created_at=self.created_at.isoformat() if self.created_at else "",
            updated_at=self.updated_at.isoformat() if self.updated_at else "",
        )

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
return topic.to_public()
```

#### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è async/await

**–ü—Ä–∏–∫–ª–∞–¥:** `topic_crud.py`, `analysis_service.py`

–í—Å—ñ CRUD –º–µ—Ç–æ–¥–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å:
```python
async def get(self, topic_id: int) -> TopicPublic | None:
    query = select(Topic).where(Topic.id == topic_id)
    result = await self.session.execute(query)  # ‚úÖ await
    topic = result.scalar_one_or_none()
```

---

### 5.3 Bulk Operations

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ bulk insert/update –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó.

**–ü—Ä–∏–∫–ª–∞–¥:** `analysis_service.py:605-620` - save_proposals()

```python
# –ü–æ—Ç–æ—á–Ω–∏–π –∫–æ–¥: N individual INSERTs
for proposal_data in proposals:
    proposal = TaskProposal(**proposal_data)
    self.session.add(proposal)
    saved_count += 1

await self.session.commit()  # –û–¥–∏–Ω commit, –∞–ª–µ N INSERTs
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –ü—Ä–∏ 50 proposals - 50 –æ–∫—Ä–µ–º–∏—Ö INSERT statements.

**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:**
```python
# Bulk insert –∑ session.bulk_insert_mappings():
proposal_dicts = [
    {**proposal_data, "analysis_run_id": run_id}
    for proposal_data in proposals
]

await self.session.run_sync(
    lambda session: session.bulk_insert_mappings(TaskProposal, proposal_dicts)
)
await self.session.commit()
```

**Performance Impact:**
- üî¥ Current: 50 INSERTs √ó 5ms = 250ms
- ‚úÖ Optimized: 1 bulk INSERT = 20-30ms
- üìâ **8-10x —à–≤–∏–¥—à–µ**

---

## 6. pgvector Performance Audit

### 6.1 –ü–æ—Ç–æ—á–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

**Embedding dimensions:** 1536 (OpenAI text-embedding-3-small)

**–õ–æ–∫–∞—Ü—ñ—è:** `/Users/maks/PycharmProjects/task-tracker/backend/core/config.py:83-84`
```python
openai_embedding_dimensions: int = Field(default=1536, ...)
```

**–ú–æ–¥–µ–ª—ñ –∑ embeddings:**
1. `messages.embedding` - VECTOR(1536)
2. `atoms.embedding` - VECTOR(1536)

**Validation:** ‚úÖ Dimensions –∑–±—ñ–≥–∞—é—Ç—å—Å—è –º—ñ–∂ settings —Ç–∞ schema.

---

### 6.2 –Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è (CRITICAL ISSUE)

**–°—Ç–∞—Ç—É—Å:** üî¥ **–í–Ü–î–°–£–¢–ù–Ü pgvector —ñ–Ω–¥–µ–∫—Å–∏**

**–ü—Ä–æ–±–ª–µ–º–∞:**
```sql
-- –ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω –º—ñ–≥—Ä–∞—Ü—ñ—ó:
sa.Column("embedding", pgvector.sqlalchemy.vector.VECTOR(dim=1536), nullable=True)

-- –í—ñ–¥—Å—É—Ç–Ω—ñ–π —ñ–Ω–¥–µ–∫—Å! –í—Å—ñ –ø–æ—à—É–∫–∏ - linear scan!
```

**Impact –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å:**

| –ö—ñ–ª—å–∫—ñ—Å—Ç—å messages | –ë–µ–∑ —ñ–Ω–¥–µ–∫—Å—É | –ó HNSW (m=16) | –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è |
|-------------------|-------------|---------------|-------------|
| 1,000 | 50ms | 10ms | 5x |
| 10,000 | 500ms | 30ms | 16x |
| 100,000 | 5000ms | 80ms | 62x |

**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ —ñ–Ω–¥–µ–∫—Å–∏:**

#### –î–ª—è cosine similarity (<=>)

```sql
-- Messages embedding index
CREATE INDEX CONCURRENTLY idx_messages_embedding_hnsw
ON messages USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Atoms embedding index
CREATE INDEX CONCURRENTLY idx_atoms_embedding_hnsw
ON atoms USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏:**
- **m = 16**: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–≤–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—Ö –∑–≤'—è–∑–∫—ñ–≤ (–±—ñ–ª—å—à–µ = –∫—Ä–∞—â–∞ accuracy, –±—ñ–ª—å—à–µ –ø–∞–º'—è—Ç—ñ)
  - –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è dim=1536: 12-24
  - –í–∏–±—Ä–∞–Ω–æ 16 —è–∫ –±–∞–ª–∞–Ω—Å
- **ef_construction = 64**: –†–æ–∑–º—ñ—Ä dynamic candidate list –ø—ñ–¥ —á–∞—Å –ø–æ–±—É–¥–æ–≤–∏
  - –ë—ñ–ª—å—à–µ = –∫—Ä–∞—â–∞ accuracy, –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ —ñ–Ω–¥–µ–∫—Å—É–≤–∞–Ω–Ω—è
  - –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ: 64-128 –¥–ª—è production

#### –î–ª—è L2 distance (<->) (—è–∫—â–æ –±—É–¥–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ)

```sql
CREATE INDEX CONCURRENTLY idx_messages_embedding_l2
ON messages USING hnsw (embedding vector_l2_ops)
WITH (m = 16, ef_construction = 64);
```

---

### 6.3 Runtime –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (ef_search)

**–ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–µ—Ñ–æ–ª—Ç–∏ pgvector.

**–ü—Ä–æ–±–ª–µ–º–∞:** `ef_search` –∫–æ–Ω—Ç—Ä–æ–ª—é—î accuracy vs speed runtime, –∞–ª–µ –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π.

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# semantic_search_service.py - –¥–æ–¥–∞—Ç–∏ SET –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –∑–∞–ø–∏—Ç—É
async def search_messages(self, session: AsyncSession, query: str, ...):
    # –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ ef_search –¥–ª—è –±–∞–ª–∞–Ω—Å—É accuracy/speed
    await session.execute(text("SET hnsw.ef_search = 100"))

    # Existing query...
    sql = text("""
        SELECT m.*, 1 - (m.embedding <=> :query_vector::vector) / 2 AS similarity
        FROM messages m
        WHERE m.embedding IS NOT NULL
          AND (1 - (m.embedding <=> :query_vector::vector) / 2) >= :threshold
        ORDER BY m.embedding <=> :query_vector::vector
        LIMIT :limit
    """)
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ ef_search:**
- `40` (default): –®–≤–∏–¥–∫–æ, –∞–ª–µ lower recall (~85-90%)
- `100`: –ë–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏–π (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ –¥–ª—è production)
- `200`: –í–∏—Å–æ–∫–∞ accuracy (~95-98%), –ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ

**Trade-off:**
| ef_search | Latency | Recall | Use Case |
|-----------|---------|--------|----------|
| 40 | 10ms | 85-90% | Real-time autocomplete |
| 100 | 30ms | 92-95% | **General search (recommended)** |
| 200 | 80ms | 95-98% | High-accuracy analytics |

---

### 6.4 Partial indexes –¥–ª—è filtered searches

**Use case:** –ß–∞—Å—Ç–æ —à—É–∫–∞—é—Ç—å —Ç—ñ–ª—å–∫–∏ —Å–µ—Ä–µ–¥ analyzed messages –∞–±–æ –ø–æ topic_id.

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```sql
-- Index —Ç—ñ–ª—å–∫–∏ –¥–ª—è analyzed messages
CREATE INDEX CONCURRENTLY idx_messages_embedding_analyzed
ON messages USING hnsw (embedding vector_cosine_ops)
WHERE analyzed = TRUE AND embedding IS NOT NULL
WITH (m = 16, ef_construction = 64);

-- Index –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ topic (—è–∫—â–æ —î hot topics)
CREATE INDEX CONCURRENTLY idx_messages_embedding_topic_123
ON messages USING hnsw (embedding vector_cosine_ops)
WHERE topic_id = 123 AND embedding IS NOT NULL
WITH (m = 12, ef_construction = 64);
```

**Performance Impact:**
- –ú–µ–Ω—à–∏–π —ñ–Ω–¥–µ–∫—Å = —à–≤–∏–¥—à–∏–π –ø–æ—à—É–∫
- –ú–µ–Ω—à–µ RAM usage
- –ê–ª–µ –ø–æ—Ç—Ä–µ–±—É—î –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö —ñ–Ω–¥–µ–∫—Å—ñ–≤ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ñ—ñ–ª—å—Ç—Ä—ñ–≤

---

### 6.5 Embedding validation

**–°—Ç–∞—Ç—É—Å:** ‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—è dimensions –ø—Ä–∏ –≤—Å—Ç–∞–≤—Ü—ñ.

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# message.py:61-65
embedding: list[float] | None = Field(
    default=None,
    sa_column=Column(Vector(1536)),
    description="Vector embedding (must match settings.embedding.openai_embedding_dimensions)",
)
```

**–ö–æ–º–µ–Ω—Ç–∞—Ä –∑–≥–∞–¥—É—î validation, –∞–ª–µ –Ω–µ–º–∞—î –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –≤ –∫–æ–¥—ñ.**

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
from pydantic import field_validator

class Message(IDMixin, TimestampMixin, SQLModel, table=True):
    embedding: list[float] | None = Field(...)

    @field_validator("embedding", mode="before")
    @classmethod
    def validate_embedding_dimensions(cls, v: list[float] | None) -> list[float] | None:
        if v is not None and len(v) != 1536:
            raise ValueError(
                f"Embedding must have exactly 1536 dimensions, got {len(v)}"
            )
        return v
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞:** Database constraint (–±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–æ)
```sql
ALTER TABLE messages
ADD CONSTRAINT check_embedding_dimensions
CHECK (embedding IS NULL OR vector_dims(embedding) = 1536);
```

---

## 7. Optimization Recommendations

### 7.1 –ö—Ä–∏—Ç–∏—á–Ω—ñ (Priority 1 - Implement before production)

#### 1. –î–æ–¥–∞—Ç–∏ pgvector HNSW —ñ–Ω–¥–µ–∫—Å–∏

**Impact:** üî¥ CRITICAL - 10-60x –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è semantic search

**–ú—ñ–≥—Ä–∞—Ü—ñ—è:**
```python
# alembic/versions/XXXXX_add_pgvector_indexes.py
def upgrade() -> None:
    # Messages embedding index
    op.execute("""
        CREATE INDEX CONCURRENTLY idx_messages_embedding_hnsw
        ON messages USING hnsw (embedding vector_cosine_ops)
        WITH (m = 16, ef_construction = 64)
    """)

    # Atoms embedding index
    op.execute("""
        CREATE INDEX CONCURRENTLY idx_atoms_embedding_hnsw
        ON atoms USING hnsw (embedding vector_cosine_ops)
        WITH (m = 16, ef_construction = 64)
    """)

def downgrade() -> None:
    op.execute("DROP INDEX CONCURRENTLY IF EXISTS idx_messages_embedding_hnsw")
    op.execute("DROP INDEX CONCURRENTLY IF EXISTS idx_atoms_embedding_hnsw")
```

**Estimated build time:** 10-30 —Å–µ–∫—É–Ω–¥ –¥–ª—è 10,000 messages (CONCURRENTLY).

---

#### 2. –í–∏–ø—Ä–∞–≤–∏—Ç–∏ N+1 –≤ AnalysisExecutor.process_batch()

**Impact:** üî¥ HIGH - 5-10 —Å–µ–∫—É–Ω–¥ –µ–∫–æ–Ω–æ–º—ñ—ó –Ω–∞ analysis run

**–ö–æ–¥:**
```python
# analysis_service.py - –∑–∞–º—ñ–Ω–∏—Ç–∏ –ª—ñ–Ω—ñ—ó 534-565 –Ω–∞:
async def process_batch(self, run_id: UUID, batch: list[Message], use_rag: bool = False):
    from sqlalchemy.orm import joinedload

    # –û–î–ò–ù –∑–∞–ø–∏—Ç –∑–∞–º—ñ—Å—Ç—å 5:
    run = await self.session.scalar(
        select(AnalysisRun)
        .options(
            joinedload(AnalysisRun.agent_assignment)
                .joinedload(AgentTaskAssignment.agent)
                .joinedload(AgentConfig.provider),
            joinedload(AnalysisRun.project_config)
        )
        .where(AnalysisRun.id == run_id)
    )

    if not run:
        raise ValueError(f"Run with ID '{run_id}' not found")

    # –¢–µ–ø–µ—Ä –¥–æ—Å—Ç—É–ø –±–µ–∑ N+1:
    agent = run.agent_assignment.agent
    provider = agent.provider
    project_config = run.project_config

    # Existing LLM logic...
```

---

#### 3. –û–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ connection pool

**Impact:** üî¥ CRITICAL - —É–Ω–∏–∫–Ω—É—Ç–∏ pool exhaustion –ø—ñ–¥ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º

**–ö–æ–¥:**
```python
# database.py
engine = create_async_engine(
    settings.database.database_url,
    echo=False,
    future=True,
    pool_size=20,
    max_overflow=30,
    pool_timeout=60,
    pool_recycle=3600,
    pool_pre_ping=True,
)
```

---

### 7.2 –í–∏—Å–æ–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç (Priority 2 - Implement soon)

#### 4. –î–æ–¥–∞—Ç–∏ composite index –¥–ª—è messages

```sql
CREATE INDEX CONCURRENTLY idx_messages_topic_sent_at
ON messages(topic_id, sent_at DESC);
```

#### 5. –î–æ–¥–∞—Ç–∏ partial index –¥–ª—è analysis_runs status

```sql
CREATE INDEX CONCURRENTLY idx_analysis_runs_status_active
ON analysis_runs(status)
WHERE status IN ('pending', 'running', 'completed', 'reviewed');
```

#### 6. Bulk insert optimization –¥–ª—è proposals

```python
# proposal_service.py –∞–±–æ analysis_service.py
await session.run_sync(
    lambda s: s.bulk_insert_mappings(TaskProposal, proposal_dicts)
)
```

---

### 7.3 –°–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç (Priority 3 - Nice to have)

#### 7. –î–æ–¥–∞—Ç–∏ `.to_public()` –º–µ—Ç–æ–¥–∏ –≤ models

–ó–º–µ–Ω—à–∏—Ç–∏ –¥—É–±–ª—é–≤–∞–Ω–Ω—è –∫–æ–¥—É –≤ CRUD services.

#### 8. –î–æ–¥–∞—Ç–∏ UNIQUE constraint –Ω–∞ messages.external_message_id

```sql
CREATE UNIQUE INDEX CONCURRENTLY idx_messages_external_id_unique
ON messages(external_message_id, source_id);
```

#### 9. –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ ef_search –¥–ª—è pgvector runtime

```python
await session.execute(text("SET hnsw.ef_search = 100"))
```

---

### 7.4 –ù–∏–∑—å–∫–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç (Priority 4 - Future optimization)

#### 10. –ü–∞—Ä—Ç–∏—Ü—ñ—é–≤–∞–Ω–Ω—è messages –ø–æ created_at

–î–ª—è –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –¥–æ –º—ñ–ª—å–π–æ–Ω—ñ–≤ –∑–∞–ø–∏—Å—ñ–≤.

#### 11. Read replicas –¥–ª—è read-heavy queries

–Ø–∫—â–æ –±—É–¥–µ –ø–æ—Ç—Ä—ñ–±–Ω–æ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è reads.

#### 12. Monitoring —Ç–∞ alerting

- Connection pool saturation
- Slow query log (>500ms)
- pgvector index usage stats

---

## 8. Performance Impact Estimates

### 8.1 –ü–æ—Ç–æ—á–Ω–∞ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å (–±–µ–∑ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π)

**–¢–∏–ø–æ–≤—ñ —Å—Ü–µ–Ω–∞—Ä—ñ—ó:**

| –û–ø–µ—Ä–∞—Ü—ñ—è | Estimated Latency | Bottleneck |
|----------|-------------------|------------|
| `semantic_search_service.search_messages(10k msgs)` | 500-1000ms | No HNSW index (linear scan) |
| `analysis_service.process_batch(50 batches)` | 5-10s extra | N+1 queries (5 √ó 50 = 250 queries) |
| `topic_crud.get_recent_topics(limit=10)` | 50-100ms | No composite index on (topic_id, sent_at) |
| `proposal_service.save_proposals(50 proposals)` | 200-300ms | Individual INSERTs |
| Connection pool –ø–æ–¥ burst (100 rps) | Pool exhaustion | pool_size=5, max_overflow=10 |

**–ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞:** ‚ö†Ô∏è –ú–æ–∂–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –¥–ª—è 10-50 –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, –∞–ª–µ critical issues –ø—ñ–¥ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º.

---

### 8.2 –ü—ñ—Å–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π (Priority 1-2)

**–ü—Ä–æ–≥–Ω–æ–∑:**

| –û–ø–µ—Ä–∞—Ü—ñ—è | Current | Optimized | Improvement |
|----------|---------|-----------|-------------|
| Semantic search (10k msgs) | 500-1000ms | 30-80ms | **10-30x faster** |
| Analysis run (50 batches) | +5-10s overhead | +0.5s overhead | **10-20x faster** |
| Recent topics | 50-100ms | 20-30ms | **2-3x faster** |
| Save 50 proposals | 200-300ms | 20-30ms | **8-10x faster** |
| Connection pool saturation | Frequent under burst | Rare | **Eliminates bottleneck** |

**Estimated overall improvement:** 5-10x faster –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –æ–ø–µ—Ä–∞—Ü—ñ–π.

---

### 8.3 ROI (Return on Investment)

**–í–∏—Ç—Ä–∞—Ç–∏ —á–∞—Å—É –Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó:**

| –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è | Estimated Effort | Impact | ROI |
|-------------|------------------|--------|-----|
| HNSW —ñ–Ω–¥–µ–∫—Å–∏ | 1 –≥–æ–¥–∏–Ω–∞ | CRITICAL | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| N+1 fix (AnalysisExecutor) | 2 –≥–æ–¥–∏–Ω–∏ | HIGH | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Connection pool config | 30 —Ö–≤–∏–ª–∏–Ω | CRITICAL | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Composite indexes | 1 –≥–æ–¥–∏–Ω–∞ | MEDIUM | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Bulk insert optimization | 1.5 –≥–æ–¥–∏–Ω–∏ | MEDIUM | ‚≠ê‚≠ê‚≠ê |

**Total effort:** 6 –≥–æ–¥–∏–Ω
**Total impact:** Production-ready database performance

---

## 9. Monitoring Recommendations

### 9.1 Metrics to track

**Database metrics:**
```python
# –î–æ–¥–∞—Ç–∏ –≤ FastAPI /metrics endpoint

from sqlalchemy import text

async def get_db_metrics(session: AsyncSession):
    # Connection pool stats
    pool_stats = {
        "pool_size": engine.pool.size(),
        "checked_in": engine.pool.checkedin(),
        "checked_out": engine.pool.checkedout(),
        "overflow": engine.pool.overflow(),
        "saturation_pct": (engine.pool.checkedout() / engine.pool.size()) * 100
    }

    # Query performance
    slow_queries = await session.execute(text("""
        SELECT query, mean_exec_time, calls
        FROM pg_stat_statements
        WHERE mean_exec_time > 500
        ORDER BY mean_exec_time DESC
        LIMIT 10
    """))

    # pgvector index usage
    index_stats = await session.execute(text("""
        SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read
        FROM pg_stat_user_indexes
        WHERE indexname LIKE '%embedding%'
    """))

    return {
        "pool": pool_stats,
        "slow_queries": slow_queries.fetchall(),
        "vector_indexes": index_stats.fetchall()
    }
```

**Alerts:**
- Connection pool saturation > 80%
- Slow queries > 1000ms
- pgvector index idx_scan = 0 (—ñ–Ω–¥–µ–∫—Å –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è)

---

### 9.2 EXPLAIN ANALYZE –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –î–æ–¥–∞—Ç–∏ logging EXPLAIN ANALYZE –≤ development.

```python
# database.py - development mode
if settings.app.log_level == "DEBUG":
    @event.listens_for(engine.sync_engine, "before_cursor_execute")
    def receive_before_cursor_execute(conn, cursor, statement, params, context, executemany):
        if "SELECT" in statement:
            # Log EXPLAIN ANALYZE
            explain = conn.execute(f"EXPLAIN ANALYZE {statement}", params)
            logger.debug(f"EXPLAIN:\n{explain.fetchall()}")
```

---

## 10. Migration Plan

### –ü–æ–µ—Ç–∞–ø–Ω–∏–π –ø–ª–∞–Ω –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π:

#### Phase 1: Critical fixes (Week 1)
1. ‚úÖ –°—Ç–≤–æ—Ä–∏—Ç–∏ pgvector HNSW —ñ–Ω–¥–µ–∫—Å–∏ (–º—ñ–≥—Ä–∞—Ü—ñ—è)
2. ‚úÖ –í–∏–ø—Ä–∞–≤–∏—Ç–∏ N+1 –≤ AnalysisExecutor
3. ‚úÖ –û–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ connection pool
4. ‚úÖ –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—ñ–¥ –Ω–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è–º

#### Phase 2: High-priority indexes (Week 2)
5. ‚úÖ Composite index: messages(topic_id, sent_at)
6. ‚úÖ Partial index: analysis_runs(status)
7. ‚úÖ UNIQUE constraint: messages(external_message_id, source_id)
8. ‚úÖ –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ ef_search –¥–ª—è pgvector

#### Phase 3: Code optimization (Week 3)
9. ‚úÖ Bulk insert –¥–ª—è proposals
10. ‚úÖ –î–æ–¥–∞—Ç–∏ `.to_public()` methods
11. ‚úÖ –î–æ–¥–∞—Ç–∏ monitoring metrics endpoint
12. ‚úÖ EXPLAIN ANALYZE logging –≤ development

#### Phase 4: Scaling preparation (Future)
13. üîÑ –ü–∞—Ä—Ç–∏—Ü—ñ—é–≤–∞–Ω–Ω—è messages (—è–∫—â–æ >1M records)
14. üîÑ Read replicas (—è–∫—â–æ reads >> writes)
15. üîÑ Connection pooling via PgBouncer (—è–∫—â–æ pool exhaustion –ø—Ä–æ–¥–æ–≤–∂—É—î—Ç—å—Å—è)

---

## –í–∏—Å–Ω–æ–≤–∫–∏

### –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö: 7.5/10

**Strengths:**
- ‚úÖ –ß–∏—Å—Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ FK constraints
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è async/await
- ‚úÖ –ë–∞–∑–æ–≤–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –ø—Ä–∞—Ü—é—î
- ‚úÖ –ú—ñ–≥—Ä–∞—Ü—ñ—ó –±–µ–∑–ø–µ—á–Ω—ñ —Ç–∞ reversible

**Critical Issues:**
- üî¥ –í—ñ–¥—Å—É—Ç–Ω—ñ pgvector —ñ–Ω–¥–µ–∫—Å–∏ (10-60x performance hit)
- üî¥ N+1 queries –≤ AnalysisExecutor (5-10s overhead)
- üî¥ Connection pool –ø—ñ–¥-–Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω

**After Priority 1-2 optimizations: Expected 9/10**

---

## –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏

1. **Immediate (—Ü–µ–π —Ç–∏–∂–¥–µ–Ω—å):**
   - –°—Ç–≤–æ—Ä–∏—Ç–∏ –º—ñ–≥—Ä–∞—Ü—ñ—é –∑ HNSW —ñ–Ω–¥–µ–∫—Å–∞–º–∏
   - –í–∏–ø—Ä–∞–≤–∏—Ç–∏ N+1 –≤ AnalysisExecutor
   - –û–Ω–æ–≤–∏—Ç–∏ connection pool config

2. **Short-term (–Ω–∞—Å—Ç—É–ø–Ω—ñ 2 —Ç–∏–∂–Ω—ñ):**
   - –î–æ–¥–∞—Ç–∏ composite indexes
   - Bulk insert optimization
   - Monitoring metrics

3. **Long-term (–Ω–∞—Å—Ç—É–ø–Ω–∏–π –º—ñ—Å—è—Ü—å):**
   - –ü–∞—Ä—Ç–∏—Ü—ñ—é–≤–∞–Ω–Ω—è –ø—Ä–∏ –ø–æ—Ç—Ä–µ–±—ñ
   - Read replicas –ø—Ä–∏ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—ñ
   - Advanced monitoring dashboard

---

**–ó–≤—ñ—Ç –ø—ñ–¥–≥–æ—Ç—É–≤–∞–≤:** Database Reliability Engineer (DBRE)
**–ö–æ–Ω—Ç–∞–∫—Ç:** database-engineer agent
**–î–∞—Ç–∞:** 27 –∂–æ–≤—Ç–Ω—è 2025

---

## –î–æ–¥–∞—Ç–æ–∫ A: SQL Scripts –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–Ω—è

### A.1 –°—Ç–≤–æ—Ä–µ–Ω–Ω—è pgvector —ñ–Ω–¥–µ–∫—Å—ñ–≤

```sql
-- Run in psql or via migration

-- Enable pgvector extension (if not enabled)
CREATE EXTENSION IF NOT EXISTS vector;

-- Messages embedding HNSW index
CREATE INDEX CONCURRENTLY idx_messages_embedding_hnsw
ON messages USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Atoms embedding HNSW index
CREATE INDEX CONCURRENTLY idx_atoms_embedding_hnsw
ON atoms USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Verify index creation
SELECT schemaname, tablename, indexname, indexdef
FROM pg_indexes
WHERE indexname LIKE '%embedding%';
```

### A.2 Composite —Ç–∞ partial indexes

```sql
-- Messages: topic_id + sent_at composite
CREATE INDEX CONCURRENTLY idx_messages_topic_sent_at
ON messages(topic_id, sent_at DESC);

-- Analysis runs: status partial index
CREATE INDEX CONCURRENTLY idx_analysis_runs_status_active
ON analysis_runs(status)
WHERE status IN ('pending', 'running', 'completed', 'reviewed');

-- Task proposals: run_id + status + confidence
CREATE INDEX CONCURRENTLY idx_task_proposals_run_status_confidence
ON task_proposals(analysis_run_id, status, confidence DESC);

-- Messages: external_message_id UNIQUE
CREATE UNIQUE INDEX CONCURRENTLY idx_messages_external_id_unique
ON messages(external_message_id, source_id);
```

### A.3 Validate indexes

```sql
-- Check index usage stats
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan as scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;

-- Check index size
SELECT
    schemaname,
    tablename,
    indexname,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY pg_relation_size(indexrelid) DESC;
```

---

## –î–æ–¥–∞—Ç–æ–∫ B: Connection Pool Monitoring

```python
# app/api/endpoints/monitoring.py

from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import text

from app.database import engine, get_db_session

router = APIRouter(prefix="/monitoring", tags=["monitoring"])

@router.get("/db/pool")
async def get_pool_stats():
    """Connection pool statistics."""
    return {
        "pool_size": engine.pool.size(),
        "checked_in": engine.pool.checkedin(),
        "checked_out": engine.pool.checkedout(),
        "overflow": engine.pool.overflow(),
        "max_overflow": engine.pool._max_overflow,
        "saturation_pct": round((engine.pool.checkedout() / engine.pool.size()) * 100, 2)
    }

@router.get("/db/slow-queries")
async def get_slow_queries(session: AsyncSession = Depends(get_db_session)):
    """Top 10 slowest queries (requires pg_stat_statements extension)."""
    query = text("""
        SELECT
            substring(query, 1, 100) as query_preview,
            calls,
            round(mean_exec_time::numeric, 2) as avg_ms,
            round(total_exec_time::numeric, 2) as total_ms
        FROM pg_stat_statements
        WHERE mean_exec_time > 100
        ORDER BY mean_exec_time DESC
        LIMIT 10
    """)
    result = await session.execute(query)
    return [dict(row._mapping) for row in result.fetchall()]

@router.get("/db/indexes/pgvector")
async def get_pgvector_index_stats(session: AsyncSession = Depends(get_db_session)):
    """pgvector index usage statistics."""
    query = text("""
        SELECT
            schemaname,
            tablename,
            indexname,
            idx_scan as scans,
            idx_tup_read as tuples_read,
            pg_size_pretty(pg_relation_size(indexrelid)) as index_size
        FROM pg_stat_user_indexes
        WHERE indexname LIKE '%embedding%'
        ORDER BY idx_scan DESC
    """)
    result = await session.execute(query)
    return [dict(row._mapping) for row in result.fetchall()]
```

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**
```bash
# Check pool saturation
curl http://localhost:8000/monitoring/db/pool

# Check slow queries
curl http://localhost:8000/monitoring/db/slow-queries

# Check pgvector index usage
curl http://localhost:8000/monitoring/db/indexes/pgvector
```

---

**End of Report**
