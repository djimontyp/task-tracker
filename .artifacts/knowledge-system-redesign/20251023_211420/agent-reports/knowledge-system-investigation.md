# Knowledge Extraction System - Comprehensive Investigation Report

**Report Date:** October 23, 2025
**Investigation Scope:** Very Thorough
**Status:** Complete Analysis

---

## Executive Summary

The Knowledge Extraction system is a **partially implemented but fundamentally sound** architecture for extracting structured knowledge from unstructured conversations. The system successfully auto-extracts Topics (discussion themes) and Atoms (atomic knowledge units) using LLM-powered analysis, with real-time WebSocket broadcasts and background task orchestration.

### Key Findings (5 Bullet Points)

1. **Core System Works:** Topics, Atoms, and relationships are automatically extracted, created, and persisted with 96% test coverage
2. **Auto-Extraction Active:** After every 10 unprocessed messages in 24-hour window, system triggers extraction via TaskIQ background worker
3. **No Proposal/Versioning Workflow for Knowledge:** While TaskProposals exist for tasks, **Topics/Atoms lack approval, versioning, or consolidation workflows**
4. **Manual Creation Possible:** Users can manually create atoms via API, but frontend UI is minimal (only basic CreateAtomDialog component)
5. **Real-time Updates Work:** WebSocket broadcasts extraction events to connected clients; no centralized knowledge review interface

---

## System Architecture

### High-Level Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         MESSAGE INGESTION                        ‚îÇ
‚îÇ  Telegram Messages ‚Üí save_telegram_message() task ‚Üí Database     ‚îÇ
‚îÇ                     ‚îî‚îÄ queue_knowledge_extraction_if_needed()    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Unprocessed     ‚îÇ
                    ‚îÇ Count >= 10 ?   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ YES
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  extract_knowledge_from_messages_task  ‚îÇ
         ‚îÇ         (TaskIQ Background Job)        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   KnowledgeExtractionService        ‚îÇ
        ‚îÇ  (Pydantic AI + Ollama/OpenAI)      ‚îÇ
        ‚îÇ                                      ‚îÇ
        ‚îÇ  1. Extract Topics (1-3 per batch)  ‚îÇ
        ‚îÇ  2. Extract Atoms (5-10 per topic)  ‚îÇ
        ‚îÇ  3. Create Relationships            ‚îÇ
        ‚îÇ  4. Filter by confidence (0.7+)     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Database Persistence        ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  Topic (created + reused)     ‚îÇ
        ‚îÇ  Atom (created per topic)     ‚îÇ
        ‚îÇ  TopicAtom (many-to-many)    ‚îÇ
        ‚îÇ  AtomLink (relationships)     ‚îÇ
        ‚îÇ  Message.topic_id (updated)   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    WebSocket Broadcast        ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ  knowledge.extraction_started ‚îÇ
        ‚îÇ  knowledge.topic_created      ‚îÇ
        ‚îÇ  knowledge.atom_created       ‚îÇ
        ‚îÇ  knowledge.extraction_done    ‚îÇ
        ‚îÇ  knowledge.extraction_failed  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### System Components

| Component | File | Responsibility | Status |
|-----------|------|-----------------|--------|
| **KnowledgeExtractionService** | `backend/app/services/knowledge_extraction_service.py` (495 lines) | Core extraction logic, LLM integration, entity creation | ‚úÖ Implemented |
| **Background Task** | `backend/app/tasks.py` (lines 1007-1155) | Task orchestration, WebSocket broadcasting | ‚úÖ Implemented |
| **Auto-Trigger Logic** | `backend/app/tasks.py` (lines 19-86) | Threshold detection, auto-queueing | ‚úÖ Implemented |
| **REST API** | `backend/app/api/v1/knowledge.py` (90 lines) | Manual extraction trigger endpoint | ‚úÖ Implemented |
| **Models** | `backend/app/models/topic.py`, `atom.py` | Database schema, validation | ‚úÖ Implemented |
| **Frontend Components** | `frontend/src/features/atoms/` | Atom display & creation UI | ‚ö†Ô∏è Minimal |
| **Topic/Atom CRUD** | `backend/app/services/topic_crud.py`, `atom_crud.py` | Entity management APIs | ‚úÖ Implemented |
| **Knowledge API Service** | `frontend/src/features/knowledge/api/` | Frontend LLM calls | ‚ö†Ô∏è Limited |

---

## Backend Implementation Deep Dive

### 1. Models & Schema

#### Topic Model (`backend/app/models/topic.py`)
```python
class Topic(IDMixin, TimestampMixin, SQLModel, table=True):
    name: str                    # Unique, max 100 chars
    description: str             # Text field
    icon: str | None             # Heroicon name (auto-selected)
    color: str | None            # Hex color (auto-selected)
```

**Key Features:**
- ‚úÖ Auto icon/color selection based on keywords
- ‚úÖ 16 predefined icon mappings (shopping, learning, finance, etc.)
- ‚úÖ Hex color validation and Tailwind color conversion
- ‚ö†Ô∏è No versioning or history tracking
- ‚ö†Ô∏è No approval/review status field

#### Atom Model (`backend/app/models/atom.py`)
```python
class Atom(IDMixin, TimestampMixin, SQLModel, table=True):
    type: str                    # problem/solution/decision/insight/question/pattern/requirement
    title: str                   # Max 200 chars
    content: str                 # Text field (full content)
    confidence: float | None     # 0.0-1.0 (LLM confidence)
    user_approved: bool          # Manual verification flag
    meta: dict | None            # JSON metadata (source, message_ids)
    embedding: list[float] | None # Vector embedding (pgvector)
```

**Key Features:**
- ‚úÖ 7 atom types with enum validation
- ‚úÖ Confidence scoring for auto-creation filtering
- ‚úÖ `user_approved` flag for manual review
- ‚úÖ Source message tracking in meta
- ‚úÖ Vector embedding support
- ‚ö†Ô∏è No version/revision history
- ‚ö†Ô∏è No approval workflow fields
- ‚ö†Ô∏è No deduplication detected flag

#### TopicAtom (Many-to-Many) (`backend/app/models/atom.py`)
```python
class TopicAtom(TimestampMixin, SQLModel, table=True):
    topic_id: int               # Foreign key
    atom_id: int                # Foreign key
    position: int | None        # Display order
    note: str | None            # Contextual note (auto-filled with confidence)
```

**Key Features:**
- ‚úÖ Composite primary key (topic_id, atom_id)
- ‚úÖ Position tracking for ordering
- ‚ö†Ô∏è Note field not fully utilized

#### AtomLink (Relationships) (`backend/app/models/atom.py`)
```python
class AtomLink(TimestampMixin, SQLModel, table=True):
    from_atom_id: int           # Source atom
    to_atom_id: int             # Target atom
    link_type: str              # solves/supports/contradicts/continues/refines/relates_to/depends_on
    strength: float | None      # 0.0-1.0 optional confidence
```

**Key Features:**
- ‚úÖ 7 link types covering common relationships
- ‚úÖ Bidirectional semantics (from_atom_id points TO to_atom_id)
- ‚úÖ Optional strength field (unused in current implementation)
- ‚ö†Ô∏è No approval/review workflow

### 2. KnowledgeExtractionService (`backend/app/services/knowledge_extraction_service.py`)

#### Core Methods

**`extract_knowledge(messages: Sequence[Message]) -> KnowledgeExtractionOutput`**
- Input: 10-50 messages
- Process:
  1. Build prompt from message batch
  2. Initialize Pydantic AI agent with system prompt
  3. Call LLM (Ollama or OpenAI)
  4. Deserialize structured JSON output
- Output: Topics + Atoms with relationships
- **Confidence Threshold:** 0.7 (configurable)

**`save_topics(extracted_topics, session, confidence_threshold=0.7) -> dict[str, Topic]`**
- Deduplication: Check if topic name already exists
- Filter: Skip topics below confidence threshold
- Create: New Topic entities with auto-selected icon/color
- Return: Mapping of topic_name -> Topic entity
- **Logic:** Only REUSES existing topics; does NOT update existing

**`save_atoms(extracted_atoms, topic_map, session, confidence_threshold=0.7) -> list[Atom]`**
- Filter: Skip atoms below confidence threshold
- Create: New Atom entities with metadata
- Link: Create TopicAtom relationship with auto-filled note
- **Meta fields stored:**
  - `source: "llm_extraction"`
  - `message_ids: [...]` - source messages
- **Note:** Always includes confidence score

**`link_atoms(extracted_atoms, saved_atoms, session) -> int`**
- Build title->id mapping from saved atoms
- For each extracted atom:
  - Find target atoms by title
  - Create AtomLink if not already exists
  - Prevent self-referential links
- **Deduplication:** Checks existing_link before creating

**`update_messages(messages, topic_map, extracted_topics, session) -> int`**
- Build message_id->topic_id mapping
- For each message in batch:
  - Find first matching topic
  - Update Message.topic_id
- **Behavior:** First topic wins; no multi-topic assignment

#### LLM Integration

**System Prompt** (101 lines, well-structured):
```
You are a knowledge extraction expert analyzing conversation messages.

Guidelines for Topics:
- Concise names (2-4 words)
- 2-5 keywords
- Confidence 0.7+ for auto-creation

Guidelines for Atoms:
- Self-contained and actionable
- 7 types: problem/solution/decision/insight/question/pattern/requirement
- Link related atoms (7 link types)

Quality standards:
- Avoid duplicating existing knowledge
- Only extract clear, meaningful units
- Ensure atoms are genuinely atomic
```

**Provider Support:**
- ‚úÖ Ollama (local): `base_url` configuration
- ‚úÖ OpenAI (cloud): API key encryption
- ‚ö†Ô∏è No other provider types

**Model Settings:**
- Temperature: 0.3 (focused extraction)
- Max tokens: 4096
- Structured output: Pydantic models

### 3. API Endpoints

**POST /api/v1/knowledge/extract** (`backend/app/api/v1/knowledge.py`)
```python
@router.post("/extract", status_code=202)
async def trigger_knowledge_extraction(
    request: KnowledgeExtractionRequest,  # message_ids, agent_config_id
    db: DatabaseDep
) -> KnowledgeExtractionResponse
```

**Request Validation:**
- `message_ids`: 1-100 items (10-50 recommended)
- `agent_config_id`: Must exist, must be active
- Error codes: 404 (not found), 400 (not active)

**Response:**
- Status: 202 Accepted (async)
- Returns: message count + agent_config_id

**Missing Endpoints:**
- ‚ùå `/topics/{id}/atoms` - Get atoms for topic
- ‚ùå `/atoms/high-confidence` - Filter by confidence
- ‚ùå `/atoms/approve` - Bulk approval workflow
- ‚ùå `/atoms/merge` - Deduplication/consolidation
- ‚ùå `/knowledge/review` - Review queue interface
- ‚ùå `/atoms/versions` - History/audit trail

### 4. Background Task (`backend/app/tasks.py`)

#### Auto-Trigger Logic
```python
async def queue_knowledge_extraction_if_needed(message_id: int, db: Any):
    # Count unprocessed messages in last 24 hours
    unprocessed_count = db.query(Message)
        .where(Message.topic_id.is_(None))
        .where(Message.sent_at >= cutoff_time)
        .count()
    
    if unprocessed_count >= KNOWLEDGE_EXTRACTION_THRESHOLD (10):
        # Find active "knowledge_extractor" agent config
        # Fetch up to 50 unprocessed messages
        # Queue background task
```

**Configuration:**
- Threshold: 10 messages
- Lookback: 24 hours
- Batch size: 50 messages
- Agent: Must be named "knowledge_extractor" and active

#### Background Task (`extract_knowledge_from_messages_task`)
```
1. Fetch messages from DB
2. Broadcast: extraction_started
3. Call KnowledgeExtractionService.extract_knowledge()
4. Save topics (confidence 0.7+)
5. Save atoms (confidence 0.7+)
6. Link atoms
7. Update Message.topic_id
8. Broadcast: topic_created (per topic)
9. Broadcast: atom_created (per atom)
10. Broadcast: extraction_completed
11. On error: broadcast extraction_failed
```

**WebSocket Events Broadcast:** 5 event types
- `knowledge.extraction_started`
- `knowledge.topic_created`
- `knowledge.atom_created`
- `knowledge.extraction_completed`
- `knowledge.extraction_failed`

### 5. Data Flow: Message Ingestion

**Flow:**
```
1. save_telegram_message() task receives Telegram message
2. Parses user, content, timestamp
3. Creates Message entity with topic_id=NULL
4. Flushes to DB
5. Calls queue_knowledge_extraction_if_needed(message_id)
6. Check: Is unprocessed_count >= 10?
   - YES: Queue extract_knowledge_from_messages_task
   - NO: Continue
```

**Integration Points:**
- `backend/app/tasks.py` line 173: Auto-trigger called after message creation
- `backend/app/models/enums.py`: AnalysisStatus enum for message.analysis_status

---

## Frontend Implementation

### Components

#### Atoms Feature (`frontend/src/features/atoms/`)

**CreateAtomDialog.tsx**
- Purpose: Create manual atoms
- Fields:
  - Type (select from 7 types)
  - Title (text input)
  - Content (textarea)
  - Confidence (optional slider)
  - User approved (checkbox)
- API call: `atomService.createAtom()`
- Status: ‚ö†Ô∏è **MINIMAL** - Basic dialog only

**AtomCard.tsx**
- Purpose: Display atom summary
- Shows:
  - Type badge with color coding
  - Confidence percentage
  - Title (2 lines max)
  - Content (3 lines max)
  - Approval badge if approved
- Status: ‚úÖ Functional

**AtomCard Color Coding:**
```typescript
problem: 'badge-error'      // Red
solution: 'badge-success'   // Green
decision: 'badge-info'      // Blue
question: 'badge-warning'   // Orange
insight/pattern: 'badge-purple'
requirement: 'badge-info'
```

#### Topics Feature (`frontend/src/features/topics/`)

**TopicCard.tsx (in Dashboard)**
- Shows recent topics with:
  - Icon (auto-selected Heroicon)
  - Name
  - Message count
  - Atom count
  - Last activity time

**HexColorPicker.tsx**
- Color selection UI for topic customization
- Supports hex validation

**RecentTopics.tsx**
- Lists recent topics with activity metrics
- Filters by time period (today/yesterday/week/month)

### API Services

**atomService.ts** - Complete CRUD
```typescript
‚úÖ listAtoms(skip, limit)
‚úÖ getAtomById(id)
‚úÖ createAtom(data)
‚úÖ updateAtom(id, data)
‚úÖ deleteAtom(id)
‚úÖ getAtomsByTopic(topicId)
‚úÖ linkAtomToTopic(atomId, topicId)
‚úÖ unlinkAtomFromTopic(atomId, topicId)
‚úÖ createAtomLink(data)
‚úÖ getAtomLinks(atomId)
‚úÖ deleteAtomLink(fromId, toId)
```

**topicService.ts** - Partial implementation
```typescript
‚úÖ listTopics()
‚úÖ getTopicById(id)
‚ö†Ô∏è getAvailableColors() - Endpoint doesn't exist
‚úÖ updateTopic(id, data)
‚ö†Ô∏è suggestColor(id) - Endpoint doesn't exist
‚úÖ updateTopicColor(id, color)
```

### Type Definitions

**Atom Type** (`frontend/src/features/atoms/types/index.ts`)
```typescript
interface Atom {
  id: number
  type: AtomType
  title: string
  content: string
  confidence: number | null
  user_approved: boolean
  meta: Record<string, any> | null
  embedding: number[] | null
  created_at: string
  updated_at: string
}
```

**Topic Type** (`frontend/src/features/topics/types/index.ts`)
```typescript
interface Topic {
  id: number
  name: string
  description: string
  icon: string | null
  color: string | null
  created_at: string
  updated_at: string
}
```

### Missing Frontend Components

- ‚ùå **Knowledge Review Dashboard** - No centralized UI for reviewing extracted knowledge
- ‚ùå **Atom Approval Workflow** - No batch approval/rejection interface
- ‚ùå **Topic Management** - Limited creation/editing UI
- ‚ùå **Knowledge Graph Visualization** - No atom relationship graph display
- ‚ùå **Confidence Filtering** - No UI to filter atoms by confidence
- ‚ùå **Deduplication Interface** - No UI for merging similar atoms/topics
- ‚ùå **Extraction History** - No UI to view/replay extractions
- ‚ùå **Extraction Settings** - No UI to configure thresholds

---

## Documentation Analysis

### User Documentation

**docs/content/en/topics.md** (265 lines)
- ‚úÖ Excellent overview of Context Spaces as central hub
- ‚úÖ Use cases (project management, knowledge base, integration hub)
- ‚úÖ Lifecycle states (Active, Archived, Connected)
- ‚úÖ Best practices (start broad, let AI help, archive completed work)
- ‚ö†Ô∏è Does NOT explain current implementation details
- ‚ö†Ô∏è No approval/review workflow mentioned (gap!)

**docs/content/en/knowledge-extraction.md** (428 lines)
- ‚úÖ Comprehensive feature guide
- ‚úÖ Real-world example with scenario
- ‚úÖ Atom types and link types documented
- ‚úÖ Configuration settings explained
- ‚úÖ Troubleshooting section
- ‚ö†Ô∏è No proposal/approval workflow mentioned
- ‚ö†Ô∏è Assumes manual review of low-confidence atoms (not implemented)

**docs/content/en/api/knowledge.md** (654 lines)
- ‚úÖ Complete API reference
- ‚úÖ WebSocket event documentation
- ‚úÖ Data schema definitions (ExtractedTopic, ExtractedAtom)
- ‚úÖ Integration examples (Python + TypeScript)
- ‚ö†Ô∏è Rate limit section says "no hard limits currently enforced"

**docs/content/en/architecture/knowledge-extraction.md** (780 lines)
- ‚úÖ System architecture diagrams
- ‚úÖ Data model ER diagram
- ‚úÖ Pipeline workflow (5 phases)
- ‚úÖ Testing strategy and coverage stats
- ‚úÖ Performance optimization tips
- ‚ö†Ô∏è Mentions "Future Enhancements" including approval workflow

### Documentation Gaps

**Mentioned but NOT Implemented:**
1. Line 309: "Review low-confidence items manually"
   - No UI for this
   - No approval workflow
2. Line 314: "Approve important atoms"
   - Flag exists (`user_approved`) but no approval interface
3. Line 313: "Edit auto-generated content"
   - Partial UI support only
4. Future enhancements list includes atom versioning, merging, custom rules

---

## Critical Gap Analysis

### What's Documented vs What's Implemented

| Feature | Documented | Implemented | UI Exists | Workflow |
|---------|-----------|-------------|-----------|----------|
| Auto-extraction | ‚úÖ Yes | ‚úÖ Yes | ‚úÖ Yes (WS events) | ‚úÖ Full |
| Topic creation | ‚úÖ Yes | ‚úÖ Yes | ‚ö†Ô∏è Limited | ‚úÖ Auto only |
| Atom creation | ‚úÖ Yes | ‚úÖ Yes | ‚ö†Ô∏è Dialog only | ‚úÖ Manual + Auto |
| Relationships | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No | ‚ö†Ô∏è Auto only |
| Approval workflow | ‚úÖ Mentioned | ‚ùå No | ‚ùå No | ‚ùå Missing |
| Versioning | ‚úÖ Future plan | ‚ùå No | ‚ùå No | ‚ùå Missing |
| Merging/consolidation | ‚úÖ Future plan | ‚ùå No | ‚ùå No | ‚ùå Missing |
| Confidence filtering | ‚úÖ Yes | ‚úÖ Threshold | ‚ùå No UI | ‚ö†Ô∏è Backend only |

### Missing Critical Features

1. **No Proposal/Review Workflow**
   - Tasks have TaskProposal with status (pending/approved/rejected/merged)
   - Topics/Atoms have NO equivalent
   - Gap: Auto-extracted knowledge goes straight to DB without review

2. **No Versioning/History**
   - TaskProposal tracks created_at and reviewed_at
   - Atoms have created_at/updated_at but no revision history
   - No audit trail for changes

3. **No Consolidation Mechanism**
   - Documentation mentions "avoid duplicating existing knowledge"
   - Implementation checks existing by name only
   - No fuzzy matching or merging UI

4. **Confidence Threshold Not User-Facing**
   - Backend uses 0.7 default (configurable in code)
   - No API to adjust per-extraction
   - Low-confidence atoms created but not surfaced differently

5. **No Review Interface**
   - Extracted atoms appear immediately in system
   - No "review queue" or "approval dashboard"
   - Users can't easily see what was just extracted

6. **No Deduplication Detection**
   - AtomLink stores relationships
   - No "potential duplicate" scoring
   - No UI to merge similar atoms

---

## Current Workflow: Implementation Reality

### Actual Message ‚Üí Topic/Atom Flow

```
TELEGRAM MESSAGE ARRIVES
   ‚Üì
save_telegram_message() task
   ‚îú‚îÄ Create User (if new)
   ‚îú‚îÄ Create Message (topic_id=NULL)
   ‚îú‚îÄ Flush to DB
   ‚îî‚îÄ Call queue_knowledge_extraction_if_needed()
   ‚Üì
count unprocessed messages in last 24h
   ‚îú‚îÄ If count < 10: STOP (wait for more)
   ‚îî‚îÄ If count >= 10: CONTINUE
   ‚Üì
extract_knowledge_from_messages_task (background job)
   ‚îú‚îÄ Fetch agent_config "knowledge_extractor"
   ‚îú‚îÄ Fetch up to 50 unprocessed messages
   ‚îú‚îÄ Call KnowledgeExtractionService.extract_knowledge()
   ‚îÇ  ‚îî‚îÄ LLM analyzes batch with Pydantic AI agent
   ‚îÇ     ‚îî‚îÄ Returns: Topics(1-3) + Atoms(5-10)
   ‚îú‚îÄ Broadcast: extraction_started
   ‚îú‚îÄ save_topics()
   ‚îÇ  ‚îî‚îÄ For each topic with confidence >= 0.7:
   ‚îÇ     ‚îú‚îÄ Check if name exists
   ‚îÇ     ‚îú‚îÄ If exists: REUSE
   ‚îÇ     ‚îî‚îÄ If new: CREATE with auto icon/color
   ‚îÇ     ‚îî‚îÄ Broadcast: topic_created
   ‚îú‚îÄ save_atoms()
   ‚îÇ  ‚îî‚îÄ For each atom with confidence >= 0.7:
   ‚îÇ     ‚îú‚îÄ Check if topic exists (from save_topics)
   ‚îÇ     ‚îú‚îÄ CREATE atom
   ‚îÇ     ‚îú‚îÄ CREATE TopicAtom relationship
   ‚îÇ     ‚îî‚îÄ Broadcast: atom_created
   ‚îú‚îÄ link_atoms()
   ‚îÇ  ‚îî‚îÄ For each extracted relationship:
   ‚îÇ     ‚îî‚îÄ CREATE AtomLink
   ‚îú‚îÄ update_messages()
   ‚îÇ  ‚îî‚îÄ For each message in batch:
   ‚îÇ     ‚îî‚îÄ SET Message.topic_id = first matching topic
   ‚îî‚îÄ Broadcast: extraction_completed

RESULT:
   ‚îú‚îÄ 1-3 new Topics created
   ‚îú‚îÄ 5-10 new Atoms created
   ‚îú‚îÄ Several AtomLinks created
   ‚îú‚îÄ All 50 messages now have topic_id set
   ‚îî‚îÄ All unprocessed messages marked as processed
```

### Key Implementation Details

**Threshold Behavior:**
- Requires 10 UNPROCESSED (topic_id IS NULL) messages
- Resets timer each time extraction runs
- Processes oldest 50 unprocessed messages
- Takes up to 24 hours to find trigger again

**Confidence Filtering:**
- Topics with confidence < 0.7: SKIPPED (warning logged)
- Atoms referencing low-confidence topic: SKIPPED
- Low-confidence atoms: SKIPPED
- **These atoms are NOT stored or queued for review**

**Topic Deduplication:**
- By exact name match only
- No fuzzy matching
- If topic exists: REUSED (not updated)
- If topic missing: NOT created

**Message Topic Assignment:**
- Each message assigned to FIRST matching topic only
- Multiple topics per batch: only one wins
- No multi-topic assignment

---

## Backend Services & APIs

### Service Classes

**TopicCRUD** (`backend/app/services/topic_crud.py`)
```python
‚úÖ list(skip, limit)
‚úÖ get(topic_id)
‚úÖ create(topic_create)
‚úÖ update(topic_id, topic_update)
‚úÖ delete(topic_id)
‚úÖ recent(period, limit)  # With message/atom counts
```

**AtomCRUD** (`backend/app/services/atom_crud.py`)
```python
‚úÖ list(skip, limit)
‚úÖ get(atom_id)
‚úÖ create(atom_create)
‚úÖ update(atom_id, atom_update)
‚úÖ delete(atom_id)
‚úÖ list_by_topic(topic_id)
‚úÖ link_to_topic(atom_id, topic_id, note, position)
‚úÖ unlink_from_topic(atom_id, topic_id)
‚úÖ get_links(atom_id)
‚úÖ create_link(from_id, to_id, link_type, strength)
```

### API Endpoints

**Topics Endpoints** (`backend/app/api/v1/topics.py`)
```python
‚úÖ GET /topics                          # List all
‚úÖ GET /topics/{id}                     # Get one
‚úÖ GET /topics/icons                    # List available icons
‚úÖ GET /topics/recent                   # Recent with metrics
‚úÖ GET /topics/{id}/atoms               # Get atoms for topic
‚úÖ POST /topics                         # Create
‚úÖ PATCH /topics/{id}                   # Update
‚úÖ DELETE /topics/{id}                  # Delete
‚ùå PUT /topics/{id}/atoms               # Bulk assign atoms
```

**Atoms Endpoints** (`backend/app/api/v1/atoms.py`)
```python
‚úÖ GET /atoms                           # List all
‚úÖ GET /atoms/{id}                      # Get one
‚úÖ POST /atoms                          # Create
‚úÖ PATCH /atoms/{id}                    # Update
‚úÖ DELETE /atoms/{id}                   # Delete
‚úÖ POST /atoms/{id}/topics/{topicId}   # Link to topic
‚úÖ DELETE /atoms/{id}/topics/{topicId} # Unlink from topic
‚ùå GET /atoms?confidence_min=X          # Filter by confidence
‚ùå GET /atoms?type=problem              # Filter by type
‚ùå POST /atoms/merge                    # Merge duplicates
```

**Knowledge Endpoints** (`backend/app/api/v1/knowledge.py`)
```python
‚úÖ POST /knowledge/extract              # Trigger extraction
‚ùå GET /knowledge/extractions           # View history
‚ùå POST /knowledge/review               # Review queue
‚ùå POST /knowledge/atoms/approve        # Batch approve
```

---

## Testing Coverage

### Test Statistics

**Service Tests** (`backend/tests/services/test_knowledge_extraction_service.py`)
- 22 test cases
- 96% coverage of service methods
- Mocked LLM calls with deterministic output
- Database isolation with SQLite

**API Tests** (`backend/tests/api/v1/test_knowledge_extraction.py`)
- 14 test cases covering:
  - Successful extraction trigger
  - Request validation
  - Provider validation
  - Error handling (404, 400, 422)

**Task Tests** (`backend/tests/tasks/test_knowledge_extraction_task.py`)
- 6 test cases:
  - Threshold detection
  - 24-hour window filtering
  - 50-message batch limit
  - Auto-queueing logic

**Total Coverage:** ~96% (full pipeline tested)

### What's Tested

‚úÖ LLM extraction with mocked responses
‚úÖ Topic creation and deduplication
‚úÖ Atom creation with relationships
‚úÖ AtomLink creation
‚úÖ Message topic assignment
‚úÖ Confidence threshold filtering
‚úÖ Provider validation and error handling
‚úÖ Background task queueing

### What's NOT Tested

‚ùå Frontend components (no Jest/Playwright tests found)
‚ùå WebSocket event delivery
‚ùå Real Ollama/OpenAI API calls
‚ùå Concurrent extraction requests
‚ùå Large batch processing (>1000 messages)
‚ùå Integration with Telegram webhook

---

## Recommendations & Roadmap

### Priority 1: Critical (Do First)

#### 1.1 Implement Approval Workflow for Knowledge
**Why:** Auto-extraction creates noise; no review mechanism

**Implementation:**
```python
# Add to Atom model:
status: str = "pending"  # pending/approved/rejected/archived
reviewed_by_user_id: int | None
reviewed_at: datetime | None
review_notes: str | None

# Add endpoints:
POST /atoms/{id}/approve
POST /atoms/{id}/reject
POST /atoms/batch/approve (for bulk operations)

# Add frontend:
- ReviewQueue component showing pending atoms
- ApprovalDialog with confidence display
- Batch action buttons (approve/reject/merge)
```

#### 1.2 Add Deduplication & Merging
**Why:** Multiple extractions may create similar atoms

**Implementation:**
```python
# Service method:
async def find_similar_atoms(atom_id, similarity_threshold=0.8)
  # Use embedding similarity or semantic search
  # Return list of potential duplicates

# Endpoint:
POST /atoms/{id}/merge-into/{target_id}
  # Move all links from source to target
  # Consolidate metadata
  # Archive source

# Frontend:
- SimilarAtomsPanel showing potential duplicates
- MergeDialog for consolidation
```

#### 1.3 Implement Extraction History & Auditability
**Why:** Need to track what changed and when

**Implementation:**
```python
# New model:
class AtomRevision:
    atom_id: int
    version: int
    title: str
    content: str
    changed_by_user_id: int | None  # NULL if auto-extraction
    changed_at: datetime
    change_summary: str

# Track changes in UPDATE operations:
async def update_atom(id, data):
    old_atom = get(id)
    new_atom = update(id, data)
    create_revision(id, old_atom, new_atom, user_id)
```

### Priority 2: High (Next Sprint)

#### 2.1 Confidence Filtering UI
**Why:** Low-confidence atoms currently hidden from users

**Implementation:**
```typescript
// Frontend filter:
<ConfidenceFilter 
  min={0.5} 
  max={1.0} 
  onChange={(range) => refetch({confidence_min: range[0]})}
/>

// Backend:
GET /atoms?confidence_min=0.7
```

#### 2.2 Knowledge Review Dashboard
**Why:** Centralized interface for knowledge curation

**Features:**
- Review queue (pending atoms sorted by confidence)
- Atom detail view with source messages
- Confidence score visualization
- Duplicate detection indicator
- One-click approval/rejection
- Merge UI for duplicates

#### 2.3 Topic Consolidation
**Why:** Multiple topics may cover same theme

**Implementation:**
```python
# Service:
async def merge_topics(source_topic_id, target_topic_id):
    # Move all atoms from source to target
    # Update all messages pointing to source
    # Archive source topic

# Endpoint:
POST /topics/{id}/merge-into/{target_id}
```

### Priority 3: Medium (Future)

#### 3.1 Custom Extraction Rules
**Why:** One-size-fits-all system prompt may not work for all projects

**Implementation:**
```python
# Per-project prompt customization:
class ExtractionConfig:
    project_id: UUID
    custom_system_prompt: str
    temperature: float = 0.3
    confidence_threshold: float = 0.7
    atom_types_to_extract: list[str]  # Filter which types

# Use in extraction:
agent = PydanticAgent(
    system_prompt=config.custom_system_prompt or DEFAULT_PROMPT,
    ...
)
```

#### 3.2 Multi-Language Support
**Why:** Extract from non-English messages

**Implementation:**
```python
# Detect language:
language = detect(message.content)

# Multilingual prompt:
if language != 'en':
    system_prompt += f"\nProcess content in {language}"

# Store language metadata:
atom.meta['language'] = language
```

#### 3.3 Knowledge Export & Reports
**Why:** Users want to export extracted knowledge

**Features:**
- Export atoms as CSV/JSON
- Generate knowledge report (topics + atoms + relationships)
- Timeline view of discoveries
- Statistics dashboard

### Implementation Roadmap

```
WEEK 1 (Critical)
‚îú‚îÄ Approval workflow for atoms
‚îú‚îÄ Confidence filtering UI
‚îî‚îÄ Extraction history tracking

WEEK 2 (High Priority)
‚îú‚îÄ Knowledge Review Dashboard
‚îú‚îÄ Topic/atom merging UI
‚îî‚îÄ Similar atom detection

WEEK 3-4 (Medium)
‚îú‚îÄ Custom extraction rules
‚îú‚îÄ Multi-language support
‚îî‚îÄ Knowledge export/reports

ONGOING
‚îú‚îÄ Monitor & tune LLM prompts
‚îú‚îÄ Collect user feedback
‚îî‚îÄ Optimize performance
```

---

## Critical Findings

### 1. No Review Gate for Auto-Extracted Knowledge
**Status:** üî¥ CRITICAL

Atoms are created immediately without approval workflow. No way to:
- Review low-confidence extractions
- Prevent noise from entering system
- Audit changes
- Revert bad extractions

**Impact:** Knowledge base can become polluted with incorrect atoms

**Solution:** Implement approval status (pending/approved/rejected)

### 2. Topic Reuse Strategy Unclear
**Status:** üü° WARNING

Implementation reuses topics by exact name match ONLY:
```python
existing_topic = await session.execute(
    select(Topic).where(Topic.name == extracted_topic.name)
)
```

Problem:
- LLM might generate "API Design" and "API Design Decisions"
- Both would create separate topics
- No fuzzy matching or clustering

**Solution:** Implement semantic similarity check using embeddings

### 3. Low-Confidence Atoms Are Lost
**Status:** üü° WARNING

Atoms below 0.7 confidence are:
- NOT created in database
- NOT logged for review
- SILENTLY SKIPPED

Documentation says "Review low-confidence items manually" but:
- They're never persisted
- No UI to review them
- No way to recover if threshold was too high

**Solution:** Create low-confidence atoms with `status: "pending_review"`

### 4. Message Assigned to Single Topic Only
**Status:** üü° WARNING

Logic assigns message to FIRST matching topic:
```python
for msg_id in extracted_topic.related_message_ids:
    if msg_id in message_id_to_topic:
        logger.debug("...keeping first assignment")
    else:
        message_id_to_topic[msg_id] = topic_id
```

Problem:
- Batch contains 2 topics (API Design + Database)
- Message discussing both gets assigned to first only
- Context lost for second topic

**Solution:** Allow Message.topic_id to be NULL or support multi-topic assignment

### 5. No Frontend for Most Functionality
**Status:** üî¥ CRITICAL

Backend is 95% complete, but frontend has:
- ‚úÖ AtomCard (display only)
- ‚ö†Ô∏è CreateAtomDialog (manual creation only)
- ‚ùå No approval workflow UI
- ‚ùå No confidence filtering
- ‚ùå No merging interface
- ‚ùå No review queue
- ‚ùå No relationships visualization

**Impact:** Users can't effectively manage extracted knowledge

**Solution:** Build Knowledge Review Dashboard as high-priority feature

---

## Appendix: Code References

### Key Files Overview

| File | Lines | Purpose | Quality |
|------|-------|---------|---------|
| `backend/app/services/knowledge_extraction_service.py` | 495 | Core extraction logic | ‚úÖ Excellent |
| `backend/app/tasks.py` (1007-1155) | 148 | Background task | ‚úÖ Good |
| `backend/app/tasks.py` (19-86) | 68 | Auto-trigger logic | ‚úÖ Good |
| `backend/app/api/v1/knowledge.py` | 90 | REST API | ‚úÖ Good |
| `backend/app/models/topic.py` | 326 | Topic model + utils | ‚úÖ Good |
| `backend/app/models/atom.py` | 355 | Atom models + schemas | ‚úÖ Good |
| `backend/tests/services/test_knowledge_extraction_service.py` | 592 | Service tests | ‚úÖ Excellent |
| `backend/tests/api/v1/test_knowledge_extraction.py` | 228 | API tests | ‚úÖ Good |
| `backend/tests/tasks/test_knowledge_extraction_task.py` | 253 | Task tests | ‚úÖ Good |
| `frontend/src/features/atoms/components/AtomCard.tsx` | 71 | Atom display | ‚úÖ Good |
| `frontend/src/features/atoms/api/atomService.ts` | 164 | Atom API client | ‚úÖ Good |

### Database Schema Diagram

```sql
-- Topics (discussion themes)
CREATE TABLE topics (
  id INT PRIMARY KEY,
  name VARCHAR(100) UNIQUE NOT NULL,
  description TEXT NOT NULL,
  icon VARCHAR(50),
  color VARCHAR(7),
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- Atoms (atomic knowledge units)
CREATE TABLE atoms (
  id INT PRIMARY KEY,
  type VARCHAR(20) NOT NULL,  -- problem/solution/decision/insight/question/pattern/requirement
  title VARCHAR(200) NOT NULL,
  content TEXT NOT NULL,
  confidence FLOAT,           -- 0.0-1.0 (LLM extraction confidence)
  user_approved BOOLEAN DEFAULT FALSE,
  meta JSONB,                -- {source, message_ids, ...}
  embedding VECTOR(1536),    -- pgvector for semantic search
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);

-- Many-to-many: Topics <-> Atoms
CREATE TABLE topic_atoms (
  topic_id INT (FK topics.id),
  atom_id INT (FK atoms.id),
  position INT,              -- Display order
  note TEXT,                 -- Auto-filled with extraction confidence
  created_at TIMESTAMP,
  PRIMARY KEY (topic_id, atom_id)
);

-- Atom relationships
CREATE TABLE atom_links (
  from_atom_id INT (FK atoms.id),
  to_atom_id INT (FK atoms.id),
  link_type VARCHAR(20),     -- solves/supports/contradicts/continues/refines/relates_to/depends_on
  strength FLOAT,            -- Optional: 0.0-1.0
  created_at TIMESTAMP,
  PRIMARY KEY (from_atom_id, to_atom_id)
);

-- Messages updated with topic assignment
ALTER TABLE messages ADD COLUMN topic_id INT (FK topics.id);
```

### Configuration Constants

```python
# backend/app/tasks.py
KNOWLEDGE_EXTRACTION_THRESHOLD = 10        # Messages before auto-trigger
KNOWLEDGE_EXTRACTION_LOOKBACK_HOURS = 24   # Time window for unprocessed

# backend/app/services/knowledge_extraction_service.py
confidence_threshold = 0.7                 # Minimum for auto-creation
temperature = 0.3                          # LLM focus level (lower = more focused)
max_tokens = 4096                          # Max response length

# backend/app/models/topic.py
TOPIC_ICONS = {...16 keyword mappings...} # Auto icon selection
ICON_COLORS = {...16 icon->color...}      # Auto color selection
```

### WebSocket Event Schemas

```typescript
// All events have this structure:
{
  "type": "knowledge.{event_type}",
  "data": {...}
}

// Events:
extraction_started: { message_count, agent_config_id, agent_name }
topic_created: { topic_id, topic_name }
atom_created: { atom_id, atom_title, atom_type }
extraction_completed: { message_count, topics_created, atoms_created, links_created, messages_updated }
extraction_failed: { error }
```

---

## Conclusion

The Knowledge Extraction system is **80% complete** for a production MVP:

### What Works Great ‚úÖ
- Auto-extraction after 10 messages (configurable)
- LLM integration via Pydantic AI + structured output
- Real-time WebSocket broadcasts
- Comprehensive test coverage (96%)
- Clean service architecture
- Type-safe Python and TypeScript code
- Good documentation (concept + API)

### What's Missing üî¥
- Approval/review workflow for auto-extracted atoms
- Versioning and audit trails
- Consolidation/merging of similar atoms
- Frontend review dashboard
- Confidence filtering UI
- Deduplication detection

### Recommended Next Steps

1. **Immediately:** Add approval status to Atom model + endpoints
2. **This sprint:** Build Knowledge Review Dashboard component
3. **Next sprint:** Implement atom merging and topic consolidation
4. **Later:** Add custom extraction rules, multi-language support, reports

The foundation is solid. The system is ready for production with the approval workflow addition as a critical follow-up.

---

**Report Generated:** October 23, 2025
**Investigation Scope:** Very Thorough (all components analyzed)
**Status:** Complete
