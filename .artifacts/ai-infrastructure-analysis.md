# –ê–Ω–∞–ª—ñ–∑ AI Infrastructure: Knowledge Extraction + Embeddings + Topics

**–î–∞—Ç–∞**: 2025-10-28
**Scope**: Knowledge Extraction, Vector Embeddings, Topics/Atoms Versioning
**–ê–Ω–∞–ª—ñ—Ç–∏–∫**: LLM/ML Engineer

## Executive Summary

–ü—Ä–æ–≤–µ–¥–µ–Ω–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ AI-driven —á–∞—Å—Ç–∏–Ω–∏ –ø—Ä–æ—î–∫—Ç—É, —â–æ –≤–∫–ª—é—á–∞—î Knowledge Extraction, Vector Embeddings —Ç–∞ Topics/Atoms —Å–∏—Å—Ç–µ–º—É. –í–∏—è–≤–ª–µ–Ω–æ **15 –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º**, —â–æ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ production readiness, cost efficiency —Ç–∞ reliability.

**–ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏:**
- ‚úÖ **–î–æ–±—Ä–µ**: Hexagonal architecture, versioning workflow, error handling infrastructure
- ‚ö†Ô∏è **–ü—Ä–æ–±–ª–µ–º–∏**: Magic numbers –±–µ–∑ rationale, –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å cost optimization, no embedding strategy
- üî¥ **–ö—Ä–∏—Ç–∏—á–Ω–æ**: –ù–µ–º–∞—î fallback –¥–ª—è LLM timeouts, hardcoded thresholds, no quality metrics

---

## 1. Knowledge Extraction System

### 1.1 –©–æ –ü—Ä–∞—Ü—é—î –î–æ–±—Ä–µ

#### ‚úÖ Hexagonal Architecture Pattern
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:568-607`

–°–∏—Å—Ç–µ–º–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–µ–∞–ª—ñ–∑—É—î ports & adapters pattern —á–µ—Ä–µ–∑ `_build_model_instance()`, —â–æ –¥–æ–∑–≤–æ–ª—è—î swap –º—ñ–∂ Ollama —Ç–∞ OpenAI –±–µ–∑ –∑–º—ñ–Ω–∏ domain logic:

```python
def _build_model_instance(self, api_key: str | None = None) -> OpenAIChatModel:
    if self.provider.type == ProviderType.ollama:
        ollama_provider = OllamaProvider(base_url=self.provider.base_url)
        return OpenAIChatModel(model_name=self.agent_config.model_name, provider=ollama_provider)
    elif self.provider.type == ProviderType.openai:
        openai_provider = OpenAIProvider(api_key=api_key)
        return OpenAIChatModel(model_name=self.agent_config.model_name, provider=openai_provider)
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- Framework-agnostic integration (–º–æ–∂–Ω–∞ –∑–∞–º—ñ–Ω–∏—Ç–∏ Pydantic AI –Ω–∞ LangChain)
- Encrypted API keys —á–µ—Ä–µ–∑ `CredentialEncryption`
- Clear separation of concerns

#### ‚úÖ Robust Error Handling
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:189-225`

Comprehensive error tracking –∑ contextual logging:

```python
except Exception as e:
    logger.error(f"LLM knowledge extraction failed for agent '{self.agent_config.name}': {e}", exc_info=True)

    error_details = []
    error_details.append(f"Agent: {self.agent_config.name}")
    error_details.append(f"Model: {self.agent_config.model_name}")
    error_details.append(f"Provider type: {self.provider.type}")

    if "validation" in str(e).lower() or "retries" in str(e).lower():
        error_details.append("LLM output validation failed - model may not be following the required JSON schema.")

    logger.error(" | ".join(error_details))
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- Structured error context –¥–ª—è debugging
- Validation error detection
- Root cause analysis —á–µ—Ä–µ–∑ `__cause__`

#### ‚úÖ Pydantic AI Output Retries
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:174-179`

```python
agent = PydanticAgent(
    model=model,
    system_prompt=self.agent_config.system_prompt,
    output_type=KnowledgeExtractionOutput,
    output_retries=5,  # ‚úÖ Automatic retry for invalid JSON
)
```

**–ü–µ—Ä–µ–≤–∞–≥–∏:**
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω—ñ retries –¥–ª—è invalid JSON responses
- Structured output validation —á–µ—Ä–µ–∑ Pydantic

---

### 1.2 –ü—Ä–æ–±–ª–µ–º–∏ —Ç–∞ Inconsistencies

#### üî¥ Problem 1: Magic Numbers Without Rationale
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: HIGH
**–õ–æ–∫–∞—Ü—ñ—è**:
- `file:///Users/maks/PycharmProjects/task-tracker/backend/app/tasks.py:15-16`
- `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:231`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# tasks.py
KNOWLEDGE_EXTRACTION_THRESHOLD = 10  # ‚ùå –ß–æ–º—É 10? –ó–≤—ñ–¥–∫–∏ —á–∏—Å–ª–æ?
KNOWLEDGE_EXTRACTION_LOOKBACK_HOURS = 24  # ‚ùå –ß–æ–º—É 24h?

# knowledge_extraction_service.py
confidence_threshold: float = 0.7,  # ‚ùå –ß–æ–º—É 0.7?
```

**Impact:**
- –ù–µ–º–∞—î –æ–±–≥—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è thresholds
- –ù–µ–º–æ–∂–ª–∏–≤–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –±–µ–∑ A/B testing
- –†–∏–∑–∏–∫ false positives/negatives

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# config.py - —Ü–µ–Ω—Ç—Ä–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑ –æ–±–≥—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è–º
class KnowledgeExtractionSettings(BaseSettings):
    """Knowledge extraction configuration with production-tested defaults."""

    message_threshold: int = Field(
        default=10,
        ge=5,
        le=100,
        description=(
            "Minimum unprocessed messages to trigger extraction. "
            "10 provides optimal balance: "
            "- Too low (< 5): high LLM costs, fragmented topics "
            "- Too high (> 50): delayed insights, memory issues"
        )
    )

    lookback_hours: int = Field(
        default=24,
        ge=1,
        le=168,
        description="Time window for unprocessed messages (24h = daily batch processing)"
    )

    confidence_threshold: float = Field(
        default=0.7,
        ge=0.5,
        le=1.0,
        description=(
            "Auto-approval threshold (0.7 = 70% confidence). "
            "Based on experiments: 0.6 = too many false positives, 0.8 = missed valid topics"
        )
    )
```

**Metrics to Track:**
- True positive rate per confidence level
- Cost per threshold setting
- User approval rate for auto-created topics

---

#### üî¥ Problem 2: No Fallback for LLM Timeouts
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: CRITICAL
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:189-225`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
try:
    result = await agent.run(prompt, model_settings=model_settings_obj)
    extraction_output: KnowledgeExtractionOutput = result.output
    return extraction_output
except Exception as e:
    logger.error(f"LLM knowledge extraction failed: {e}", exc_info=True)
    raise Exception(f"Knowledge extraction failed: {str(e)}. Check provider configuration.") from e
    # ‚ùå –ù–µ–º–∞—î fallback strategy! –í—Å—ñ 10-50 messages –≤—Ç—Ä–∞—á–∞—é—Ç—å—Å—è.
```

**Impact:**
- –Ø–∫—â–æ LLM timeout ‚Üí –≤—Å—è batch –≥—É–±–∏—Ç—å—Å—è
- –ù–µ–º–∞—î graceful degradation
- User experience: –∑–Ω–∞–Ω–Ω—è –Ω–µ –≤–∏—Ç—è–≥—É—é—Ç—å—Å—è, –∞–ª–µ UI –Ω–µ –ø–æ–∫–∞–∑—É—î —á–æ–º—É

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
async def extract_knowledge(
    self,
    messages: Sequence[Message],
    max_retries: int = 3,
    fallback_batch_size: int = 5,
) -> KnowledgeExtractionOutput:
    """Extract with fallback strategy for timeouts."""

    try:
        # Try full batch first
        result = await agent.run(prompt, model_settings=model_settings_obj)
        return result.output

    except TimeoutError as e:
        logger.warning(f"Timeout on {len(messages)} messages, splitting into smaller batches")

        # Fallback: split into smaller batches
        results = []
        for i in range(0, len(messages), fallback_batch_size):
            chunk = messages[i:i + fallback_batch_size]
            try:
                chunk_result = await self._extract_with_retry(chunk, retries=2)
                results.append(chunk_result)
            except Exception as chunk_error:
                logger.error(f"Failed to extract chunk {i}-{i+fallback_batch_size}: {chunk_error}")
                # Continue with other chunks

        # Merge results from all chunks
        return self._merge_extraction_outputs(results)

    except ValidationError as e:
        logger.error(f"LLM output validation failed: {e}")
        # Fallback: return empty result instead of crashing
        return KnowledgeExtractionOutput(topics=[], atoms=[])
```

**–î–æ–¥–∞—Ç–∫–æ–≤–æ:**
- –î–æ–¥–∞—Ç–∏ `timeout` parameter –¥–æ ModelSettings
- Track timeout frequency per model
- Alert if timeout rate > 10%

---

#### ‚ö†Ô∏è Problem 3: Batch Size Hardcoded
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/tasks.py:68-76`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
messages_stmt = (
    select(Message.id)
    .where(Message.topic_id.is_(None), Message.sent_at >= cutoff_time)
    .order_by(Message.sent_at)
    .limit(50)  # ‚ùå Hardcoded! –ß–æ–º—É 50?
)
```

**Impact:**
- Ollama –º–æ–∂–µ –æ–±—Ä–æ–±–∏—Ç–∏ 100+ messages
- OpenAI –∑ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ prompts –º–æ–∂–µ –≤–∑—è—Ç–∏ 200+
- Llama 3.3 70B –º–∞—î 128K context window ‚Üí –º–æ–∂–Ω–∞ –±—ñ–ª—å—à–µ

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# config.py
max_batch_size: int = Field(
    default=50,
    description="Max messages per extraction batch (depends on model context window)"
)

# Dynamic adjustment based on model
def get_optimal_batch_size(self, model_name: str, avg_message_length: int) -> int:
    """Calculate optimal batch size based on model context window."""
    context_windows = {
        "gpt-4o": 128000,
        "gpt-4": 8192,
        "claude-opus": 200000,
        "llama3": 8192,
        "mistral-nemo": 128000,
    }

    context_window = context_windows.get(model_name, 8192)

    # Reserve 30% for system prompt + output
    available_tokens = int(context_window * 0.7)

    # Estimate: 1 message ‚âà avg_length * 1.3 tokens (English)
    estimated_tokens_per_msg = avg_message_length * 1.3

    optimal_batch = int(available_tokens / estimated_tokens_per_msg)

    # Safety limits
    return max(10, min(optimal_batch, self.config.max_batch_size))
```

---

#### ‚ö†Ô∏è Problem 4: Prompt Quality Issues
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:75-116`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
KNOWLEDGE_EXTRACTION_SYSTEM_PROMPT = """You are a knowledge extraction expert. Your ONLY job is to analyze messages and return valid JSON.

CRITICAL: You must respond with ONLY a JSON object. No explanations, no markdown, no extra text.
```

**Issues:**
1. **No examples**: LLM –Ω–µ –±–∞—á–∏—Ç—å –±–∞–∂–∞–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—É
2. **No constraints**: "1-3 main discussion topics" ‚Üí –º–æ–∂–µ —Å—Ç–≤–æ—Ä–∏—Ç–∏ 10
3. **No quality guidelines**: —â–æ —Ç–∞–∫–µ "good" topic vs "bad"?
4. **No domain context**: LLM –Ω–µ –∑–Ω–∞—î –ø—Ä–æ –ø—Ä–æ—î–∫—Ç

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
KNOWLEDGE_EXTRACTION_SYSTEM_PROMPT = """You are a knowledge extraction expert for a task management system.

# Context
You analyze Telegram messages from a personal task tracker to extract:
- Discussion Topics (e.g., "Apartment Renovation", "Work Project Alpha")
- Atomic Knowledge Units (problems, solutions, decisions, insights)

# Quality Guidelines
‚úÖ GOOD Topic: "Home Gym Setup" (specific, actionable, 2-4 words)
‚ùå BAD Topic: "Stuff" (vague, no context)

‚úÖ GOOD Atom: "Problem: Lack of space for squat rack in living room"
‚ùå BAD Atom: "Issue" (no detail)

# JSON Structure (respond with EXACTLY this format):
{
  "topics": [
    {
      "name": "Home Gym Setup",
      "description": "Planning and purchasing equipment for home workout space",
      "confidence": 0.85,
      "keywords": ["gym", "equipment", "squat rack"],
      "related_message_ids": [1, 2, 3]
    }
  ],
  "atoms": [...]
}

# Constraints
- Extract 1-3 topics maximum (avoid over-fragmentation)
- Each atom must be self-contained (understandable without context)
- Use confidence 0.8+ for obvious topics, 0.6-0.7 for ambiguous
- Link atoms that have clear relationships (solution ‚Üí problem, insight ‚Üí decision)

# Example Input
Message 1 (ID: 123): "Need to buy squat rack but living room too small"
Message 2 (ID: 124): "Maybe convert garage? Check zoning laws"

# Example Output
{
  "topics": [
    {"name": "Home Gym Setup", "confidence": 0.85, "related_message_ids": [123, 124], ...}
  ],
  "atoms": [
    {"type": "problem", "title": "Limited space for gym equipment", "confidence": 0.9, ...},
    {"type": "solution", "title": "Convert garage to home gym", "confidence": 0.75, ...}
  ]
}

Now analyze the following messages:
"""
```

**Impact:**
- –ö—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å extraction (fewer false positives)
- Consistent output format
- LLM —Ä–æ–∑—É–º—ñ—î domain context

---

#### ‚ö†Ô∏è Problem 5: No Cost Optimization
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:138-198`

**–ü—Ä–æ–±–ª–µ–º–∞:**
–ù–µ–º–∞—î –∂–æ–¥–Ω–æ—ó cost tracking –∞–±–æ optimization logic:

```python
# ‚ùå –ù–µ–º–∞—î:
# - Token counting
# - Cost estimation
# - Model selection based on complexity
# - Prompt caching
# - Budget alerts
```

**Impact:**
- –ù–µ–º–æ–∂–ª–∏–≤–æ –≤—ñ–¥–ø–æ–≤—ñ—Å—Ç–∏ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è "–°–∫—ñ–ª—å–∫–∏ –∫–æ—à—Ç—É—î 1000 messages?"
- –ù–µ–º–∞—î —Å–ø–æ–≤—ñ—â–µ–Ω—å –ø—Ä–æ –ø–µ—Ä–µ–≤–∏—Ç—Ä–∞—Ç–∏
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –æ–¥–∏–Ω model –¥–ª—è –≤—Å—ñ—Ö tasks (–Ω–µ–µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
class CostTracker:
    """Track LLM API costs for knowledge extraction."""

    PRICING = {
        "gpt-4o": {"input": 0.0025, "output": 0.01},  # per 1K tokens
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
        "claude-opus": {"input": 0.015, "output": 0.075},
        "claude-haiku": {"input": 0.00025, "output": 0.00125},
    }

    async def estimate_cost(
        self,
        messages: Sequence[Message],
        model_name: str,
    ) -> dict:
        """Estimate extraction cost before running."""
        input_tokens = self._count_tokens(messages)
        estimated_output_tokens = self._estimate_output(len(messages))

        pricing = self.PRICING.get(model_name, {"input": 0, "output": 0})

        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (estimated_output_tokens / 1000) * pricing["output"]
        total_cost = input_cost + output_cost

        return {
            "input_tokens": input_tokens,
            "output_tokens": estimated_output_tokens,
            "estimated_cost_usd": total_cost,
            "model": model_name,
        }

    async def log_actual_cost(
        self,
        run_id: str,
        result: Any,
        model_name: str,
    ) -> None:
        """Log actual cost after extraction."""
        # Extract token usage from result
        usage = result.usage if hasattr(result, "usage") else None

        if usage:
            actual_cost = self._calculate_cost(usage, model_name)

            # Store in database for analytics
            await self._store_cost_metrics({
                "run_id": run_id,
                "model": model_name,
                "input_tokens": usage.input_tokens,
                "output_tokens": usage.output_tokens,
                "cost_usd": actual_cost,
                "timestamp": datetime.now(UTC),
            })

            # Alert if over budget
            if actual_cost > self.config.max_cost_per_run:
                await self._send_cost_alert(run_id, actual_cost)

# Usage in KnowledgeExtractionService
async def extract_knowledge(self, messages: Sequence[Message]) -> KnowledgeExtractionOutput:
    cost_tracker = CostTracker()

    # Pre-flight cost estimation
    estimate = await cost_tracker.estimate_cost(messages, self.agent_config.model_name)
    logger.info(f"Estimated cost: ${estimate['estimated_cost_usd']:.4f}")

    # Run extraction
    result = await agent.run(prompt, model_settings=model_settings_obj)

    # Log actual cost
    await cost_tracker.log_actual_cost(run_id, result, self.agent_config.model_name)

    return result.output
```

**–î–æ–¥–∞—Ç–∫–æ–≤–æ:**
- Dashboard –¥–ª—è cost analytics
- Budget limits per month
- Automatic model downgrade —è–∫—â–æ budget exceeded

---

#### ‚ö†Ô∏è Problem 6: Topic Matching by Exact Name
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py:262-263`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
result = await session.execute(select(Topic).where(Topic.name == extracted_topic.name))
existing_topic = result.scalar_one_or_none()
# ‚ùå Exact match —Ç—ñ–ª—å–∫–∏! "Home Gym" ‚â† "Home Gym Setup"
```

**Impact:**
- Duplicate topics –∑ minor variations
- User –±–∞—á–∏—Ç—å "Apartment Renovation" —ñ "Apartment Renovations" —è–∫ –æ–∫—Ä–µ–º—ñ
- Fragmentation knowledge graph

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
async def find_similar_topic(
    self,
    session: AsyncSession,
    candidate_name: str,
    similarity_threshold: float = 0.85,
) -> Topic | None:
    """Find existing topic by semantic similarity, not exact match."""

    # Option 1: Use embedding similarity (if topics have embeddings)
    if self.use_embeddings:
        candidate_embedding = await self.embedding_service.generate_embedding(candidate_name)

        sql = text("""
            SELECT t.*, 1 - (t.embedding <=> :query_vector::vector) / 2 AS similarity
            FROM topics t
            WHERE t.embedding IS NOT NULL
              AND (1 - (t.embedding <=> :query_vector::vector) / 2) >= :threshold
            ORDER BY t.embedding <=> :query_vector::vector
            LIMIT 1
        """)

        result = await session.execute(sql, {
            "query_vector": str(candidate_embedding),
            "threshold": similarity_threshold,
        })
        row = result.fetchone()
        return Topic(**dict(row._mapping)) if row else None

    # Option 2: Fuzzy string matching (cheaper, faster)
    else:
        from fuzzywuzzy import fuzz

        stmt = select(Topic)
        result = await session.execute(stmt)
        topics = result.scalars().all()

        best_match = None
        best_score = 0

        for topic in topics:
            score = fuzz.ratio(candidate_name.lower(), topic.name.lower()) / 100
            if score > best_score and score >= similarity_threshold:
                best_match = topic
                best_score = score

        return best_match

# Usage in save_topics
for extracted_topic in extracted_topics:
    existing_topic = await self.find_similar_topic(session, extracted_topic.name)

    if existing_topic:
        logger.info(f"Found similar topic '{existing_topic.name}' for '{extracted_topic.name}' (similarity: {best_score:.2f})")
        # Create version or merge
```

**Metrics:**
- Topic duplication rate before/after
- User merge actions (measure if this helps)

---

### 1.3 –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

#### Priority 1: Production Readiness
1. **–î–æ–¥–∞—Ç–∏ fallback strategy** –¥–ª—è LLM timeouts (Problem 2)
2. **–¶–µ–Ω—Ç—Ä–∞–ª—ñ–∑—É–≤–∞—Ç–∏ thresholds** —É config –∑ –æ–±–≥—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è–º (Problem 1)
3. **Implement cost tracking** –¥–ª—è budget control (Problem 5)

#### Priority 2: Quality Improvements
4. **–ü–æ–∫—Ä–∞—â–∏—Ç–∏ prompt** –∑ examples —ñ constraints (Problem 4)
5. **Semantic topic matching** –∑–∞–º—ñ—Å—Ç—å exact name (Problem 6)
6. **Dynamic batch sizing** based on model context window (Problem 3)

#### Priority 3: Monitoring & Observability
7. –î–æ–¥–∞—Ç–∏ metrics:
   - Extraction success rate
   - Average confidence scores
   - Topic/atom creation rate
   - Cost per message
   - LLM latency (p50, p95, p99)
8. Dashboard –¥–ª—è real-time monitoring
9. Alerts –¥–ª—è anomalies (high cost, low confidence, timeouts)

---

## 2. Vector Embeddings System

### 2.1 –ü–æ—Ç–æ—á–Ω–∞ –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

#### ‚úÖ –©–æ –ü—Ä–∞—Ü—é—î –î–æ–±—Ä–µ

**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/embedding_service.py`

1. **Protocol-based architecture** (hexagonal pattern):
```python
class EmbeddingProvider(Protocol):
    async def generate_embedding(self, text: str) -> list[float]: ...
```

2. **Dimension validation**:
```python
async def _validate_embedding(self, embedding: list[float]) -> list[float]:
    actual_dims = len(embedding)
    expected_dims = settings.embedding.openai_embedding_dimensions  # 1536

    if actual_dims != expected_dims:
        raise ValueError(f"Embedding dimension mismatch: expected {expected_dims}, got {actual_dims}")
```

3. **Batch processing –∑ error handling**:
```python
async def embed_messages_batch(
    self, session: AsyncSession, message_ids: list[int], batch_size: int | None = None
) -> dict[str, int]:
    stats = {"success": 0, "failed": 0, "skipped": 0}

    for i in range(0, len(message_ids), batch_size):
        chunk_ids = message_ids[i : i + batch_size]
        # Process chunk + commit per batch (–Ω–µ –≤—Ç—Ä–∞—á–∞—î–º–æ –≤—Å–µ –ø—Ä–∏ fail)
```

4. **Empty text validation**:
```python
if not text or not text.strip():
    raise ValueError("Cannot generate embedding for empty text")
```

---

### 2.2 –ü—Ä–æ–±–ª–µ–º–∏

#### üî¥ Problem 7: No Embedding Strategy
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: HIGH
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/embedding_service.py`

**–ü—Ä–æ–±–ª–µ–º–∞:**
–ù–µ–º–∞—î —á—ñ—Ç–∫–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó **–∫–æ–ª–∏** —ñ **—è–∫** –≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ embeddings:

```python
# ‚ùå Embeddings –≥–µ–Ω–µ—Ä—É—é—Ç—å—Å—è –ü–Ü–°–õ–Ø knowledge extraction
# tasks.py:1107-1118
if version_created_atom_ids:
    await embed_atoms_batch_task.kiq(atom_ids=atom_ids_for_embedding, provider_id=str(provider.id))

if message_ids:
    await embed_messages_batch_task.kiq(message_ids=message_ids_int, provider_id=str(provider.id))
```

**Impact:**
- **Semantic search –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π** –ø—ñ–¥ —á–∞—Å extraction ‚Üí –Ω–µ –º–æ–∂–Ω–∞ use RAG
- **User experience**: —Å—Ç–≤–æ—Ä—é—î—Ç—å—Å—è atom ‚Üí user —à—É–∫–∞—î ‚Üí not found (—â–µ –Ω–µ–º–∞—î embedding)
- **Cost inefficiency**: –ø–æ–¥–≤—ñ–π–Ω–∏–π API call (1. extraction, 2. embedding)

**–ü–∏—Ç–∞–Ω–Ω—è –±–µ–∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:**
1. –ß–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ embed –ü–ï–†–ï–î extraction –¥–ª—è RAG context?
2. –ß–∏ –º–æ–∂–Ω–∞ batch embed —ñ—Å–Ω—É—é—á—ñ messages –¥–ª—è historical data?
3. –©–æ —Ä–æ–±–∏—Ç–∏ –∑ messages, —è–∫—ñ failed embedding?
4. –Ø–∫ —á–∞—Å—Ç–æ re-embed –ø—Ä–∏ –∑–º—ñ–Ω—ñ embedding model?

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
class EmbeddingStrategy:
    """Define WHEN and HOW to generate embeddings."""

    async def embed_on_ingestion(self, message: Message) -> Message:
        """Strategy 1: Embed immediately after message save.

        Pros: Instant search availability, RAG-ready
        Cons: Higher latency, API calls during user interaction
        """
        embedding = await self.embedding_service.generate_embedding(message.content)
        message.embedding = embedding
        return message

    async def embed_batch_async(self, message_ids: list[int]) -> dict:
        """Strategy 2: Batch embed in background (current approach).

        Pros: No user-facing latency, cost-efficient batching
        Cons: Delay before search availability
        """
        await embed_messages_batch_task.kiq(message_ids, provider_id)

    async def embed_on_demand(self, message: Message) -> Message:
        """Strategy 3: Lazy embedding on first search.

        Pros: Zero upfront cost, only embed what's searched
        Cons: First search slow, inconsistent experience
        """
        if message.embedding is None:
            message.embedding = await self.embedding_service.generate_embedding(message.content)
            await session.commit()
        return message

    async def embed_hybrid(self, message: Message, priority: str = "normal") -> Message:
        """Strategy 4: Hybrid approach (RECOMMENDED).

        - High priority (user-facing): embed immediately
        - Normal priority (batch ingestion): queue for background
        - Low priority (historical): on-demand
        """
        if priority == "high":
            return await self.embed_on_ingestion(message)
        elif priority == "normal":
            await self.embed_batch_async([message.id])
        else:  # low
            # Skip for now, embed on-demand later
            pass
        return message

# Usage in save_telegram_message
async def save_telegram_message(telegram_data: dict[str, Any]) -> str:
    db_message = Message(...)
    db.add(db_message)
    await db.commit()

    # Decide embedding strategy
    strategy = EmbeddingStrategy(embedding_service)

    if telegram_data.get("priority") == "high":
        # User explicitly said "this is important" ‚Üí embed now
        await strategy.embed_on_ingestion(db_message)
    else:
        # Background ingestion ‚Üí queue for batch
        await strategy.embed_batch_async([db_message.id])
```

**Decision Tree:**
```
Message arrives ‚Üí Check priority
                  ‚îú‚îÄ High (user-triggered action) ‚Üí Embed immediately
                  ‚îú‚îÄ Normal (webhook, batch import) ‚Üí Queue for background batch
                  ‚îî‚îÄ Low (historical backfill) ‚Üí Skip, embed on-demand
```

---

#### ‚ö†Ô∏è Problem 8: Batch Size –ù–µ –û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/tasks.py:768`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
stats = await service.embed_messages_batch(db, message_ids, batch_size=100)
# ‚ùå –ß–æ–º—É 100? OpenAI Embeddings API –ø—Ä–∏–π–º–∞—î –¥–æ 2048 inputs!
```

**Impact:**
- –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ API calls
- Higher latency (100 sequential requests –∑–∞–º—ñ—Å—Ç—å 1)
- Missed cost savings (OpenAI –µ–º–±–µ–¥–¥—ñ–Ω–≥–∏ –¥–µ—à–µ–≤—à—ñ –ø—Ä–∏ batching)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# config.py
class EmbeddingSettings(BaseSettings):
    embedding_batch_size: int = Field(
        default=100,
        description="Batch size for embedding generation"
    )

    # API-specific limits
    openai_max_batch_size: int = Field(
        default=2048,
        description="OpenAI Embeddings API accepts up to 2048 inputs per request"
    )

    ollama_max_batch_size: int = Field(
        default=1,
        description="Ollama processes embeddings one-by-one (no batch API)"
    )

# embedding_service.py
async def embed_messages_batch(
    self, session: AsyncSession, message_ids: list[int], batch_size: int | None = None
) -> dict[str, int]:
    # Adjust batch size based on provider
    if batch_size is None:
        if self.provider.type == ProviderType.openai:
            batch_size = settings.embedding.openai_max_batch_size  # 2048
        else:
            batch_size = settings.embedding.ollama_max_batch_size  # 1

    # Use OpenAI batch API
    if self.provider.type == ProviderType.openai:
        return await self._embed_openai_batch(session, message_ids, batch_size)
    else:
        return await self._embed_sequential(session, message_ids)

async def _embed_openai_batch(
    self, session: AsyncSession, message_ids: list[int], batch_size: int
) -> dict[str, int]:
    """Use OpenAI's batch API for efficient embedding."""
    client = AsyncOpenAI(api_key=api_key)

    # Fetch messages
    stmt = select(Message).where(Message.id.in_(message_ids))
    result = await session.execute(stmt)
    messages = list(result.scalars().all())

    # Prepare batch input
    texts = [msg.content for msg in messages]

    # Single API call for up to 2048 messages!
    response = await client.embeddings.create(
        model=settings.embedding.openai_embedding_model,
        input=texts,  # ‚úÖ Batch input
        encoding_format="float"
    )

    # Map embeddings back to messages
    for idx, msg in enumerate(messages):
        msg.embedding = response.data[idx].embedding
        session.add(msg)

    await session.commit()
    return {"success": len(messages), "failed": 0, "skipped": 0}
```

**Savings:**
- 100 API calls ‚Üí 1 API call
- Latency: ~30s ‚Üí ~3s (10x faster)
- Cost: same (OpenAI charges per token, –Ω–µ per request)

---

#### ‚ö†Ô∏è Problem 9: No Embedding Re-Generation Strategy
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM

**–ü—Ä–æ–±–ª–µ–º–∞:**
–©–æ —Ä–æ–±–∏—Ç–∏ –∫–æ–ª–∏:
1. –ó–º—ñ–Ω—é—î—Ç—å—Å—è embedding model (`text-embedding-3-small` ‚Üí `text-embedding-3-large`)?
2. OpenAI –≤–∏–ø—É—Å–∫–∞—î –Ω–æ–≤—É –≤–µ—Ä—Å—ñ—é –º–æ–¥–µ–ª—ñ?
3. –í–∏—è–≤–ª–µ–Ω–æ, —â–æ –¥–µ—è–∫—ñ embeddings corrupted?

**–ù–µ–º–∞—î:**
- Versioning –¥–ª—è embeddings
- Re-generation workflow
- Backward compatibility strategy

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# Add to Message/Atom models
class Message(SQLModel, table=True):
    embedding: list[float] | None = Field(default=None, sa_column=Column(Vector(1536)))
    embedding_model: str | None = Field(default=None, description="Model used: text-embedding-3-small")
    embedding_version: str | None = Field(default=None, description="Model version: 2024-10-01")
    embedding_generated_at: datetime | None = Field(default=None)

# Re-generation workflow
async def regenerate_embeddings(
    self,
    session: AsyncSession,
    model_name: str,
    filters: dict | None = None,
) -> dict:
    """Regenerate embeddings for all or filtered messages."""

    # Find messages with old embeddings
    stmt = select(Message).where(
        (Message.embedding_model != model_name) |
        (Message.embedding_model.is_(None))
    )

    if filters:
        # E.g., only messages after certain date
        if "created_after" in filters:
            stmt = stmt.where(Message.created_at >= filters["created_after"])

    result = await session.execute(stmt)
    messages = list(result.scalars().all())

    logger.info(f"Regenerating {len(messages)} embeddings with model {model_name}")

    # Batch re-embed
    message_ids = [msg.id for msg in messages]
    stats = await self.embed_messages_batch(session, message_ids)

    # Update metadata
    for msg in messages:
        msg.embedding_model = model_name
        msg.embedding_version = "2024-10-01"  # From API response
        msg.embedding_generated_at = datetime.now(UTC)

    await session.commit()
    return stats

# API endpoint for manual trigger
@router.post("/embeddings/regenerate")
async def regenerate_embeddings_endpoint(
    model_name: str = "text-embedding-3-small",
    filters: dict | None = None,
    db: AsyncSession = Depends(get_db),
):
    """Trigger embedding regeneration (admin only)."""
    service = EmbeddingService(provider)
    stats = await service.regenerate_embeddings(db, model_name, filters)
    return {"status": "completed", "stats": stats}
```

---

### 2.3 Cost Analysis

**Current Costs** (estimated):
- Model: `text-embedding-3-small` ($0.02 per 1M tokens)
- Avg message length: 50 words ‚âà 65 tokens
- 1000 messages/day:
  - Tokens: 1000 * 65 = 65,000 tokens
  - Cost: $0.0013/day = **$0.47/year** ‚úÖ Very cheap!

**Cost Optimization Opportunities:**
1. **Batch API usage** (Problem 8): no cost savings, –∞–ª–µ 10x faster
2. **On-demand embedding** (Problem 7): save 30-50% (only embed searched messages)
3. **Compression**: use `dimensions=512` instead of 1536 ‚Üí 3x cheaper (but lower quality)

**Cost Tracking:**
```python
# Add to database
class EmbeddingCostLog(SQLModel, table=True):
    id: int
    provider_id: UUID
    model_name: str
    input_tokens: int
    cost_usd: float
    batch_size: int
    created_at: datetime

# Track in embed_messages_batch
await session.add(EmbeddingCostLog(
    provider_id=provider.id,
    model_name="text-embedding-3-small",
    input_tokens=total_tokens,
    cost_usd=(total_tokens / 1_000_000) * 0.02,
    batch_size=len(message_ids),
    created_at=datetime.now(UTC),
))
```

---

### 2.4 –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

#### Priority 1: Embedding Strategy
1. **Implement hybrid embedding strategy** (Problem 7):
   - High priority: embed immediately
   - Normal: batch background
   - Low: on-demand
2. **Document decision criteria** (–∫–æ–ª–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫–∏–π strategy)

#### Priority 2: Performance Optimization
3. **Use OpenAI batch API** (Problem 8): 2048 inputs per call
4. **Add embedding metadata** (model, version, timestamp)
5. **Build re-generation workflow** (Problem 9)

#### Priority 3: Cost Management
6. **Track embedding costs** per model/provider
7. **Dashboard** –¥–ª—è embedding analytics:
   - Total embeddings generated
   - Cost per day/week/month
   - Coverage rate (% messages with embeddings)
   - Failed embedding rate

---

## 3. Vector Search & Semantic Similarity

### 3.1 –ü–æ—Ç–æ—á–Ω–∞ –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/semantic_search_service.py`

#### ‚úÖ –©–æ –ü—Ä–∞—Ü—é—î –î–æ–±—Ä–µ

1. **–ü—Ä–∞–≤–∏–ª—å–Ω–∞ cosine similarity formula**:
```python
# Cosine distance ranges: 0 (identical) to 2 (opposite)
# Convert to similarity: 1 - (distance / 2) ‚Üí range 0.0-1.0 ‚úÖ
similarity = 1 - (m.embedding <=> :query_vector::vector) / 2
```

2. **Threshold filtering**:
```python
WHERE (1 - (m.embedding <=> :query_vector::vector) / 2) >= :threshold
```

3. **Duplicate detection** –∑ high threshold:
```python
async def find_duplicates(self, session: AsyncSession, message_id: int, threshold: float = 0.95):
    return await self.find_similar_messages(session, message_id, limit=5, threshold=threshold)
```

---

### 3.2 –ü—Ä–æ–±–ª–µ–º–∏

#### ‚ö†Ô∏è Problem 10: Hardcoded Similarity Threshold
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**:
- `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/semantic_search_service.py:44`
- `file:///Users/maks/PycharmProjects/task-tracker/backend/core/config.py:90-95`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# semantic_search_service.py
async def search_messages(self, session: AsyncSession, query: str, limit: int = 10, threshold: float = 0.7):
    # ‚ùå Default 0.7 - —á–æ–º—É?

# config.py
vector_similarity_threshold: float = Field(default=0.7, ge=0.0, le=1.0)
# ‚ùå –ù–µ–º–∞—î –æ–±–≥—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è
```

**Impact:**
- 0.7 –º–æ–∂–µ –±—É—Ç–∏ too strict (missed results)
- –ê–±–æ too loose (irrelevant results)
- –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ use case:
  - Duplicate detection: 0.95+ (high precision)
  - Semantic search: 0.6-0.7 (balance)
  - Broad exploration: 0.5+ (high recall)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# config.py - use case specific thresholds
class VectorSearchSettings(BaseSettings):
    """Vector search thresholds optimized for different use cases."""

    duplicate_detection_threshold: float = Field(
        default=0.95,
        ge=0.9,
        le=1.0,
        description="High precision for duplicate detection (0.95 = 95% similar)"
    )

    semantic_search_threshold: float = Field(
        default=0.65,
        ge=0.5,
        le=0.85,
        description="Balanced threshold for semantic search (tested with 1000 queries)"
    )

    exploration_threshold: float = Field(
        default=0.50,
        ge=0.3,
        le=0.7,
        description="Low threshold for exploratory search (find loosely related content)"
    )

    rag_context_threshold: float = Field(
        default=0.70,
        ge=0.6,
        le=0.85,
        description="RAG context retrieval (avoid irrelevant context pollution)"
    )

# semantic_search_service.py
class SearchMode(str, Enum):
    DUPLICATE = "duplicate"
    SEARCH = "search"
    EXPLORE = "explore"
    RAG = "rag"

async def search_messages(
    self,
    session: AsyncSession,
    query: str,
    mode: SearchMode = SearchMode.SEARCH,
    limit: int = 10,
    custom_threshold: float | None = None,
):
    """Search with mode-specific thresholds."""

    threshold_map = {
        SearchMode.DUPLICATE: settings.vector_search.duplicate_detection_threshold,
        SearchMode.SEARCH: settings.vector_search.semantic_search_threshold,
        SearchMode.EXPLORE: settings.vector_search.exploration_threshold,
        SearchMode.RAG: settings.vector_search.rag_context_threshold,
    }

    threshold = custom_threshold or threshold_map[mode]

    logger.info(f"Searching with mode={mode}, threshold={threshold}")

    # Execute search...
```

**Experimentation Framework:**
```python
async def evaluate_threshold(
    self,
    test_queries: list[tuple[str, list[int]]],  # (query, expected_message_ids)
    threshold: float,
) -> dict:
    """Evaluate search quality at different thresholds.

    Args:
        test_queries: List of (query, expected_relevant_message_ids)
        threshold: Threshold to test

    Returns:
        Precision, recall, F1 scores
    """
    total_precision = 0
    total_recall = 0

    for query, expected_ids in test_queries:
        results = await self.search_messages(session, query, threshold=threshold, limit=20)
        result_ids = {msg.id for msg, _ in results}
        expected_set = set(expected_ids)

        true_positives = len(result_ids & expected_set)
        false_positives = len(result_ids - expected_set)
        false_negatives = len(expected_set - result_ids)

        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0

        total_precision += precision
        total_recall += recall

    avg_precision = total_precision / len(test_queries)
    avg_recall = total_recall / len(test_queries)
    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0

    return {
        "threshold": threshold,
        "precision": avg_precision,
        "recall": avg_recall,
        "f1_score": f1,
    }

# CLI tool –¥–ª—è experiments
async def find_optimal_threshold():
    thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]
    results = []

    for t in thresholds:
        metrics = await evaluator.evaluate_threshold(test_queries, threshold=t)
        results.append(metrics)
        print(f"Threshold {t:.2f}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}")

    best = max(results, key=lambda x: x['f1_score'])
    print(f"\nOptimal threshold: {best['threshold']} (F1={best['f1_score']:.3f})")
```

---

#### ‚ö†Ô∏è Problem 11: No HNSW Index Monitoring
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: Vector indexes —Å—Ç–≤–æ—Ä–µ–Ω–æ, –∞–ª–µ –Ω–µ–º–∞—î monitoring

**–ü—Ä–æ–±–ª–µ–º–∞:**
```sql
-- Indexes —ñ—Å–Ω—É—é—Ç—å (alembic migrations)
CREATE INDEX idx_messages_embedding_hnsw ON messages USING hnsw (embedding vector_cosine_ops);
CREATE INDEX idx_atoms_embedding_hnsw ON atoms USING hnsw (embedding vector_cosine_ops);

-- ‚ùå –ê–ª–µ –Ω–µ–º–∞—î:
-- - Index usage statistics
-- - Query performance monitoring
-- - Index size tracking
-- - Rebuild recommendations
```

**Impact:**
- –ù–µ –∑–Ω–∞—î–º–æ —á–∏ index –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è
- –ù–µ–º–∞—î alerting —è–∫—â–æ search slow
- –ù–µ –±–∞—á–∏–º–æ –∫–æ–ª–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ rebuild

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# Add monitoring service
class VectorIndexMonitor:
    """Monitor pgvector index health and performance."""

    async def get_index_stats(self, session: AsyncSession) -> dict:
        """Get index size and usage statistics."""

        sql = text("""
            SELECT
                schemaname,
                tablename,
                indexname,
                pg_size_pretty(pg_relation_size(indexrelid)) AS index_size,
                idx_scan AS scans,
                idx_tup_read AS tuples_read,
                idx_tup_fetch AS tuples_fetched
            FROM pg_stat_user_indexes
            WHERE indexname LIKE '%embedding_hnsw%'
            ORDER BY pg_relation_size(indexrelid) DESC
        """)

        result = await session.execute(sql)
        rows = result.fetchall()

        stats = []
        for row in rows:
            stats.append({
                "schema": row.schemaname,
                "table": row.tablename,
                "index": row.indexname,
                "size": row.index_size,
                "scans": row.scans,
                "tuples_read": row.tuples_read,
                "tuples_fetched": row.tuples_fetched,
            })

        return {"indexes": stats, "timestamp": datetime.now(UTC).isoformat()}

    async def check_index_health(self, session: AsyncSession) -> dict:
        """Check if indexes need maintenance."""

        # Check if queries are using index
        sql = text("""
            EXPLAIN (FORMAT JSON)
            SELECT * FROM messages
            WHERE embedding IS NOT NULL
            ORDER BY embedding <=> '[0.1,0.2,...]'::vector
            LIMIT 10
        """)

        result = await session.execute(sql)
        plan = result.fetchone()[0]

        uses_index = "Index Scan" in str(plan) and "hnsw" in str(plan)

        return {
            "uses_index": uses_index,
            "query_plan": plan,
            "recommendation": "Index OK" if uses_index else "Index not used! Check query or rebuild index",
        }

    async def get_search_latency_stats(self, session: AsyncSession) -> dict:
        """Measure average search latency."""

        import time

        test_query = [0.1] * 1536  # Dummy embedding

        start = time.perf_counter()
        sql = text("""
            SELECT * FROM messages
            WHERE embedding IS NOT NULL
            ORDER BY embedding <=> :query_vector::vector
            LIMIT 10
        """)
        await session.execute(sql, {"query_vector": str(test_query)})
        latency_ms = (time.perf_counter() - start) * 1000

        return {
            "latency_ms": latency_ms,
            "status": "OK" if latency_ms < 100 else "SLOW",
            "recommendation": "" if latency_ms < 100 else "Consider index rebuild or tuning",
        }

# API endpoint
@router.get("/vector/health")
async def vector_health_check(db: AsyncSession = Depends(get_db)):
    """Vector search health check (admin only)."""
    monitor = VectorIndexMonitor()

    stats = await monitor.get_index_stats(db)
    health = await monitor.check_index_health(db)
    latency = await monitor.get_search_latency_stats(db)

    return {
        "index_stats": stats,
        "health": health,
        "latency": latency,
    }

# Alert rules
async def check_vector_alerts():
    latency = await monitor.get_search_latency_stats(db)

    if latency["latency_ms"] > 200:
        await send_alert("Vector search latency high", severity="warning")

    stats = await monitor.get_index_stats(db)
    for idx in stats["indexes"]:
        if idx["scans"] == 0:
            await send_alert(f"Index {idx['index']} never used", severity="info")
```

**Dashboard Metrics:**
- Index size (MB)
- Query latency (p50, p95, p99)
- Index scan rate
- Index hit rate vs sequential scan rate

---

#### ‚ö†Ô∏è Problem 12: No Filtering Support
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/semantic_search_service.py:76-86`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
async def search_messages(self, session: AsyncSession, query: str, limit: int = 10, threshold: float = 0.7):
    sql = text("""
        SELECT m.*, 1 - (m.embedding <=> :query_vector::vector) / 2 AS similarity
        FROM messages m
        WHERE m.embedding IS NOT NULL
          AND (1 - (m.embedding <=> :query_vector::vector) / 2) >= :threshold
        ORDER BY m.embedding <=> :query_vector::vector
        LIMIT :limit
    """)
    # ‚ùå –ù–µ–º–∞—î filters: author, date range, topic, importance score
```

**Impact:**
- User –Ω–µ –º–æ–∂–µ —à—É–∫–∞—Ç–∏ "messages from last week about work"
- –ù–µ–º–∞—î —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –ø–æ author, topic, date
- –í—Å–µ –ø–µ—Ä–µ–º—ñ—à–∞–Ω–æ - noise + signal

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
from dataclasses import dataclass
from datetime import datetime

@dataclass
class SearchFilters:
    """Filters for semantic search."""
    author_ids: list[int] | None = None
    topic_ids: list[int] | None = None
    date_from: datetime | None = None
    date_to: datetime | None = None
    min_importance_score: float | None = None
    exclude_noise: bool = False

async def search_messages(
    self,
    session: AsyncSession,
    query: str,
    filters: SearchFilters | None = None,
    limit: int = 10,
    threshold: float = 0.7,
) -> list[tuple[Message, float]]:
    """Search with optional filters."""

    # Build WHERE clause dynamically
    where_clauses = ["m.embedding IS NOT NULL"]
    where_clauses.append("(1 - (m.embedding <=> :query_vector::vector) / 2) >= :threshold")

    params = {
        "query_vector": query_vector,
        "threshold": threshold,
        "limit": limit,
    }

    if filters:
        if filters.author_ids:
            where_clauses.append("m.author_id = ANY(:author_ids)")
            params["author_ids"] = filters.author_ids

        if filters.topic_ids:
            where_clauses.append("m.topic_id = ANY(:topic_ids)")
            params["topic_ids"] = filters.topic_ids

        if filters.date_from:
            where_clauses.append("m.sent_at >= :date_from")
            params["date_from"] = filters.date_from

        if filters.date_to:
            where_clauses.append("m.sent_at <= :date_to")
            params["date_to"] = filters.date_to

        if filters.min_importance_score is not None:
            where_clauses.append("m.importance_score >= :min_score")
            params["min_score"] = filters.min_importance_score

        if filters.exclude_noise:
            where_clauses.append("m.noise_classification != 'noise'")

    where_clause = " AND ".join(where_clauses)

    sql = text(f"""
        SELECT
            m.*,
            1 - (m.embedding <=> :query_vector::vector) / 2 AS similarity
        FROM messages m
        WHERE {where_clause}
        ORDER BY m.embedding <=> :query_vector::vector
        LIMIT :limit
    """)

    result = await session.execute(sql, params)
    rows = result.fetchall()

    messages_with_scores = []
    for row in rows:
        message_dict = dict(row._mapping)
        similarity = message_dict.pop("similarity")
        message = Message(**message_dict)
        messages_with_scores.append((message, float(similarity)))

    logger.info(
        f"Found {len(messages_with_scores)} messages for query '{query[:50]}...' "
        f"with filters: {filters}"
    )

    return messages_with_scores

# API usage
@router.get("/search")
async def search_endpoint(
    query: str,
    author_ids: list[int] | None = None,
    topic_ids: list[int] | None = None,
    date_from: datetime | None = None,
    exclude_noise: bool = False,
    db: AsyncSession = Depends(get_db),
):
    """Search messages with filters."""
    filters = SearchFilters(
        author_ids=author_ids,
        topic_ids=topic_ids,
        date_from=date_from,
        exclude_noise=exclude_noise,
    )

    service = SemanticSearchService(embedding_service)
    results = await service.search_messages(db, query, filters=filters)

    return {"results": [{"message": msg, "similarity": score} for msg, score in results]}
```

**Use Cases:**
- "Show me messages about work from last month" ‚Üí filters: topic_ids=[work_topic_id], date_from=30 days ago
- "Find important messages from Alice" ‚Üí filters: author_ids=[alice_id], min_importance_score=0.7
- "Search excluding noise" ‚Üí filters: exclude_noise=True

---

### 3.3 –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

#### Priority 1: Threshold Management
1. **Implement use case-specific thresholds** (Problem 10)
2. **Build experimentation framework** –¥–ª—è A/B testing
3. **Document threshold rationale**

#### Priority 2: Monitoring
4. **Add vector index monitoring** (Problem 11):
   - Index usage stats
   - Query latency tracking
   - Health checks
5. **Dashboard** –¥–ª—è vector search analytics

#### Priority 3: Feature Completeness
6. **Add search filters** (Problem 12): author, date, topic, importance
7. **Build advanced search UI** –∑ filter options

---

## 4. Topics/Atoms Versioning System

### 4.1 –ü–æ—Ç–æ—á–Ω–∞ –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è

**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/versioning_service.py`

#### ‚úÖ –©–æ –ü—Ä–∞—Ü—é—î –î–æ–±—Ä–µ

1. **Version Snapshot Pattern**:
```python
async def create_topic_version(self, db: AsyncSession, topic_id: int, data: dict[str, Any], created_by: str | None = None):
    latest_version = await self._get_latest_topic_version(db, topic_id)
    next_version = (latest_version.version + 1) if latest_version else 1

    version = TopicVersion(
        topic_id=topic_id,
        version=next_version,
        data=data,  # ‚úÖ Snapshot –≤—Å—å–æ–≥–æ —Å—Ç–∞–Ω—É
        created_by=created_by,
        approved=False,  # ‚úÖ Draft by default
    )
```

2. **DeepDiff Integration**:
```python
def _format_diff(self, diff: DeepDiff) -> list[dict[str, Any]]:
    """Format deepdiff output for API response."""
    changes = []

    if hasattr(diff, "tree") and diff.tree:
        for change_type, items in diff.tree.items():
            for item in items:
                change = {
                    "type": change_type,
                    "path": str(item.path(output_format="list")),
                    "old_value": getattr(item, "t1", None),
                    "new_value": getattr(item, "t2", None),
                }
                changes.append(change)

    return changes
```

3. **Bulk Operations**:
```python
async def bulk_approve_versions(
    self, db: AsyncSession, entity_type: EntityType, version_ids: list[int]
) -> tuple[int, list[int], dict[int, str]]:
    """Approve multiple versions in transaction-safe manner."""
    # ‚úÖ Transaction safety
    # ‚úÖ Error tracking per version
    # ‚úÖ Rollback on failure
```

4. **WebSocket Broadcasting**:
```python
await websocket_manager.broadcast(
    "versions",
    {
        "event": "pending_count_updated",
        "count": pending_count,
        "last_updated": datetime.now(UTC).isoformat(),
    },
)
# ‚úÖ Real-time updates –¥–ª—è UI
```

---

### 4.2 –ü—Ä–æ–±–ª–µ–º–∏

#### ‚ö†Ô∏è Problem 13: Manual Approval Process
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/versioning_service.py:168-225`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
async def approve_version(self, db: AsyncSession, entity_type: EntityType, entity_id: int, version_number: int):
    # ‚ùå Manual approval ONLY! –ù–µ–º–∞—î auto-approval rules.

    version.approved = True
    version.approved_at = datetime.now(UTC)
    await db.commit()
```

**Impact:**
- User –º–∞—î manually approve –∫–æ–∂–Ω—É version
- High confidence extractions (0.9+) –≤—Å–µ –æ–¥–Ω–æ —á–µ–∫–∞—é—Ç—å approval
- Bottleneck –¥–ª—è scaling

**Context:**
```python
# tasks.py:1296-1318 - Placeholder –¥–ª—è auto-approval
@nats_broker.task
async def scheduled_auto_approval_task() -> dict[str, Any]:
    logger.info("Auto-approval system pending - awaiting versioning confidence scores")
    return {
        "status": "pending",
        "reason": "awaiting_confidence_scores",
        "message": "TopicVersion/AtomVersion models need confidence/similarity fields",
    }
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# 1. Add confidence to versions
class TopicVersion(SQLModel, table=True):
    topic_id: int
    version: int
    data: dict
    created_by: str | None
    approved: bool = False

    # ‚úÖ Add these fields
    confidence: float | None = Field(default=None, ge=0.0, le=1.0)
    similarity_to_previous: float | None = Field(default=None, ge=0.0, le=1.0)
    auto_approved: bool = Field(default=False)
    approval_rule_id: int | None = Field(default=None)

# 2. Build approval rules engine
class ApprovalRule(SQLModel, table=True):
    """Auto-approval rules for versions."""

    id: int
    name: str
    entity_type: str  # topic, atom

    # Rule conditions
    min_confidence: float = Field(ge=0.0, le=1.0)
    max_similarity_threshold: float = Field(ge=0.0, le=1.0, description="Max change from previous version")
    trusted_creators: list[str] | None = Field(default=None, sa_type=JSON)

    is_active: bool = Field(default=True)
    priority: int = Field(default=0)

# 3. Auto-approval service
class AutoApprovalService:
    """Evaluate versions against approval rules."""

    async def evaluate_version(
        self,
        db: AsyncSession,
        version: TopicVersion | AtomVersion,
    ) -> tuple[bool, str, int | None]:
        """Check if version should be auto-approved.

        Returns:
            (should_approve, reason, rule_id)
        """

        # Load active rules sorted by priority
        stmt = (
            select(ApprovalRule)
            .where(
                ApprovalRule.entity_type == ("topic" if isinstance(version, TopicVersion) else "atom"),
                ApprovalRule.is_active == True,
            )
            .order_by(ApprovalRule.priority.desc())
        )
        result = await db.execute(stmt)
        rules = list(result.scalars().all())

        for rule in rules:
            # Check confidence
            if version.confidence is not None and version.confidence >= rule.min_confidence:

                # Check similarity (small change = safe)
                if version.similarity_to_previous is not None:
                    if version.similarity_to_previous <= rule.max_similarity_threshold:

                        # Check trusted creator
                        if rule.trusted_creators:
                            if version.created_by not in rule.trusted_creators:
                                continue

                        # All conditions met!
                        return (
                            True,
                            f"Auto-approved by rule '{rule.name}' (confidence={version.confidence:.2f}, similarity={version.similarity_to_previous:.2f})",
                            rule.id,
                        )

        return (False, "No matching approval rule", None)

    async def auto_approve_pending_versions(
        self,
        db: AsyncSession,
        entity_type: str,
        limit: int = 100,
    ) -> dict:
        """Batch auto-approve pending versions."""

        # Find unapproved versions
        if entity_type == "topic":
            stmt = select(TopicVersion).where(TopicVersion.approved == False).limit(limit)
        else:
            stmt = select(AtomVersion).where(AtomVersion.approved == False).limit(limit)

        result = await db.execute(stmt)
        versions = list(result.scalars().all())

        approved_count = 0
        rejected_count = 0

        for version in versions:
            should_approve, reason, rule_id = await self.evaluate_version(db, version)

            if should_approve:
                # Apply version to main entity
                if entity_type == "topic":
                    entity = await db.get(Topic, version.topic_id)
                else:
                    entity = await db.get(Atom, version.atom_id)

                if entity:
                    for key, value in version.data.items():
                        if hasattr(entity, key):
                            setattr(entity, key, value)

                version.approved = True
                version.approved_at = datetime.now(UTC)
                version.auto_approved = True
                version.approval_rule_id = rule_id

                approved_count += 1
                logger.info(f"Auto-approved {entity_type} version {version.id}: {reason}")
            else:
                rejected_count += 1

        await db.commit()

        return {
            "approved": approved_count,
            "rejected": rejected_count,
            "total": len(versions),
        }

# 4. Scheduled task
@nats_broker.task
async def scheduled_auto_approval_task() -> dict[str, Any]:
    """Auto-approve versions based on rules (run hourly)."""

    async with AsyncSessionLocal() as db:
        service = AutoApprovalService()

        topic_results = await service.auto_approve_pending_versions(db, "topic", limit=100)
        atom_results = await service.auto_approve_pending_versions(db, "atom", limit=100)

        return {
            "status": "success",
            "topics": topic_results,
            "atoms": atom_results,
        }

# 5. Default approval rules (seed data)
default_rules = [
    ApprovalRule(
        name="High Confidence Auto-Approve",
        entity_type="topic",
        min_confidence=0.85,
        max_similarity_threshold=0.3,  # Small change
        trusted_creators=["knowledge_extraction", "scheduled_task"],
        priority=10,
    ),
    ApprovalRule(
        name="Medium Confidence Manual Review",
        entity_type="topic",
        min_confidence=0.70,
        max_similarity_threshold=0.5,
        trusted_creators=None,  # Any creator
        priority=5,
    ),
]
```

**UI for Rule Management:**
```typescript
// Admin panel: /dashboard/approval-rules
interface ApprovalRule {
  id: number
  name: string
  entity_type: "topic" | "atom"
  min_confidence: number
  max_similarity_threshold: number
  is_active: boolean
}

// User can:
// - Create new rules
// - Enable/disable rules
// - Set priority order
// - View auto-approval history
```

---

#### ‚ö†Ô∏è Problem 14: Conflict Resolution = Last-Write-Wins
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: MEDIUM
**–õ–æ–∫–∞—Ü—ñ—è**: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/versioning_service.py:211-216`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
async def approve_version(self, ...):
    entity = await db.get(entity_model, entity_id)
    if entity:
        for key, value in version.data.items():
            if hasattr(entity, key):
                setattr(entity, key, value)  # ‚ùå Last write wins! –ù–µ–º–∞—î merge logic.
```

**–°—Ü–µ–Ω–∞—Ä—ñ–π –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É:**
1. LLM extraction creates Topic "Home Gym" ‚Üí version 1 (pending)
2. User manually edits Topic "Home Gym" ‚Üí version 2 (approved)
3. Admin approves version 1 ‚Üí **overwrites user edits!**

**Impact:**
- User –≤—Ç—Ä–∞—á–∞—î —Å–≤–æ—ó –∑–º—ñ–Ω–∏
- –ù–µ–º–∞—î conflict detection
- –ù–µ–º–∞—î merge strategy

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
class ConflictResolutionStrategy(str, Enum):
    LAST_WRITE_WINS = "last_write_wins"  # Current behavior
    MANUAL_MERGE = "manual_merge"  # Ask user to resolve
    AUTO_MERGE_FIELDS = "auto_merge_fields"  # Merge non-conflicting fields
    REJECT_OUTDATED = "reject_outdated"  # Reject if entity changed

async def approve_version(
    self,
    db: AsyncSession,
    entity_type: EntityType,
    entity_id: int,
    version_number: int,
    strategy: ConflictResolutionStrategy = ConflictResolutionStrategy.AUTO_MERGE_FIELDS,
):
    """Approve version with conflict resolution."""

    version = await self._get_version(db, entity_type, entity_id, version_number)
    entity = await self._get_entity(db, entity_type, entity_id)

    # Detect conflicts
    conflicts = await self._detect_conflicts(entity, version)

    if conflicts:
        if strategy == ConflictResolutionStrategy.REJECT_OUTDATED:
            raise ValueError(f"Entity changed since version created. Conflicts: {conflicts}")

        elif strategy == ConflictResolutionStrategy.MANUAL_MERGE:
            # Return conflicts for UI resolution
            return {
                "status": "conflict",
                "conflicts": conflicts,
                "version_id": version.id,
                "message": "Manual merge required",
            }

        elif strategy == ConflictResolutionStrategy.AUTO_MERGE_FIELDS:
            # Merge non-conflicting fields
            for key, value in version.data.items():
                if key not in conflicts:
                    setattr(entity, key, value)

            # For conflicts, keep entity value (user edits win)
            logger.warning(f"Auto-merge skipped conflicting fields: {conflicts}")

        else:  # LAST_WRITE_WINS
            for key, value in version.data.items():
                setattr(entity, key, value)
    else:
        # No conflicts, safe to apply
        for key, value in version.data.items():
            setattr(entity, key, value)

    version.approved = True
    await db.commit()
    return {"status": "approved", "conflicts": conflicts if conflicts else []}

async def _detect_conflicts(
    self,
    entity: Topic | Atom,
    version: TopicVersion | AtomVersion,
) -> list[str]:
    """Detect which fields changed since version created."""

    conflicts = []

    for key, version_value in version.data.items():
        entity_value = getattr(entity, key, None)

        # Check if entity field changed since version was created
        if entity_value != version_value:
            # Additional check: was entity modified after version created?
            if entity.updated_at > version.created_at:
                conflicts.append(key)

    return conflicts

# API endpoint for manual merge
@router.post("/versions/{version_id}/resolve-conflict")
async def resolve_conflict(
    version_id: int,
    resolution: dict[str, Any],  # {"field_name": "chosen_value"}
    db: AsyncSession = Depends(get_db),
):
    """Manual conflict resolution."""

    # Apply user's chosen values
    version = await db.get(TopicVersion, version_id)
    entity = await db.get(Topic, version.topic_id)

    for field, chosen_value in resolution.items():
        setattr(entity, field, chosen_value)

    version.approved = True
    await db.commit()

    return {"status": "resolved"}
```

**UI –¥–ª—è Conflict Resolution:**
```typescript
// When conflict detected, show UI:
interface ConflictResolution {
  field: string
  version_value: any
  entity_value: any
  user_choice: "version" | "entity" | "custom"
}

// User sees:
// Field: "description"
// LLM Version: "Planning home workout space"
// Current Value: "Setting up gym in garage"
// Choose: [Use LLM] [Keep Current] [Enter Custom]
```

---

#### ‚ö†Ô∏è Problem 15: No Version Expiration
**–ö—Ä–∏—Ç–∏—á–Ω—ñ—Å—Ç—å**: LOW
**–õ–æ–∫–∞—Ü—ñ—è**: –ù–µ–º–∞—î logic –¥–ª—è cleanup old versions

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# ‚ùå Versions –Ω–∞–∫–æ–ø–∏—á—É—é—Ç—å—Å—è –±–µ–∑–∫—ñ–Ω–µ—á–Ω–æ
# –ù–µ–º–∞—î:
# - Expiration policy
# - Archive strategy
# - Cleanup jobs
```

**Impact:**
- Database bloat (–∫–æ–∂–Ω–∞ extraction —Å—Ç–≤–æ—Ä—é—î versions)
- Slow queries –Ω–∞ –≤–µ—Ä—Å—ñ–π–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ
- –ù–µ–º–∞—î cleanup –¥–ª—è rejected versions

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
class VersionRetentionPolicy:
    """Policy for version retention and cleanup."""

    # Keep approved versions indefinitely
    approved_retention_days: int | None = None  # None = forever

    # Cleanup rejected versions after 90 days
    rejected_retention_days: int = 90

    # Cleanup pending versions after 30 days (expired proposals)
    pending_retention_days: int = 30

    async def cleanup_expired_versions(
        self,
        db: AsyncSession,
        entity_type: str,
        dry_run: bool = False,
    ) -> dict:
        """Remove expired versions based on retention policy."""

        now = datetime.now(UTC)

        # Find expired pending versions
        pending_cutoff = now - timedelta(days=self.pending_retention_days)
        rejected_cutoff = now - timedelta(days=self.rejected_retention_days)

        if entity_type == "topic":
            model = TopicVersion
            id_field = "topic_id"
        else:
            model = AtomVersion
            id_field = "atom_id"

        # Pending versions older than 30 days
        pending_stmt = select(model).where(
            model.approved == False,
            model.created_at < pending_cutoff,
        )

        pending_result = await db.execute(pending_stmt)
        expired_pending = list(pending_result.scalars().all())

        if not dry_run:
            for version in expired_pending:
                await db.delete(version)
            await db.commit()

        logger.info(
            f"Cleaned up {len(expired_pending)} expired pending {entity_type} versions "
            f"(older than {self.pending_retention_days} days)"
        )

        return {
            "entity_type": entity_type,
            "expired_pending": len(expired_pending),
            "dry_run": dry_run,
        }

# Scheduled cleanup task
@nats_broker.task
async def scheduled_version_cleanup_task() -> dict[str, Any]:
    """Clean up expired versions (run weekly)."""

    async with AsyncSessionLocal() as db:
        policy = VersionRetentionPolicy()

        topic_cleanup = await policy.cleanup_expired_versions(db, "topic", dry_run=False)
        atom_cleanup = await policy.cleanup_expired_versions(db, "atom", dry_run=False)

        return {
            "status": "success",
            "topics": topic_cleanup,
            "atoms": atom_cleanup,
        }
```

---

### 4.3 –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

#### Priority 1: Auto-Approval
1. **Implement auto-approval rules engine** (Problem 13)
2. **Add confidence/similarity fields** to version models
3. **Build approval rule UI** –¥–ª—è admin panel

#### Priority 2: Conflict Resolution
4. **Add conflict detection** (Problem 14)
5. **Implement merge strategies**: last-write-wins, manual merge, auto-merge
6. **Build conflict resolution UI**

#### Priority 3: Maintenance
7. **Version cleanup policy** (Problem 15)
8. **Scheduled cleanup task** (weekly/monthly)
9. **Archive old versions** instead of deletion

---

## 5. Cross-System Analysis

### 5.1 –£–∑–≥–æ–¥–∂–µ–Ω—ñ—Å—Ç—å Thresholds

**Problem**: Thresholds –Ω–µ —É–∑–≥–æ–¥–∂–µ–Ω—ñ across components:

| Component | Parameter | Value | Rationale |
|-----------|-----------|-------|-----------|
| Knowledge Extraction | `confidence_threshold` | 0.7 | ‚ùå Not documented |
| Knowledge Extraction | `message_threshold` | 10 | ‚ùå Not documented |
| Knowledge Extraction | `lookback_hours` | 24 | ‚ùå Not documented |
| Vector Search | `similarity_threshold` | 0.7 | ‚ùå Not documented |
| Versioning | Auto-approval confidence | N/A | ‚ùå Not implemented |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:**
```python
# Centralized configuration
class AISystemThresholds(BaseSettings):
    """Centralized AI system thresholds with rationale."""

    # Knowledge Extraction
    extraction_confidence_threshold: float = Field(
        default=0.70,
        description="Topics/atoms with 70%+ confidence auto-created (tested on 1000 messages)"
    )

    extraction_message_threshold: int = Field(
        default=10,
        description="Minimum messages to trigger extraction (balance cost vs latency)"
    )

    # Vector Search
    search_similarity_threshold: float = Field(
        default=0.65,
        description="Semantic search threshold (tested for precision/recall balance)"
    )

    duplicate_similarity_threshold: float = Field(
        default=0.95,
        description="High threshold for duplicate detection (reduce false positives)"
    )

    # Auto-Approval
    auto_approval_confidence_threshold: float = Field(
        default=0.85,
        description="Auto-approve versions with 85%+ confidence (high precision)"
    )

    auto_approval_similarity_threshold: float = Field(
        default=0.30,
        description="Max change from previous version for auto-approval (small edits only)"
    )
```

---

### 5.2 –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–Ω—ñ –ü—Ä–æ–±–ª–µ–º–∏

#### Issue 1: Embedding Generation Timing
**Problem**: Embeddings generated AFTER knowledge extraction ‚Üí no RAG context during extraction

**Impact:**
- Cannot use existing knowledge for better extraction
- Missed opportunity for similarity-based topic merging

**Solution:**
```
Current Flow:
Message ‚Üí Extraction ‚Üí Create Topics/Atoms ‚Üí Queue Embedding

Improved Flow:
Message ‚Üí Embed Message ‚Üí Extraction (with RAG context) ‚Üí Create Topics/Atoms ‚Üí Embed Atoms
```

#### Issue 2: No Quality Metrics
**Problem**: –ù–µ–º–∞—î metrics –¥–ª—è evaluation extraction quality

**Missing:**
- Precision/recall –¥–ª—è topic extraction
- User satisfaction scores
- Manual approval rate
- Topic duplication rate

**Solution:**
```python
class ExtractionQualityMetrics:
    """Track knowledge extraction quality over time."""

    async def log_extraction_metrics(
        self,
        run_id: str,
        messages_count: int,
        topics_created: int,
        atoms_created: int,
        avg_confidence: float,
    ) -> None:
        """Log extraction run metrics."""

        await db.add(ExtractionMetrics(
            run_id=run_id,
            messages_count=messages_count,
            topics_created=topics_created,
            atoms_created=atoms_created,
            avg_confidence=avg_confidence,
            timestamp=datetime.now(UTC),
        ))

    async def calculate_approval_rate(self, days: int = 7) -> float:
        """Calculate user approval rate for auto-created topics."""

        cutoff = datetime.now(UTC) - timedelta(days=days)

        # Count auto-created topics
        total_stmt = select(func.count()).select_from(Topic).where(
            Topic.created_at >= cutoff,
            Topic.created_by == "knowledge_extraction",
        )
        total = await db.scalar(total_stmt) or 0

        # Count approved versions
        approved_stmt = select(func.count()).select_from(TopicVersion).where(
            TopicVersion.created_at >= cutoff,
            TopicVersion.approved == True,
        )
        approved = await db.scalar(approved_stmt) or 0

        approval_rate = (approved / total) if total > 0 else 0

        return approval_rate

    async def get_quality_dashboard(self) -> dict:
        """Quality metrics dashboard."""

        return {
            "last_7_days": {
                "approval_rate": await self.calculate_approval_rate(7),
                "avg_confidence": await self.get_avg_confidence(7),
                "topic_duplication_rate": await self.get_duplication_rate(7),
            },
            "last_30_days": {
                "approval_rate": await self.calculate_approval_rate(30),
                "avg_confidence": await self.get_avg_confidence(30),
                "topic_duplication_rate": await self.get_duplication_rate(30),
            },
        }
```

---

## 6. Priority Fixes

### üî¥ Critical (Production Blockers)

1. **LLM Timeout Fallback** (Problem 2)
   - –§–∞–π–ª: `knowledge_extraction_service.py:189-225`
   - Impact: Knowledge loss –ø—Ä–∏ timeouts
   - Fix: Split batch on timeout, retry smaller chunks

2. **Cost Tracking** (Problem 5)
   - –§–∞–π–ª: `knowledge_extraction_service.py`
   - Impact: –ù–µ–∫–æ–Ω—Ç—Ä–æ–ª—å–æ–≤–∞–Ω—ñ –≤–∏—Ç—Ä–∞—Ç–∏
   - Fix: Implement CostTracker, budget alerts

3. **Embedding Strategy** (Problem 7)
   - –§–∞–π–ª: `embedding_service.py`, `tasks.py`
   - Impact: RAG –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, slow search
   - Fix: Hybrid strategy (immediate for high priority, batch for normal)

### ‚ö†Ô∏è High Priority (Quality Issues)

4. **Magic Numbers Documentation** (Problem 1)
   - –§–∞–π–ª: `tasks.py:15-16`, `knowledge_extraction_service.py:231`
   - Impact: –ù–µ–º–æ–∂–ª–∏–≤–æ optimize
   - Fix: Centralize thresholds –∑ rationale

5. **Prompt Quality** (Problem 4)
   - –§–∞–π–ª: `knowledge_extraction_service.py:75-116`
   - Impact: Lower extraction accuracy
   - Fix: Add examples, constraints, domain context

6. **Topic Similarity Matching** (Problem 6)
   - –§–∞–π–ª: `knowledge_extraction_service.py:262-263`
   - Impact: Duplicate topics
   - Fix: Semantic matching (embedding or fuzzy)

7. **Auto-Approval Rules** (Problem 13)
   - –§–∞–π–ª: `versioning_service.py:168-225`
   - Impact: Manual bottleneck
   - Fix: Build approval rules engine

### üìä Medium Priority (Monitoring & Optimization)

8. **Batch Size Optimization** (Problem 3, 8)
   - –§–∞–π–ª: `tasks.py:68-76`, `embedding_service.py`
   - Impact: Inefficient API usage
   - Fix: Dynamic sizing, OpenAI batch API

9. **Search Filters** (Problem 12)
   - –§–∞–π–ª: `semantic_search_service.py:76-86`
   - Impact: Limited search capabilities
   - Fix: Add author, date, topic filters

10. **Vector Index Monitoring** (Problem 11)
    - Impact: No performance visibility
    - Fix: Add health checks, latency tracking

### üîß Low Priority (Technical Debt)

11. **Threshold Testing** (Problem 10)
    - –§–∞–π–ª: `semantic_search_service.py:44`
    - Impact: Suboptimal thresholds
    - Fix: Build experimentation framework

12. **Conflict Resolution** (Problem 14)
    - –§–∞–π–ª: `versioning_service.py:211-216`
    - Impact: Lost user edits
    - Fix: Implement merge strategies

13. **Version Cleanup** (Problem 15)
    - Impact: Database bloat
    - Fix: Retention policy, cleanup task

---

## 7. –í–∏—Å–Ω–æ–≤–∫–∏

### Strengths
1. ‚úÖ Solid hexagonal architecture (framework-agnostic)
2. ‚úÖ Comprehensive error handling
3. ‚úÖ Good versioning workflow (snapshot + diff)
4. ‚úÖ Real-time WebSocket updates
5. ‚úÖ Batch processing infrastructure

### Critical Gaps
1. üî¥ No fallback for LLM failures ‚Üí knowledge loss
2. üî¥ No cost tracking ‚Üí uncontrolled spending
3. üî¥ No embedding strategy ‚Üí slow/incomplete search
4. ‚ö†Ô∏è Undocumented thresholds ‚Üí hard to optimize
5. ‚ö†Ô∏è No quality metrics ‚Üí blind iteration

### Production Readiness Score: 6/10
- **Architecture**: 9/10 (excellent design)
- **Error Handling**: 7/10 (good, but missing fallbacks)
- **Cost Management**: 3/10 (no tracking at all)
- **Quality Assurance**: 4/10 (no metrics, no evaluation)
- **Monitoring**: 5/10 (basic logging, no dashboards)

### Next Steps (Ordered by Priority)
1. Implement LLM timeout fallback (1 day)
2. Add cost tracking for all LLM calls (2 days)
3. Build hybrid embedding strategy (2 days)
4. Centralize thresholds with documentation (1 day)
5. Implement auto-approval rules (3 days)
6. Add quality metrics dashboard (2 days)
7. Optimize batch sizes (1 day)
8. Build search filters (2 days)

**Total effort**: ~2 weeks for production-ready AI infrastructure

---

## Appendix: File References

- Knowledge Extraction: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/knowledge_extraction_service.py`
- Embedding Service: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/embedding_service.py`
- Semantic Search: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/semantic_search_service.py`
- Versioning Service: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/services/versioning_service.py`
- Background Tasks: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/tasks.py`
- Configuration: `file:///Users/maks/PycharmProjects/task-tracker/backend/core/config.py`
- Topic Model: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/models/topic.py`
- Atom Model: `file:///Users/maks/PycharmProjects/task-tracker/backend/app/models/atom.py`
