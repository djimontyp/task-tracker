"""consolidate_sqlmodel_architecture

Revision ID: 7fd78d0a94e6
Revises: 320aa0020950
Create Date: 2025-09-26 04:05:12.925412

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "7fd78d0a94e6"
down_revision: Union[str, Sequence[str], None] = "320aa0020950"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema - drop complex unused tables in correct dependency order."""
    # Drop tables in dependency order (child tables first, then parents)

    # Drop dependent tables first
    op.drop_table("output")  # depends on issue
    op.drop_table("issue")  # depends on message
    op.drop_table("processingjob")  # depends on stream

    # Drop message table and its indexes
    op.drop_index(op.f("ix_message_sent_at"), table_name="message")
    op.drop_index(op.f("ix_message_thread_key"), table_name="message")
    op.drop_table("message")  # depends on source and stream

    # Drop stream table (depends on source)
    op.drop_table("stream")

    # Drop parent tables
    op.drop_table("source")
    op.drop_table("llmprovider")  # independent table

    # Update working table column types to match consolidated models
    op.alter_column(
        "simple_messages",
        "content",
        existing_type=sa.VARCHAR(),
        type_=sa.Text(),
        existing_nullable=False,
    )
    op.alter_column(
        "simple_tasks",
        "description",
        existing_type=sa.VARCHAR(),
        type_=sa.Text(),
        existing_nullable=False,
    )

    # Drop unused indexes from tasks table (these don't exist in our current schema anyway)
    try:
        op.drop_index(op.f("ix_task_ai_generated"), table_name="tasks")
    except Exception:
        pass  # Index might not exist
    try:
        op.drop_index(op.f("ix_task_category"), table_name="tasks")
    except Exception:
        pass  # Index might not exist
    try:
        op.drop_index(op.f("ix_task_priority"), table_name="tasks")
    except Exception:
        pass  # Index might not exist
    try:
        op.drop_index(op.f("ix_task_status"), table_name="tasks")
    except Exception:
        pass  # Index might not exist


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_index(op.f("ix_task_status"), "tasks", ["status"], unique=False)
    op.create_index(op.f("ix_task_priority"), "tasks", ["priority"], unique=False)
    op.create_index(op.f("ix_task_category"), "tasks", ["category"], unique=False)
    op.create_index(
        op.f("ix_task_ai_generated"), "tasks", ["ai_generated"], unique=False
    )
    op.alter_column(
        "simple_tasks",
        "description",
        existing_type=sa.Text(),
        type_=sa.VARCHAR(),
        existing_nullable=False,
    )
    op.alter_column(
        "simple_messages",
        "content",
        existing_type=sa.Text(),
        type_=sa.VARCHAR(),
        existing_nullable=False,
    )
    op.create_table(
        "output",
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("id", sa.BIGINT(), autoincrement=True, nullable=False),
        sa.Column("issue_id", sa.BIGINT(), autoincrement=False, nullable=False),
        sa.Column(
            "processor_type", sa.VARCHAR(length=50), autoincrement=False, nullable=False
        ),
        sa.Column(
            "external_id", sa.VARCHAR(length=100), autoincrement=False, nullable=False
        ),
        sa.Column("status", sa.VARCHAR(length=20), autoincrement=False, nullable=False),
        sa.ForeignKeyConstraint(
            ["issue_id"],
            ["issue.id"],
            name=op.f("output_issue_id_fkey"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("output_pkey")),
    )
    op.create_table(
        "issue",
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("id", sa.BIGINT(), autoincrement=True, nullable=False),
        sa.Column("message_id", sa.BIGINT(), autoincrement=False, nullable=False),
        sa.Column(
            "classification", sa.VARCHAR(length=50), autoincrement=False, nullable=False
        ),
        sa.Column(
            "category", sa.VARCHAR(length=50), autoincrement=False, nullable=False
        ),
        sa.Column(
            "priority", sa.VARCHAR(length=20), autoincrement=False, nullable=False
        ),
        sa.Column(
            "confidence",
            sa.DOUBLE_PRECISION(precision=53),
            autoincrement=False,
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["message_id"],
            ["message.id"],
            name=op.f("issue_message_id_fkey"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("issue_pkey")),
    )
    op.create_table(
        "llmprovider",
        sa.Column("id", sa.BIGINT(), autoincrement=True, nullable=False),
        sa.Column("name", sa.VARCHAR(length=100), autoincrement=False, nullable=False),
        sa.Column("type", sa.VARCHAR(length=50), autoincrement=False, nullable=False),
        sa.Column(
            "config",
            postgresql.JSONB(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False),
        sa.Column(
            "usage_stats",
            postgresql.JSONB(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("llmprovider_pkey")),
    )
    op.create_table(
        "stream",
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "id",
            sa.BIGINT(),
            server_default=sa.text("nextval('stream_id_seq'::regclass)"),
            autoincrement=True,
            nullable=False,
        ),
        sa.Column("source_id", sa.BIGINT(), autoincrement=False, nullable=False),
        sa.Column("type", sa.VARCHAR(length=50), autoincrement=False, nullable=False),
        sa.Column(
            "external_id", sa.VARCHAR(length=100), autoincrement=False, nullable=False
        ),
        sa.Column("name", sa.VARCHAR(length=100), autoincrement=False, nullable=False),
        sa.Column(
            "config",
            postgresql.JSONB(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.ForeignKeyConstraint(
            ["source_id"],
            ["source.id"],
            name="stream_source_id_fkey",
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name="stream_pkey"),
        sa.UniqueConstraint(
            "source_id",
            "external_id",
            name="uq_stream_source_external",
            postgresql_include=[],
            postgresql_nulls_not_distinct=False,
        ),
        postgresql_ignore_search_path=False,
    )
    op.create_table(
        "processingjob",
        sa.Column("id", sa.BIGINT(), autoincrement=True, nullable=False),
        sa.Column("stream_id", sa.BIGINT(), autoincrement=False, nullable=False),
        sa.Column(
            "processor_type", sa.VARCHAR(length=50), autoincrement=False, nullable=False
        ),
        sa.Column("status", sa.VARCHAR(length=20), autoincrement=False, nullable=False),
        sa.Column(
            "config",
            postgresql.JSONB(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "started_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.Column(
            "completed_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=True
        ),
        sa.ForeignKeyConstraint(
            ["stream_id"],
            ["stream.id"],
            name=op.f("processingjob_stream_id_fkey"),
            ondelete="CASCADE",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("processingjob_pkey")),
    )
    op.create_table(
        "source",
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "id",
            sa.BIGINT(),
            server_default=sa.text("nextval('source_id_seq'::regclass)"),
            autoincrement=True,
            nullable=False,
        ),
        sa.Column("type", sa.VARCHAR(length=50), autoincrement=False, nullable=False),
        sa.Column("name", sa.VARCHAR(length=100), autoincrement=False, nullable=False),
        sa.Column(
            "config",
            postgresql.JSONB(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("is_active", sa.BOOLEAN(), autoincrement=False, nullable=False),
        sa.PrimaryKeyConstraint("id", name="source_pkey"),
        postgresql_ignore_search_path=False,
    )
    op.create_table(
        "message",
        sa.Column(
            "created_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "updated_at",
            postgresql.TIMESTAMP(timezone=True),
            server_default=sa.text("now()"),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column("id", sa.BIGINT(), autoincrement=True, nullable=False),
        sa.Column("source_id", sa.BIGINT(), autoincrement=False, nullable=False),
        sa.Column("stream_id", sa.BIGINT(), autoincrement=False, nullable=True),
        sa.Column(
            "external_message_id",
            sa.VARCHAR(length=100),
            autoincrement=False,
            nullable=False,
        ),
        sa.Column("subject", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column("content", sa.TEXT(), autoincrement=False, nullable=False),
        sa.Column("content_html", sa.TEXT(), autoincrement=False, nullable=True),
        sa.Column(
            "payload",
            postgresql.JSONB(astext_type=sa.Text()),
            autoincrement=False,
            nullable=True,
        ),
        sa.Column(
            "author", sa.VARCHAR(length=100), autoincrement=False, nullable=False
        ),
        sa.Column("thread_key", sa.VARCHAR(), autoincrement=False, nullable=True),
        sa.Column(
            "sent_at", postgresql.TIMESTAMP(), autoincrement=False, nullable=False
        ),
        sa.ForeignKeyConstraint(
            ["source_id"],
            ["source.id"],
            name=op.f("message_source_id_fkey"),
            ondelete="CASCADE",
        ),
        sa.ForeignKeyConstraint(
            ["stream_id"],
            ["stream.id"],
            name=op.f("message_stream_id_fkey"),
            ondelete="SET NULL",
        ),
        sa.PrimaryKeyConstraint("id", name=op.f("message_pkey")),
        sa.UniqueConstraint(
            "source_id",
            "stream_id",
            "external_message_id",
            name=op.f("uq_msg_src_stream_extid"),
            postgresql_include=[],
            postgresql_nulls_not_distinct=False,
        ),
    )
    op.create_index(
        op.f("ix_message_thread_key"), "message", ["thread_key"], unique=False
    )
    op.create_index(op.f("ix_message_sent_at"), "message", ["sent_at"], unique=False)
    # ### end Alembic commands ###
